{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import CuDNNGRU\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../\"\n",
    "path_data_clean = root_path + \"data/clean/\"\n",
    "dfNoticias = pd.read_pickle(path_data_clean + \"/dfNoticiasCleanV2.p\")\n",
    "#dfNoticias.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1277689\n",
      "total chars: 90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_text = \"\"\n",
    "count = 0\n",
    "for index, row in dfNoticias.iterrows():    \n",
    "    if index < 800: #max aporx 3500\n",
    "        raw_text += row[\"Cuerpo\"]\n",
    "        \n",
    "text = raw_text.lower()\n",
    "print(len(text))\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 425863\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "\n",
    "maxlen = 100 # 100 largo secuencia, \n",
    "step = 3 # 3pasos \n",
    "\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arquitectura red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# callbacks entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0]:\n",
    "    #for diversity in [0.5]:\n",
    "        print('\\n\\n\\n')\n",
    "        print('----- diversidad:', diversity)\n",
    "        print('\\n\\n')\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        #generated += sentence\n",
    "        print('----- semilla: \"' + sentence + '\"')\n",
    "        print('\\n\\n')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(300):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "# updatable plot\n",
    "# a minimal example (sort of)\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"train_loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"validation_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()    \n",
    "        \n",
    "\n",
    "def saveFullModelState(epoch, _):        \n",
    "        model.save('weights/my_model.h5')\n",
    "        \n",
    "filepath=\"weights/weights.best.hdf5\"\n",
    "checkpointBest = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "        \n",
    "filepath=\"weights/weights-improvement-{epoch:02d}-{loss:.4f}-test1.hdf5\"\n",
    "checkpointAll = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build the model: a single LSTM\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(128, input_shape=(maxlen, len(chars)) , return_sequences=True))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(LSTM(64))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(GRU(128, input_shape=(maxlen, len(chars)) , return_sequences=True))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(GRU(64))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(128, input_shape=(maxlen, len(chars)) , return_sequences=True))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(LSTM(128))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(GRU(256, input_shape=(maxlen, len(chars)) , return_sequences=True))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(GRU(256))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model.load_weights(\"weights/weights.best.hdf5\")\n",
    "optimizer = RMSprop(lr=0.001) #0.01\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=1024,# 128\n",
    "          epochs=60,\n",
    "          validation_split=0.30,\n",
    "          callbacks=[plot_losses,print_callback,checkpointAll,checkpointBest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

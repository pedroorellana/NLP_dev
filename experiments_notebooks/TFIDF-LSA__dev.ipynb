{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('__file__ = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir /opt/NLP/Relationships/TFIDF-LSA__dev.models\n",
      "rm -f  /opt/NLP/Relationships/TFIDF-LSA__dev.models/*\n"
     ]
    }
   ],
   "source": [
    "#import warnings\n",
    "import os, sys, re, time, gc, types, string, unicodedata, unidecode, string, warnings, inspect\n",
    "warnings.filterwarnings('ignore', '.*do not.*',)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "from ctypes import cdll, CDLL\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "# Utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn import LinearClassifier\n",
    "from tensorflow.contrib.learn import DNNClassifier\n",
    "\n",
    "# Para extraer el texto plano del HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import scipy\n",
    "import scipy.sparse as sparse\n",
    "#import scipy.io\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import numpy as np\n",
    "\n",
    "import _pickle as pickle\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "\n",
    "import sklearn as skl \n",
    "#from sklearn.base import TransformerMixin\n",
    "#from sklearn.decomposition import TruncatedSVD\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.feature_extraction.text import HashingVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#from sklearn.preprocessing import Normalizer\n",
    "#from sklearn.decomposition import LatentDirichletAllocation\n",
    "#from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordBag, WordHash\n",
    "from wordbatch.models import FTRL\n",
    "\n",
    "# Para obrrar objetos Pandas y liberar memoria (Problema memory Leak())\n",
    "cdll.LoadLibrary(\"libc.so.6\")\n",
    "libc = CDLL(\"libc.so.6\")\n",
    "\n",
    "\n",
    "# Borro todos los modelos y creo directorio ?\n",
    "modelsPath = os.getcwd()+\"/\"+__file__.split(\".\")[0]+\".models\"\n",
    "if True:\n",
    "    print(\"mkdir \"+modelsPath)\n",
    "    os.system(\"mkdir \"+modelsPath)\n",
    "    print(\"rm -f  \"+modelsPath+\"/*\")\n",
    "    os.system(\"rm -f  \"+modelsPath+\"/*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Carga stop word\n",
    "nltk.download('stopwords')\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# punkt:  módulo contiene modelos para la tokenización de textos\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\"dLake\":\"nlp/news/cooperativa\",\"dSet\":\"allconhtml\",\"model\":\"LSA\",\"model-vec\":\"TFIDFWB\",\"vParam\":\"nw1000\",\"mParam\":\"svd100\"\n",
    "modelVec = \"TFIDFWB\"\n",
    "model = \"LSA\"\n",
    "dSet=\"sup300ch\"\n",
    "\n",
    "dLake = \"nlp/news/cooperativa\"\n",
    "nWordsTFIDF = [100,500,1000,5000]\n",
    "#sizeReducSVD = {100:[50], 500:[50,100,300], 1000:[50,100,500], 5000:[50,100,500,1000]}\n",
    "sizeReducSVD = {1000:[50,100,500]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(366143, 16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Borra de memoria objeto pandas df\n",
    "exec(\"try:\\n  del corpus\\nexcept:\\n  pass\\nlibc.malloc_trim(0)\")\n",
    "\n",
    "# Todas las noticias\n",
    "if True:\n",
    "    # dSet = \"allconhtml\"\n",
    "    # Lee archivo Pandas con todas las noticias\n",
    "    corpus = pd.read_pickle(\"/opt/tsmAI/DataLakes/nlp/news/cooperativa/models/corpus.pan\")\n",
    "\n",
    "    # Elimino: Duplicados, Cuerpos con None, Cuerpos de largo <= 300\n",
    "    corpus.drop_duplicates(inplace= True)\n",
    "    corpus = corpus[ corpus['Cuerpo'].notnull() & (corpus['Cuerpo'].str.len()>300) ]\n",
    "    corpus.reset_index(drop=True, inplace= True)\n",
    "\n",
    "    # Verifico que el ID de cada doc es unico\n",
    "    if  not pd.Series(corpus['ID']).is_unique:\n",
    "        print(\"ERROR: El ID de los documentos no son unicos !!\")\n",
    "\n",
    "        \n",
    "# df['ID'].tolist()\n",
    "os.system(\"rm -f \"+modelsPath+\"/\"+dSet+\"_\"+modelVec+\"_id.npy\")\n",
    "np.save(modelsPath+\"/\"+dSet+\"_\"+modelVec+\"_id\",corpus['ID'].tolist())\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso de BeautifulSoup\n",
    "import urllib.request\n",
    "with urllib.request.urlopen(\"http://www.cnn.com\") as url:\n",
    "    html = url.read()\n",
    "soup = BeautifulSoup(html,\"html.parser\") #lxml\n",
    "\n",
    "# kill all script and style elements\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.extract()    # rip it out\n",
    "\n",
    "# get text\n",
    "text = soup.get_text()\n",
    "\n",
    "# break into lines and remove leading and trailing space on each\n",
    "lines = (line.strip() for line in text.splitlines())\n",
    "# break multi-headlines into a line each\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "# drop blank lines\n",
    "text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "#print(text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NORMALIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp_news_cooperativa__sup300ch_TFIDFWB_normalize\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\" Funcion de normalizacion \"\"\"\n",
    "    \n",
    "    # Get plain text from HTML\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    \n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out    \n",
    "    \n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    #lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    #chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blan\n",
    "    #k lines\n",
    "    #text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    # split into words\n",
    "    tokens = nltk.tokenize.word_tokenize(text,language='spanish', preserve_line=False)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]    \n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are n<<<<<<<<<<<<<<<<<<<<<\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # stop word and remove accent\n",
    "    def strip_accents(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    stop_words = set(spanish_stopwords)\n",
    "    words = [strip_accents(w) for w in words if not w in stop_words]\n",
    "    \n",
    "    return u\" \".join(words)\n",
    "\n",
    "\n",
    "#import copy\n",
    "#exec(dSet+\"_\"+modelVec+\"_normalize=copy.copy(normalize_text3)\")\n",
    "\n",
    "new_normalize_text_name= dLake.replace(\"/\",\"_\")+\"__\"+dSet+\"_\"+modelVec+\"_normalize\"\n",
    "new_normalize_text = inspect.getsource(normalize_text).replace(\"normalize_text\",new_normalize_text_name)\n",
    "exec(new_normalize_text)\n",
    "os.system(\"rm -f \"+modelsPath+\"/\"+dSet+\"_\"+modelVec+\"_normalize.py\")\n",
    "with open(modelsPath+\"/\"+dSet+\"_\"+modelVec+\"_normalize.py\", \"w\") as f:\n",
    "    f.write(new_normalize_text)\n",
    "    \n",
    "print(new_normalize_text_name)\n",
    "\n",
    "#eval(dSet+\"_\"+modelVec+\"_normalize(\\\"el perro\\\")\")\n",
    "#print(inspect.getsource(eval(dSet+\"_\"+modelVec+\"_normalize\")))\n",
    "#eval(dSet+\"_\"+modelVec+\"_normalize\")\n",
    "#def test2(x):\n",
    "#    print(x(\"el perro\"))\n",
    "\n",
    "#test2(eval(\"normalize_text\"))\n",
    "#normalize_text.__name__ = \"sss\"\n",
    "#print(normalize_text.__name__)\n",
    "#import inspect\n",
    "#print(inspect.getmembers(normalize_text))\n",
    "#print(inspect.getsource(normalize_text))\n",
    "\n",
    "# Ver una notiza y como queda después de normalizar\n",
    "#print(len(spanish_stopwords))\n",
    "#print(\"-------------------------------\")\n",
    "#print( df.values[100][0][:400] )\n",
    "#print(\"-------------------------------\")\n",
    "#print( normalize_text(df.values[100][0])[:400] )\n",
    "#print(X_untransformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf:100\n",
      "366143\n",
      "20\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "tfidf:500\n",
      "366143\n",
      "20\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "tfidf:1000\n",
      "366143\n",
      "20\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "tfidf:5000\n",
      "366143\n",
      "20\n",
      "Normalize text\n",
      "Extract wordbags\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def doTfidf(docs,_n_words,n_cpu):\n",
    "    n_docs = len(docs)\n",
    "    print(n_docs)\n",
    "    print(n_cpu)\n",
    "    batch_size = int(n_docs/n_cpu)\n",
    "    extractor=(WordBag, {\"hash_ngrams\": 1, \"hash_ngrams_weights\": [1.0, 1.0],\n",
    "                         \"hash_size\": 2**22, \"norm\": \"l2\", \"tf\": 1.0,\"idf\": 1.0})\n",
    "    wb = wordbatch.WordBatch(eval(new_normalize_text_name), \n",
    "                             extractor= extractor, \n",
    "                             procs= n_cpu, \n",
    "                             n_words= _n_words,\n",
    "                             minibatch_size= batch_size)\n",
    "    \n",
    "    #wb.normalize_text = abstractmethod(normalize_text)\n",
    "    wb.dictionary_freeze = True\n",
    "    X = wb.fit_transform(docs,reset= False)\n",
    "    wb.nonZeroIndex = np.array(np.clip(X.getnnz(axis=0) - 1, 0, 1), dtype = bool)\n",
    "    X = X[:, wb.nonZeroIndex]\n",
    "    # Agrego funcion normalize    \n",
    "    return wb,X\n",
    "\n",
    "for nw in nWordsTFIDF:\n",
    "    print(\"tfidf:\"+str(nw))\n",
    "    vParam = \"nw\"+str(nw)\n",
    "    wb, X  = doTfidf(corpus['Cuerpo'].tolist(),nw,20)\n",
    "    apf = modelsPath+\"/\"+dSet+\"_\"+modelVec+\"-\"+vParam\n",
    "    os.system(\"rm -f \"+apf+\".pk\")\n",
    "    os.system(\"rm -f \"+apf+\".npz\")\n",
    "    with open(apf+\".pk\", 'wb') as file:\n",
    "        pickle.dump(wb, file) \n",
    "    scipy.sparse.save_npz(apf+\".npz\", X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X[0][0].todense())\n",
    "#X.shape\n",
    "#test = wb.transform([\"chile argentina\"])[:,wb.nonZeroIndex]\n",
    "#print(test.shape)\n",
    "\n",
    "#import inspect\n",
    "#print(inspect.getsource(wb.normalize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(\"el\")\n",
    "#normalize_text = staticmethod(normalize_text)\n",
    "#normalize_text.keys\n",
    "#print(len(list(map(lambda x: normalize_text(x),df['Cuerpo'].tolist()))))\n",
    "#print(len(df['Cuerpo'].tolist()))\n",
    "#print(X.shape)\n",
    "\n",
    "#XX = wb.transform([\"chile argentina\"])\n",
    "\n",
    "#print(XX.shape)\n",
    "\n",
    "#XX.todense().shape\n",
    "\n",
    "#print(wb)\n",
    "\n",
    "#import inspect\n",
    "\n",
    "#print(inspect.getsource(word_tokenize))\n",
    "\n",
    "#normalize_text(\"help help\")\n",
    "\n",
    "#print(XX[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(wb.dictionary)\n",
    "#print(wb.max_df)\n",
    "#print(wb.min_df)\n",
    "#print(wb.n_words)\n",
    "#test = wb.transform([\"chile argentina\"])\n",
    "#print(test.shape)\n",
    "#test.shape\n",
    "#wb.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA\n",
    "\n",
    "### link con explicacion simple y buena, info de SLA, SLA hecho a mano en vez de hensing.\n",
    "\n",
    "http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nw:100 --> sw:50\n",
      "nw:500 --> sw:50\n",
      "nw:500 --> sw:100\n",
      "nw:500 --> sw:300\n",
      "nw:1000 --> sw:50\n",
      "nw:1000 --> sw:100\n",
      "nw:1000 --> sw:500\n",
      "nw:5000 --> sw:50\n",
      "nw:5000 --> sw:100\n",
      "nw:5000 --> sw:500\n",
      "nw:5000 --> sw:1000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "\n",
    "for nw in sizeReducSVD.keys():\n",
    "    for sw in sizeReducSVD[nw]:\n",
    "        print(\"nw:\"+str(nw)+\" --> sw:\"+str(sw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svd:50  tfidf:1000\n",
      "svd:100  tfidf:1000\n",
      "svd:500  tfidf:1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for nw in sizeReducSVD.keys():\n",
    "    for sr in sizeReducSVD[nw]:\n",
    "        mParam = \"svd\"+str(sr)\n",
    "        #if sr==nw:\n",
    "        #    print(\"** svd:\"+str(sr)+\"  tfidf:\"+str(nw))\n",
    "        #    continue\n",
    "        print(\"svd:\"+str(sr)+\"  tfidf:\"+str(nw))\n",
    "        svd = TruncatedSVD(sr)\n",
    "        lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "        # Run SVD on the training data, then project the training data.\n",
    "        X_tfidf = sparse.load_npz(modelsPath+\"/\"+dSet+\"_\"+modelVec+\"-\"+vParam+\".npz\")\n",
    "        X_train_lsa = lsa.fit_transform(X_tfidf)\n",
    "        os.system(\"rm -f \"+modelsPath+\"/\"+dSet+\"_\"+modelVec+\"-\"+vParam+\"_\"+model+\"-\"+mParam+\".npy\")\n",
    "        np.save(modelsPath+\"/\"+dSet+\"_\"+modelVec+\"-\"+vParam+\"_\"+model+\"-\"+mParam, X_train_lsa)\n",
    "        os.system(\"rm -f \"+modelsPath+\"/\"+dSet+\"_\"+modelVec+\"-\"+vParam+\"_\"+model+\"-\"+mParam+\".pk\")\n",
    "        with open(modelsPath+\"/\"+dSet+\"_\"+modelVec+\"-\"+vParam+\"_\"+model+\"-\"+mParam+\".pk\", 'wb') as file:\n",
    "            pickle.dump(lsa, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(modelsPath+\"/\"+dSet+\"_\"+modelVec+\"-nw\"+str(nWordsTFIDF[0])+\".pk\", \"rb\") as file:\n",
    "    TFIDF = pickle.load(file)\n",
    "with open(modelsPath+\"/\"+dSet+\"_\"+modelVec+\"-nw\"+str(nWordsTFIDF[0])+\"_\"+model+\"-svd\"+str(sizeReducSVD[0])+\".pk\", \"rb\") as file:\n",
    "    LSA = pickle.load(file)\n",
    "Xt_lsa = np.load(modelsPath+\"/\"+dSet+\"_\"+modelVec+\"-nw\"+str(nWordsTFIDF[0])+\"_\"+model+\"-svd\"+str(sizeReducSVD[0])+\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg as LA\n",
    "\n",
    "def rowNormalize(x):\n",
    "    a = x\n",
    "    a = np.power(a,2)\n",
    "    a = np.sum(a,axis=1)\n",
    "    a = 1/np.power(a,1/2)\n",
    "    print(a)\n",
    "    return (x.T*a.T).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Test con ID de noticia 5000\n",
    "if False:\n",
    "    newNumber = 5000\n",
    "    test = Xt_lsa[newNumber]\n",
    "    error2 = np.power(Xt_lsa- test ,2)\n",
    "    error2 = np.sum(error2,axis=1) #.tolist()\n",
    "    sortSimilarity = np.argsort(error2)\n",
    "    print( corpus.values[newNumber] )\n",
    "    for k in range(0, 5):\n",
    "        print(\"########### noticia  \"+ str(k) +\" mas similar ############\")\n",
    "        print( corpus.values[sortSimilarity[k]] )\n",
    "\n",
    "    \n",
    "# Test con texto de la misma moticia ID 5000\n",
    "if False:\n",
    "    s0 = \"nEn un partido muy deslucido y que sólo al final tuvo emociones, Audax Italiano sacó a relucir su buen juego y en un minuto dio vuelta un resultado adverso para superar por 2-1 a Unión San Felipe en la quinta fecha del Torneo de Apertura, en un compromiso que se jugó en el Estadio Lucio Fariña.\\nEl duelo en la primera mitad fue muy lento y carente de emociones y recién a los 75 minutos de juego llegaron los goles.\\n\\n\\n\\n\\n\\n\\n\\nMauro Olivi anotó una de las cifras de los audinos. (Foto: UPI)\\n\\n\\n\\n\\nY fue el elenco de Nelson Cossio el que abrió el marcador, ya que con un impecable remate de Miguel Angel González venció la resistencia del portero Alejandro Sánchez y desató la alegría de los cerca de 500 hinchas sanfelipeños que llegaron a Quillota.\\nPero los itálicos no querían perder el tranco ganador y en un minuto revirtieron su suerte y consiguieron los tres puntos.\\nA los 87\\' Mauro Olivi aprovechó un error del buen portero Diego Sánchez y emparejó las acciones, y a los 88\\' Mario Cáceres desniveló el marcador a favor de los audinos.\\nCon a victoria, el cuadro de La Florida llegó a las nueve unidades, mientras que el equipo del Aconcagua se quedó con siete.\\n\\xa0\\nEstadísticas\\nAudax Italiano 2: Alejandro Sánchez; Carlos Garrido, Christian Vilches (89\\': Enzo Cabrera), Lucas Domínguez, Bryan Carrasco; Cristian Canuhé, Christián Jélvez, Matías Campos Toro, Facundo Pereyra (73\\': Angelo Delgado) (77\\': Mario Cáceres); Mauro Olivi \\xa0y Cristián Canío. DT Omar Labruna.\\nU. San Felipe 1: Diego Sánchez; David Fernández, Felipe Salinas, Omar Merlo, Esteban Sáez; Sebastián Páez, Gonzalo Espinoza, Kilian Virviescas, Miguel Angel González (85\\': Luis Valenzuela); Matías Urbano (80\\': Diego Alvarado) y Víctor Meza. (70\\': Ramiro Fergonzi). DT Nelson Cossio.\\nGoles: 0-1: 75\\'; Miguel Angel González (USF), 1-1: 87\\'; Mauro Olivi (AI), 2-1: 88\\'; Mario Cáceres (AI)\\nAmarillas: Sánchez, Jélvez (AI) Salinas (USF)\\nEstadio: Lucio Fariña, Quillota\\nArbitro: Carlos Ulloa\\n\"\n",
    "    #s0 = \"En un partido muy deslucido y que sólo al final tuvo emociones, Audax Italiano sacó a relucir su buen juego y en un minuto dio vuelta un resultado adverso para superar por 2-1 a Unión San Felipe en la quinta fecha del Torneo de Apertura, en un compromiso que se jugó en el Estadio Lucio Fariña.\\nEl duelo en la primera mitad f\"\n",
    "    s = normalize_text(s0)\n",
    "    test_tfidf = TFIDF.transform([s0])[:,TFIDF.nonZeroIndex]\n",
    "    test_tfidf_svd = LSA.transform(test_tfidf)\n",
    "    error2 = np.power(Xt_lsa- test_tfidf_svd ,2)\n",
    "    error2 = np.sum(error2,axis=1) #.tolist()\n",
    "    sortSimilarity = np.argsort(error2)\n",
    "    for k in range(0, 5):\n",
    "        docID = corpus[\"ID\"][sortSimilarity[k]]\n",
    "        print(\"########### noticia  \"+ str(k) +\" mas similar : \"+docID+\"############  \")\n",
    "        print( corpus.values[sortSimilarity[k]] )\n",
    "\n",
    "        \n",
    "# Test con texto de la misma moticia ID pero con vector normalizado\n",
    "if False:\n",
    "    s = \"En un partido muy deslucido y que sólo al final tuvo emociones, Audax Italiano sacó a relucir su buen juego y en un minuto dio vuelta un resultado adverso para superar por 2-1 a Unión San Felipe en la quinta fecha del Torneo de Apertura, en un compromiso que se jugó en el Estadio Lucio Fariña.\\nEl duelo en la primera mitad f\"\n",
    "    s = normalize_text(s)\n",
    "    test_tfidf = TFIDF.transform([s])[:,TFIDF.nonZeroIndex]\n",
    "    test_tfidf_svd = LSA.transform(test_tfidf)\n",
    "    \n",
    "    #Xt_lsa = rowNormalize(Xt_lsa)\n",
    "    #test_tfidf_svd = rowNormalize(test_tfidf_svd)\n",
    "\n",
    "    error2 = np.power(Xt_lsa- test_tfidf_svd ,2)\n",
    "    error2 = np.sum(error2,axis=1) #.tolist()\n",
    "    sortSimilarity = np.argsort(error2)\n",
    "    print( corpus.values[newNumber] )\n",
    "    docID = corpus[\"ID\"][sortSimilarity[k]]\n",
    "    for k in range(0, 5):\n",
    "        print(\"########### noticia  \"+ str(k) +\" mas similar : \"+docID+\"############  \")\n",
    "        print( corpus.values[sortSimilarity[k]] )\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sscd\n"
     ]
    }
   ],
   "source": [
    "# Get plain text from HTML\n",
    "#soup = BeautifulSoup(\"<p>ss</p>cd\", 'html.parser')\n",
    "# kill all script and style elements\n",
    "#for script in soup([\"script\", \"style\"]):\n",
    "#    script.extract()    # rip it out    \n",
    "# get text\n",
    "#text = soup.get_text()\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAW DATA (XML) To PANDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index, row in df.iterrows():\n",
    "    count +=1 \n",
    "    if count==200000:\n",
    "        break\n",
    "    soup = BeautifulSoup(row[\"Cuerpo\"], 'html.parser')\n",
    "    \n",
    "    #scriptList = soup.findAll(\"script\")\n",
    "    scriptList = soup.find(\"script\")\n",
    "    if  not scriptList == None:\n",
    "        if not scriptList.text==\"\":\n",
    "            print(\"\\n\"+str(index))\n",
    "            #scriptList = soup.find('script')\n",
    "            print(scriptList)\n",
    "            print(scriptList.text)\n",
    "        \n",
    "    #if len(scriptList)>0:\n",
    "    #    print(scriptList)\n",
    "## Get plain text from HTML\n",
    "#soup = BeautifulSoup(text, 'html.parser')\n",
    "#print(soup.findAll(\"script\"))\n",
    "\n",
    "#df.shape\n",
    "#df.head()\n",
    "#print(df[\"ID\"][2442]+\" - \"+corpus[\"ID\"][0])\n",
    "#df[df[\"ID\"]==\"20140929095927\"]\n",
    "#print(\"----\")\n",
    "#text = df[\"Cuerpo\"][2442]+\":: <script>dd</script>\"\n",
    "\n",
    "# Get plain text from HTML\n",
    "##soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "#print(soup.findAll(\"script\"))\n",
    "\n",
    "# kill all script and style elements\n",
    "##for script in soup([\"script\", \"style\"]):\n",
    "#    script.extract()    # rip it out    \n",
    "# get text\n",
    "##text = soup.get_text()\n",
    "# break into lines and remove leading and trailing space on each\n",
    "##lines = (line.strip() for line in text.splitlines())\n",
    "# break multi-headlines into a line each\n",
    "##chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "# drop blank lines\n",
    "#text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "#print(text)\n",
    "\n",
    "\n",
    "#import inspect\n",
    "#print(inspect.getsource(normalize_text))\n",
    "#wb,X = doTfidf(df['Cuerpo'].tolist(),1000)\n",
    "#wb.normalize_text(\"el auto\")\n",
    "\n",
    "#class NormalizeText: pass\n",
    "#NormalizeText.normalize_text =  staticmethod(normalize_text)\n",
    "#types.MethodType( normalize_text, NormalizeText )\n",
    "\n",
    "#normalizeText = types.MethodType( normalize_text, NormalizeText )\n",
    "\n",
    "#types.MethodType( normalize_text, NormalizeText )(\"el auto \")\n",
    "#def foo(self): pass\n",
    "#Employee.foo = foo\n",
    "\n",
    "\n",
    "#__main__.normalize_text(\"sdsd\")\n",
    "#X.toarray()\n",
    "\n",
    "#print(X.tocsc())\n",
    "#XX = X[:, np.array(np.clip(X.getnnz(axis=0) - 1, 0, 1), dtype = bool)]\n",
    "#type(XX)\n",
    "#XX.todense()\n",
    "#word_comment[:, np.array(np.clip(word_comment.getnnz(axis=0) - 1, 0, 1), dtype = bool)]\n",
    "#\n",
    "#wb.transform([\"sss\"])\n",
    "#with open('tfidf_test1.pk', 'wb') as file:\n",
    "#     pickle.dump(wb, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

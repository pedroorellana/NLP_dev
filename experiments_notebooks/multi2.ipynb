{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so | _pywrap_tensorflow_internal\n",
      "/root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/toco/python/_tensorflow_wrap_toco.so | _tensorflow_wrap_toco\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#import _pickle as pickle\n",
    "import pickle\n",
    "import re, sys, unidecode\n",
    "#import unidecode\n",
    "\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import scikitplot.plotters as skplt\n",
    "\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordBag, WordHash\n",
    "from wordbatch.models import FTRL\n",
    "\n",
    "from tensorflow.contrib.learn import DNNClassifier\n",
    "from tensorflow.contrib.learn import DNNEstimator\n",
    "#from tecnosmartlib import DataObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plt.style.use('fivethirtyeight')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoticias = pd.read_pickle(\"dfNoticias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre procesing\n",
    "\n",
    "Cleaning data, select clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting useful information...done.\n"
     ]
    }
   ],
   "source": [
    "print('Selecting useful information...', end='')\n",
    "_map = {}\n",
    "_map[\"Cuerpo\"] = []\n",
    "_map[\"Seccion\"] = []\n",
    "_map[\"Seccion2\"] = []\n",
    "_map[\"Seccion3\"] = []\n",
    "\n",
    "count = 0\n",
    "for index, row in dfNoticias.iterrows():\n",
    "    if (row[\"Seccion_1\"] != None and row[\"Cuerpo\"] != None):\n",
    "        _map[\"Cuerpo\"].append(row[\"Cuerpo\"]) \n",
    "        _map[\"Seccion\"].append(row[\"Seccion_1\"])\n",
    "        _map[\"Seccion2\"].append(row[\"Seccion_2\"])\n",
    "        _map[\"Seccion3\"].append(row[\"Seccion_3\"])\n",
    "        if row[\"Seccion_1\"] == row[\"Seccion_2\"] and row[\"Seccion_2\"] == row[\"Seccion_3\"]:\n",
    "            count += 1\n",
    "        if row[\"Seccion_1\"] == row[\"Seccion_2\"] and row[\"Seccion_3\"] == None:\n",
    "            count += 1\n",
    "        if row[\"Seccion_1\"] == row[\"Seccion_3\"] and row[\"Seccion_2\"] == None:  \n",
    "            count += 1 \n",
    "        if row[\"Seccion_2\"] == None and row[\"Seccion_3\"] == None:\n",
    "            count += 1      \n",
    "print('done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = list(set(_map[\"Seccion\"]))\n",
    "\n",
    "# def mapping(inputEle):\n",
    "#     for element in categories:\n",
    "#     for idx, element in enumerate(categories):\n",
    "#         if element == in     \n",
    "\n",
    "\n",
    "\n",
    "# map(function_to_apply, list_of_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( len(_map['Seccion2']) )\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data before dropping duplicates: 250339\n",
      "Number of data after dropping duplicates : 238931\n",
      "\n",
      "Number of duplicated data : 11408\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(_map)\n",
    "numberOfDuplicates = df.shape[0]\n",
    "print('Number of data before dropping duplicates: {}'.format(df.shape[0]))\n",
    "df = df.drop_duplicates(inplace= False)\n",
    "df.reset_index(drop=True, inplace= True)\n",
    "numberOfDuplicates -= df.shape[0]\n",
    "print('Number of data after dropping duplicates : {}'.format(df.shape[0]))\n",
    "print('\\nNumber of duplicated data : {}'.format(numberOfDuplicates))\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 238931\n"
     ]
    }
   ],
   "source": [
    "X_untransformed = df['Cuerpo'].reset_index(drop=True)\n",
    "y_untransformed = df['Seccion'].reset_index(drop=True)\n",
    "\n",
    "assert X_untransformed.shape[0] == y_untransformed.shape[0], 'X and y dimenssion must be the same.'\n",
    "print('Number of data: {}'.format(X_untransformed.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seccion\n",
      "Corporativo        82\n",
      "Cultura          3128\n",
      "Deportes        76458\n",
      "Economía         6750\n",
      "Entretención    29438\n",
      "Mundo           31664\n",
      "País            75180\n",
      "Sociedad         9079\n",
      "Tecnología       7130\n",
      "Name: Cuerpo, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "classes = df.groupby('Seccion')['Cuerpo'].nunique()\n",
    "\n",
    "print(classes)\n",
    "nClasses = classes.shape[0]\n",
    "\n",
    "#print('\\nNumber of classes: {}'.format(nClasses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraccion\n",
    "\n",
    "### Data representation\n",
    "\n",
    "TFID calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "TFIDF end time :100.51729583740234\n",
      "Number of features: 1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "print(\"start\")\n",
    "\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "def normalize_text(text):\n",
    "    return u\" \".join([x for x in [y for y in text.lower().strip().split(\" \")] \n",
    "                      if len(x) > 1 and x not in spanish_stopwords])\n",
    "\n",
    "X_untransformed = X_untransformed\n",
    "n_docs = X_untransformed.shape[0]\n",
    "n_cpu = 20\n",
    "\n",
    "batch_size = int(n_docs/n_cpu)\n",
    "\n",
    "wb = wordbatch.WordBatch(normalize_text, \n",
    "                         extractor=(WordBag, {\"hash_ngrams\": 1, \"hash_ngrams_weights\": [1.0, 1.0],\n",
    "                                              \"hash_size\": 2**28, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "                                              \"idf\": 1.0}), procs=n_cpu, n_words=1000, minibatch_size=batch_size)\n",
    "wb.dictionary_freeze = True\n",
    "word_comment = wb.fit_transform(list(X_untransformed),reset= False)\n",
    "X_transformed = word_comment[:, np.array(np.clip(word_comment.getnnz(axis=0) - 1, 0, 1), dtype = bool)]\n",
    "\n",
    "end = time.time()\n",
    "print(\"TFIDF end time :\" + str(end - start) )\n",
    "\n",
    "\n",
    "X = X_transformed\n",
    "print('Number of features: {}'.format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DataObject():\n",
    "    def __init__(self, x, y):\n",
    "        self.X = x\n",
    "        self.Y = y\n",
    "\n",
    "        try:\n",
    "            self.Y.reset_index(drop=True, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        self.X_train = self.X\n",
    "        self.Y_train = self.Y\n",
    "\n",
    "        self.X_test  = None\n",
    "        self.Y_test  = None\n",
    "\n",
    "        self.X_test_dense = None\n",
    "\n",
    "        self.k = 0\n",
    "    def next_train_batch(self, batch_size):\n",
    "        indexes = np.random.choice(self.X_train.shape[0], size=batch_size, replace=False)\n",
    "        return self.X_train[indexes], self.Y_train.reindex(indexes)\n",
    "\n",
    "    def set_train_test(self,train_fraction = None, test_fraction = None, dense=True):\n",
    "        # Tests for create subsets\n",
    "        if train_fraction == None and test_fraction == None:\n",
    "            train_fraction = 0.8\n",
    "        elif train_fraction == None:\n",
    "            assert 0 <= test_fraction and test_fraction < 1, 'Test fraction must be in [0,1)'\n",
    "            train_fraction = 1 - test_fraction\n",
    "        else:\n",
    "            assert 0 < train_fraction and train_fraction <= 1, 'Train fraction must be in (0,1]'\n",
    "\n",
    "        train_indices = np.random.choice(self.X.shape[0], round(train_fraction*self.X.shape[0]), replace=False)\n",
    "        test_indices = np.array(list(set(range(self.X.shape[0])) - set(train_indices)))\n",
    "\n",
    "        self.X_train = self.X[train_indices]\n",
    "        self.Y_train = self.Y.reindex(train_indices)\n",
    "        self.Y_train = self.Y_train.reset_index()\n",
    "\n",
    "        self.X_test = self.X[test_indices]\n",
    "        self.Y_test = self.Y.reindex(test_indices)\n",
    "        self.Y_test = self.Y_test.reset_index()\n",
    "        if dense:\n",
    "            self.X_test_dense = self.X_test.todense()\n",
    "        return True\n",
    "\n",
    "\n",
    "Y = y_untransformed\n",
    "Y = Y.reset_index(drop=True)\n",
    "\n",
    "def remove_accents(a):\n",
    "    return unidecode.unidecode(a)\n",
    "\n",
    "data = DataObject(X,Y.apply(remove_accents))\n",
    "\n",
    "labels = [remove_accents(x) for x in list(data.Y.unique())]\n",
    "#print(labels)\n",
    "\n",
    "data.set_train_test(0.8)\n",
    "#print(data.Y_train.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### DNN graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f799dc3e940>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': 500, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': './tmp2'}\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(42)\n",
    "\n",
    "feature_columns = [tf.contrib.layers.real_valued_column('x', dimension=1000)]\n",
    "#feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n",
    "\n",
    "\n",
    "classifier = DNNEstimator(                                \n",
    "                           head = tf.contrib.learn.multi_label_head(n_classes=9,weight_column_name='class_weights' ) , feature_columns=feature_columns,\n",
    "                           hidden_units=[2000, 1000, 100],\n",
    "                           dropout=0.1,\n",
    "                           #model_dir = './model-parallel/seccion_dnn',\n",
    "                           model_dir = './tmp2',\n",
    "                           config = tf.contrib.learn.RunConfig(save_checkpoints_steps = 500,\n",
    "                           save_checkpoints_secs = None)\n",
    "                          )\n",
    "\n",
    "\n",
    "#tensorboard --logdir=./tmp2\n",
    "# 10.20.0.114:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions graph tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100000\n",
    "\n",
    "# Define the test inputs\n",
    "def get_train_inputs():\n",
    "    print(\"Paso por aca.........................\")\n",
    "    X_ = data.X_train\n",
    "    Y_ = data.Y_train\n",
    "    X_ = X_.todense()\n",
    "    x = X_\n",
    "    Y_ = Y_['Seccion'].reset_index(drop=True)\n",
    "    #print(Y_)\n",
    "    from sklearn.preprocessing import LabelBinarizer    \n",
    "    #LabelBinarizer().fit(range(9)).transform(print(Y_))\n",
    "#     y = y.apply(remove_accents)\n",
    "#     y = y.values\n",
    "\n",
    "    y = Y_.values\n",
    "    #print(type(y))\n",
    "    \n",
    "    # computing lengths of each class\n",
    "    sizes = {label: Y_[Y_ == label].shape[0] for label in labels}\n",
    "\n",
    "    # creating weights for each sample\n",
    "    scale_factor = 10e3\n",
    "    weights = np.asarray([1.0/sizes[label] for label in y])\n",
    "    weights = scale_factor*weights\n",
    "    weights = weights[:,np.newaxis]\n",
    "    print(weights)\n",
    "    \n",
    "    #print('weights shape: {}'.format(weights.shape))\n",
    "    \n",
    "    \n",
    "    labels2 = list( set(y) )\n",
    "    def to_index(strInput):\n",
    "        tmp0 = [0]*len(labels2) \n",
    "        tmp0[labels2.index(strInput)] = 1\n",
    "        return tmp0\n",
    "\n",
    "    y = list( map(lambda x: to_index(x),y) )    \n",
    "\n",
    "    #print(np.array(y))\n",
    "    #print(y[:,np.newaxis])\n",
    "    \n",
    "    dataset = tf.estimator.inputs.numpy_input_fn({'x': x, 'class_weights': weights}, np.array(y), shuffle=True, batch_size=500, num_epochs=epochs)\n",
    "    \n",
    "    #dataset = tf.estimator.inputs.numpy_input_fn({'x': x, 'class_weights': weights}, y[:,np.newaxis], shuffle=True, batch_size=500, num_epochs=epochs)\n",
    "    return dataset\n",
    "\n",
    "def get_test_inputs():\n",
    "    X_ = data.X_test_dense\n",
    "    Y_ = data.Y_test\n",
    "#     X_ = X_.todense()\n",
    "    x = X_\n",
    "    Y_ = Y_['Seccion'].reset_index(drop=True)\n",
    "    y = Y_.values\n",
    "    \n",
    "    # computing lengths of each class\n",
    "    sizes = {label: Y_[Y_ == label].shape[0] for label in labels}\n",
    "#     print(sizes)\n",
    "\n",
    "    # creating weights for each sample\n",
    "    scale_factor = 10e3\n",
    "    weights = np.asarray([1.0/sizes[label] for label in y])\n",
    "    weights = scale_factor*weights\n",
    "    weights = weights[:,np.newaxis]\n",
    "    print('weights shape: {}'.format(weights.shape))\n",
    "\n",
    "    labels2 = list( set(y) )\n",
    "    def to_index(strInput):\n",
    "        tmp0 = [0]*len(labels2) \n",
    "        tmp0[labels2.index(strInput)] = 1\n",
    "        return tmp0\n",
    "\n",
    "    y = list( map(lambda x: to_index(x),y) )    \n",
    "    dataset = tf.estimator.inputs.numpy_input_fn({'x': x, 'class_weights': weights}, np.array(y), shuffle=False)    \n",
    "    \n",
    "    #dataset = tf.estimator.inputs.numpy_input_fn({'x': x, 'class_weights': weights}, y[:,np.newaxis], shuffle=False)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.set_random_seed(42)\n",
    "\n",
    "# feature_columns = [tf.contrib.layers.real_valued_column('x', dimension=1000)]\n",
    "# #feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n",
    "\n",
    "# #cross_entropy = tf.losses.sparse_softmax_cross_entropy(get_train_inputs())\n",
    "# x_ = tf.placeholder(tf.float64, [1,1000])\n",
    "# y_ = tf.placeholder(tf.int64, [1,1000])\n",
    "# w_ = tf.placeholder(tf.int64, [1])\n",
    "\n",
    "# cross_entropy = tf.losses.sigmoid_cross_entropy(multi_class_labels=y_, logits=x_)\n",
    "\n",
    "\n",
    "# classifier = DNNEstimator(                                \n",
    "#                            head = tf.contrib.learn.multi_label_head(n_classes=9,weight_column_name='class_weights',loss_fn=cross_entropy ) , feature_columns=feature_columns,\n",
    "#                            hidden_units=[2000, 1000, 100],\n",
    "#                            dropout=0.7,\n",
    "#                            #model_dir = './model-parallel/seccion_dnn',\n",
    "#                            model_dir = './tmp2',\n",
    "#                            config = tf.contrib.learn.RunConfig(save_checkpoints_steps = 500,\n",
    "#                            save_checkpoints_secs = None)\n",
    "#                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sigmoid_cross_entropy in module tensorflow.python.ops.losses.losses_impl:\n",
      "\n",
      "sigmoid_cross_entropy(multi_class_labels, logits, weights=1.0, label_smoothing=0, scope=None, loss_collection='losses', reduction='weighted_sum_by_nonzero_weights')\n",
      "    Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits.\n",
      "    \n",
      "    `weights` acts as a coefficient for the loss. If a scalar is provided,\n",
      "    then the loss is simply scaled by the given value. If `weights` is a\n",
      "    tensor of shape `[batch_size]`, then the loss weights apply to each\n",
      "    corresponding sample.\n",
      "    \n",
      "    If `label_smoothing` is nonzero, smooth the labels towards 1/2:\n",
      "    \n",
      "        new_multiclass_labels = multiclass_labels * (1 - label_smoothing)\n",
      "                                + 0.5 * label_smoothing\n",
      "    \n",
      "    Args:\n",
      "      multi_class_labels: `[batch_size, num_classes]` target integer labels in\n",
      "        `(0, 1)`.\n",
      "      logits: Float `[batch_size, num_classes]` logits outputs of the network.\n",
      "      weights: Optional `Tensor` whose rank is either 0, or the same rank as\n",
      "        `labels`, and must be broadcastable to `labels` (i.e., all dimensions must\n",
      "        be either `1`, or the same as the corresponding `losses` dimension).\n",
      "      label_smoothing: If greater than `0` then smooth the labels.\n",
      "      scope: The scope for the operations performed in computing the loss.\n",
      "      loss_collection: collection to which the loss will be added.\n",
      "      reduction: Type of reduction to apply to loss.\n",
      "    \n",
      "    Returns:\n",
      "      Weighted loss `Tensor` of the same type as `logits`. If `reduction` is\n",
      "      `NONE`, this has the same shape as `logits`; otherwise, it is scalar.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If the shape of `logits` doesn't match that of\n",
      "        `multi_class_labels` or if the shape of `weights` is invalid, or if\n",
      "        `weights` is None.  Also if `multi_class_labels` or `logits` is None.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.losses.sigmoid_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso por aca.........................\n",
      "[[0.16648076]\n",
      " [0.16347616]\n",
      " [0.16347616]\n",
      " ...\n",
      " [0.16648076]\n",
      " [0.16347616]\n",
      " [0.16648076]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.estimator.inputs.numpy_io.numpy_input_fn.<locals>.input_fn>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_train_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_ = data.X_train\n",
    "# Y_ = data.Y_train\n",
    "# X_ = X_.todense()\n",
    "# x = X_\n",
    "# Y_ = Y_['Seccion'].reset_index(drop=True)\n",
    "# #print(Y_)\n",
    "# from sklearn.preprocessing import LabelBinarizer    \n",
    "# #LabelBinarizer().fit(range(9)).transform(print(Y_))\n",
    "# #     y = y.apply(remove_accents)\n",
    "# #     y = y.values\n",
    "\n",
    "# y = Y_.values\n",
    "\n",
    "# labels = list( set(y) )\n",
    "\n",
    "# def to_index(strInput):\n",
    "#     tmp0 = [0]*len(labels) \n",
    "#     tmp0[labels.index(strInput)] = 1\n",
    "#     return tmp0\n",
    "#     #print(tmp0)\n",
    "    \n",
    "# y_new = list( map(lambda x: to_index(x),y) )\n",
    "\n",
    "# #print(y_new)\n",
    "# #print(labels)\n",
    "#print(labels.find(y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_index(strInput):\n",
    "#     tmp0 = [0]*len(labels) \n",
    "#     tmp0[labels.index(strInput)] = 1\n",
    "#     print(tmp0)\n",
    "\n",
    "# to_index('Cultura')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights shape: (47786, 1)\n",
      "Paso por aca.........................\n",
      "weights shape: (191145, 1)\n",
      "[[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 1 0]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-45117e634d68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#classifier.fit(input_fn=get_train_inputs(), monitors=[validation_monitor], steps=epochs, max_steps=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_train_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidation_monitor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 instructions)\n\u001b[0;32m--> 316\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    318\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    478\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_session_run_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks)\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_random_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m       \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_or_create_global_step_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    input_fn= get_test_inputs(),\n",
    "    every_n_steps=500,\n",
    "    #early_stopping_metric=\"accuracy\",\n",
    "    early_stopping_metric=\"loss\",\n",
    "    early_stopping_metric_minimize=True,\n",
    "    early_stopping_rounds=5000\n",
    "    )\n",
    "\n",
    "#classifier.fit(input_fn=get_train_inputs(), monitors=[validation_monitor], steps=epochs, max_steps=None)\n",
    "classifier.fit(input_fn=get_train_inputs(), monitors=[validation_monitor], max_steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_inputs():\n",
    "    X_ = data.X_test_dense\n",
    "    Y_ = data.Y_test\n",
    "#     X_ = X_.todense()\n",
    "    x = X_\n",
    "    Y_ = Y_['Seccion'].reset_index(drop=True)\n",
    "    y = Y_.values\n",
    "    \n",
    "    # computing lengths of each class\n",
    "    sizes = {label: Y_[Y_ == label].shape[0] for label in labels}\n",
    "#     print(sizes)\n",
    "\n",
    "    # creating weights for each sample\n",
    "    scale_factor = 10e3\n",
    "    weights = np.asarray([1.0/sizes[label] for label in y])\n",
    "    weights = scale_factor*weights\n",
    "    weights = weights[:,np.newaxis]\n",
    "    print('weights shape: {}'.format(weights.shape))\n",
    "\n",
    "    labels2 = list( set(y) )\n",
    "    def to_index(strInput):\n",
    "        tmp0 = [0]*len(labels2) \n",
    "        tmp0[labels2.index(strInput)] = 1\n",
    "        return tmp0\n",
    "\n",
    "    y = list( map(lambda x: to_index(x),y) )    \n",
    "    #dataset = tf.estimator.inputs.numpy_input_fn({'x': x, 'class_weights': weights}, np.array(y), shuffle=False)    \n",
    "    \n",
    "    #dataset = tf.estimator.inputs.numpy_input_fn({'x': x, 'class_weights': weights}, y[:,np.newaxis], shuffle=False)\n",
    "    x = tf.constant(x)\n",
    "    class_weights =tf.constant(weights) \n",
    "    y = tf.constant(np.array(y))\n",
    "    return {'x':x,'class_weights': class_weights}, y\n",
    "\n",
    "\n",
    "def get_train_inputs():\n",
    "    print(\"Paso por aca.........................\")\n",
    "    X_ = data.X_train\n",
    "    Y_ = data.Y_train\n",
    "    X_ = X_.todense()\n",
    "    x = X_\n",
    "    Y_ = Y_['Seccion'].reset_index(drop=True)\n",
    "    #print(Y_)\n",
    "    from sklearn.preprocessing import LabelBinarizer    \n",
    "    #LabelBinarizer().fit(range(9)).transform(print(Y_))\n",
    "#     y = y.apply(remove_accents)\n",
    "#     y = y.values\n",
    "\n",
    "    y = Y_.values\n",
    "    #print(type(y))\n",
    "    \n",
    "    # computing lengths of each class\n",
    "    sizes = {label: Y_[Y_ == label].shape[0] for label in labels}\n",
    "#     print(sizes)\n",
    "\n",
    "    # creating weights for each sample\n",
    "    scale_factor = 10e3\n",
    "    weights = np.asarray([1.0/sizes[label] for label in y])\n",
    "    weights = scale_factor*weights\n",
    "    weights = weights[:,np.newaxis]\n",
    "    print('weights shape: {}'.format(weights.shape))\n",
    "    \n",
    "    \n",
    "    labels2 = list( set(y) )\n",
    "    def to_index(strInput):\n",
    "        tmp0 = [0]*len(labels2) \n",
    "        tmp0[labels2.index(strInput)] = 1\n",
    "        return tmp0\n",
    "\n",
    "    y = list( map(lambda x: to_index(x),y) )    \n",
    "\n",
    "    print(np.array(y))\n",
    "    #print(y[:,np.newaxis])\n",
    "    \n",
    "    #dataset = tf.estimator.inputs.numpy_input_fn({'x': x, 'class_weights': weights}, np.array(y), shuffle=True, batch_size=500, num_epochs=epochs)\n",
    "    \n",
    "    #dataset = tf.estimator.inputs.numpy_input_fn({'x': x, 'class_weights': weights}, y[:,np.newaxis], shuffle=True, batch_size=500, num_epochs=epochs)\n",
    "    #return dataset\n",
    "\n",
    "    x = tf.constant(x)\n",
    "    class_weights =tf.constant(weights) \n",
    "    y = tf.constant(np.array(y))\n",
    "    return {'x':x,'class_weights': class_weights}, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights shape: (47786, 1)\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-10-19:02:54\n",
      "INFO:tensorflow:Restoring parameters from ./tmp2/model.ckpt-113514\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-10-19:03:55\n",
      "INFO:tensorflow:Saving dict for global step 113514: accuracy = 0.8517599, auc = 0.60530096, auc_precision_recall = 0.1733294, global_step = 113514, loss = 0.813079\n",
      "{'loss': 0.813079, 'accuracy': 0.8517599, 'auc': 0.60530096, 'auc_precision_recall': 0.1733294, 'global_step': 113514}\n"
     ]
    }
   ],
   "source": [
    "# def input_fn_evaluate():\n",
    "#     dataset = {'x': tf.constant(data.X_test_dense)}\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "# pred_test = classifier.evaluate(input_fn=get_test_inputs)\n",
    "\n",
    "\n",
    "score = classifier.evaluate(input_fn=get_test_inputs,\n",
    "                                    steps=1)\n",
    "\n",
    "\n",
    "# # print(pred_test)\n",
    "\n",
    "# y_test_hat = np.asarray([x.decode('UTF-8') for x in list(pred_test)])\n",
    "# y_test_hat = y_test_hat.astype(str)\n",
    "# y_test = data.Y_test['Seccion'].values\n",
    "# y_test = y_test.astype(str)\n",
    "# print('Test shape: {}\\nReal shape: {}'.format(y_test_hat.shape, y_test.shape))\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# acc = accuracy_score(y_true=y_test, y_pred=y_test_hat)\n",
    "# print('Accuracy in test: {}'.format(acc))\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights shape: (47786, 1)\n",
      "INFO:tensorflow:Restoring parameters from ./tmp2/model.ckpt-113514\n"
     ]
    }
   ],
   "source": [
    "pred_test = classifier.predict(input_fn=get_test_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = classifier.evaluate(input_fn=get_test_inputs,steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(out.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights shape: (47786, 1)\n",
      "INFO:tensorflow:Restoring parameters from ./tmp2/model.ckpt-113514\n",
      "[{'logits': array([-5.4730062 , -2.201419  , -2.8765526 , -3.602121  , -3.4763107 ,\n",
      "       -2.3182328 , -0.71752167, -0.34028864, -2.3252192 ], dtype=float32), 'probabilities': array([0.00418104, 0.09962312, 0.0533249 , 0.02654213, 0.02999383,\n",
      "       0.08962414, 0.32793894, 0.4157394 , 0.08905575], dtype=float32), 'classes': array([0, 0, 0, 0, 0, 0, 0, 0, 0])}, {'logits': array([-6.2931013 , -3.193989  , -3.5452323 , -4.1725717 , -5.077106  ,\n",
      "       -3.0382257 , -1.4346077 ,  0.36818933, -2.1173959 ], dtype=float32), 'probabilities': array([0.0018456 , 0.03939255, 0.02805227, 0.01517863, 0.00619926,\n",
      "       0.04572853, 0.19238175, 0.5910214 , 0.10741749], dtype=float32), 'classes': array([0, 0, 0, 0, 0, 0, 0, 1, 0])}, {'logits': array([-7.2853928 , -4.410593  , -6.4298477 , -3.8925962 , -4.6113157 ,\n",
      "       -2.8711119 , -2.0688114 , -0.36548015, -0.6972369 ], dtype=float32), 'probabilities': array([0.00068501, 0.01200217, 0.0016101 , 0.0199848 , 0.00984093,\n",
      "       0.05360022, 0.11216535, 0.4096336 , 0.33242515], dtype=float32), 'classes': array([0, 0, 0, 0, 0, 0, 0, 0, 0])}, {'logits': array([-8.736505 , -3.8399577, -4.769907 , -4.7861767, -8.339127 ,\n",
      "       -3.4683776, -2.9103637,  1.1535275, -4.0229387], dtype=float32), 'probabilities': array([1.6058853e-04, 2.1042218e-02, 8.4098438e-03, 8.2752481e-03,\n",
      "       2.3892387e-04, 3.0225499e-02, 5.1643621e-02, 7.6015466e-01,\n",
      "       1.7585499e-02], dtype=float32), 'classes': array([0, 0, 0, 0, 0, 0, 0, 1, 0])}, {'logits': array([-10.145388 ,  -5.1614075,  -6.456252 ,  -5.145359 , -10.466707 ,\n",
      "        -4.5351133,  -4.2181263,   1.7347009,  -3.9432511], dtype=float32), 'probabilities': array([3.9255188e-05, 5.7009370e-03, 1.5682082e-03, 5.7926318e-03,\n",
      "       2.8467835e-05, 1.0611872e-02, 1.4512497e-02, 8.5001272e-01,\n",
      "       1.9016454e-02], dtype=float32), 'classes': array([0, 0, 0, 0, 0, 0, 0, 1, 0])}, {'logits': array([-10.235367  ,   0.36156058,  -0.89250207,  -9.6835    ,\n",
      "        -7.2724586 ,  -7.402654  ,  -6.3800235 ,  -7.583804  ,\n",
      "       -13.75009   ], dtype=float32), 'probabilities': array([3.5877412e-05, 5.8941817e-01, 2.9059377e-01, 6.2299165e-05,\n",
      "       6.9392077e-04, 6.0926104e-04, 1.6922146e-03, 5.0836412e-04,\n",
      "       1.0676072e-06], dtype=float32), 'classes': array([0, 1, 0, 0, 0, 0, 0, 0, 0])}, {'logits': array([-8.023292 , -4.2395935, -5.391219 , -4.4448366, -7.5690603,\n",
      "       -3.650111 , -2.4972098,  1.0489244, -2.8769813], dtype=float32), 'probabilities': array([3.2763206e-04, 1.4208653e-02, 4.5357482e-03, 1.1602818e-02,\n",
      "       5.1591097e-04, 2.5329964e-02, 7.6054014e-02, 7.4056828e-01,\n",
      "       5.3303268e-02], dtype=float32), 'classes': array([0, 0, 0, 0, 0, 0, 0, 1, 0])}, {'logits': array([-4.805561  ,  0.23847836, -2.210533  , -3.6314726 , -2.505835  ,\n",
      "       -3.1997843 , -2.045443  , -3.0323274 , -5.8912644 ], dtype=float32), 'probabilities': array([0.00811767, 0.5593386 , 0.09880861, 0.02579421, 0.07545013,\n",
      "       0.03917384, 0.11451364, 0.04598662, 0.00275586], dtype=float32), 'classes': array([0, 1, 0, 0, 0, 0, 0, 0, 0])}, {'logits': array([-4.9689693, -0.7579447, -2.327578 , -3.5478685, -1.3890288,\n",
      "       -2.1690176, -0.8352172, -2.3580892, -3.7037735], dtype=float32), 'probabilities': array([0.00690233, 0.31909266, 0.08886456, 0.02798049, 0.19956286,\n",
      "       0.10256743, 0.30254307, 0.08642494, 0.02403833], dtype=float32), 'classes': array([0, 0, 0, 0, 0, 0, 0, 0, 0])}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = list(classifier.predict(input_fn = get_test_inputs))\n",
    "print(predictions[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predictions[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-72ad0a99ebd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"probabilities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "predictions[1:3][\"probabilities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(classifier.predict(input_fn = get_test_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42175\n",
      "47786\n"
     ]
    }
   ],
   "source": [
    "#predictions[1:10]\n",
    "\n",
    "out = list(map(lambda x:x[\"classes\"] ,predictions))\n",
    "sums = np.sum(out,axis = 1)\n",
    "total = sum(sums>=1)\n",
    "print(total)\n",
    "print(len(sums))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass-multioutput is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-8caa7a4da957>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \"\"\"\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass-multioutput is not supported"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "import sklearn\n",
    "sklearn.metrics.confusion_matrix(out, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp2/model.ckpt-113514\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-7f744c18f0db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# print(pred_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0my_test_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my_test_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Seccion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-7f744c18f0db>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# print(pred_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0my_test_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my_test_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Seccion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "def input_fn_evaluate():\n",
    "    dataset = {'x': tf.constant(data.X_test_dense)}\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "pred_test = classifier.predict(input_fn=input_fn=get_test_inputs)\n",
    "\n",
    "# print(pred_test)\n",
    "\n",
    "y_test_hat = np.asarray([x.decode('UTF-8') for x in list(pred_test)])\n",
    "y_test_hat = y_test_hat.astype(str)\n",
    "y_test = data.Y_test['Seccion'].values\n",
    "y_test = y_test.astype(str)\n",
    "print('Test shape: {}\\nReal shape: {}'.format(y_test_hat.shape, y_test.shape))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_true=y_test, y_pred=y_test_hat)\n",
    "print('Accuracy in test: {}'.format(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_test = list(classifier.predict(input_fn = get_test_inputs))\n",
    "print(predictions[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
       " array([0, 1, 1, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = list(map(lambda x:x[\"classes\"] ,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-148-18b557612b5d>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-148-18b557612b5d>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    confusion_matrix(out, out,[1 1 1 1 1 1 1 1 1])\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "out = list(map(lambda x:x[\"classes\"] ,predictions))\n",
    "out = np.array(out)\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def confusion_matrix(yt, yp, classes):\n",
    "    instcount = yt.shape[0]\n",
    "    n_classes = classes.shape[0]\n",
    "    mtx = np.zeros((n_classes, 4))\n",
    "    for i in range(instcount):\n",
    "        for c in range(n_classes):\n",
    "            mtx[c,0] += 1 if yt[i,c]==1 and yp[i,c]==1 else 0\n",
    "            mtx[c,1] += 1 if yt[i,c]==1 and yp[i,c]==0 else 0\n",
    "            mtx[c,2] += 1 if yt[i,c]==0 and yp[i,c]==0 else 0\n",
    "            mtx[c,3] += 1 if yt[i,c]==0 and yp[i,c]==1 else 0\n",
    "    mtx = [[m0/(m0+m1), m1/(m0+m1), m2/(m2+m3), m3/(m2+m3)] for m0,m1,m2,m3 in mtx]\n",
    "    plt.figure(num=None, figsize=(5, 15), dpi=100, facecolor='w', edgecolor='k')\n",
    "    plt.imshow(mtx, interpolation='nearest',cmap='Blues')\n",
    "    plt.title(\"title\")\n",
    "    tick_marks = np.arange(n_classes)\n",
    "    plt.xticks(np.arange(4), ['1 - 1','1 - 0','0 - 0','0 - 1'])\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    for i, j in itertools.product(range(n_classes), range(4)):\n",
    "        plt.text(j, i, round(mtx[i][j],2), horizontalalignment=\"center\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('labels')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "confusion_matrix(out, out,[1 1 1 1 1 1 1 1 1])\n",
    "    \n",
    "# y1 = np.genfromtxt('y_true.csv', delimiter=\",\")\n",
    "# y2 = np.genfromtxt('y_pred.csv', delimiter=\",\")\n",
    "# labels = np.genfromtxt('labels.csv', delimiter=\",\", dtype=\"|S\")\n",
    "# confusion_matrix(y1, y2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pais' 'Pais' 'Pais' ... 'Deportes' 'Pais' 'Deportes']\n"
     ]
    }
   ],
   "source": [
    "y_test = data.Y_test['Seccion'].values\n",
    "y_test = y_test.astype(str)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso por aca.........................\n",
      "weights shape: (191145, 1)\n",
      "[[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "x,y =get_train_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-ba15604c532a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#with sess.as_default()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "#with sess.as_default()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba = classifier.predict_proba(input_fn=input_fn_evaluate,as_iterable=False)\n",
    "np.sum(predict_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = skplt.plot_confusion_matrix(y_test, y_test_hat,normalize=True)\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "for tick in ax.xaxis.get_minor_ticks():\n",
    "    tick.tick1line.set_markersize(0)\n",
    "    tick.tick2line.set_markersize(0)\n",
    "    tick.label1.set_horizontalalignment('center')\n",
    "\n",
    "plt2.xticks(rotation=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

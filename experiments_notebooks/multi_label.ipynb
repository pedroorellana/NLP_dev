{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so | _pywrap_tensorflow_internal\n",
      "/root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/toco/python/_tensorflow_wrap_toco.so | _tensorflow_wrap_toco\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#import _pickle as pickle\n",
    "import pickle\n",
    "import re, sys, unidecode\n",
    "#import unidecode\n",
    "\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import scikitplot.plotters as skplt\n",
    "\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordBag, WordHash\n",
    "from wordbatch.models import FTRL\n",
    "\n",
    "from tensorflow.contrib.learn import DNNClassifier\n",
    "from tensorflow.contrib.learn import DNNEstimator\n",
    "\n",
    "#from tecnosmartlib import DataObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoticias = pd.read_pickle(\"dfNoticias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre procesing\n",
    "\n",
    "Cleaning data, select clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting useful information...done.\n"
     ]
    }
   ],
   "source": [
    "print('Selecting useful information...', end='')\n",
    "_map = {}\n",
    "_map[\"Cuerpo\"] = []\n",
    "_map[\"Seccion\"] = []\n",
    "_map[\"Seccion2\"] = []\n",
    "_map[\"Seccion3\"] = []\n",
    "\n",
    "count = 0\n",
    "for index, row in dfNoticias.iterrows():\n",
    "    if (row[\"Seccion_1\"] != None and row[\"Cuerpo\"] != None):\n",
    "        _map[\"Cuerpo\"].append(row[\"Cuerpo\"]) \n",
    "        _map[\"Seccion\"].append(row[\"Seccion_1\"])\n",
    "        _map[\"Seccion2\"].append(row[\"Seccion_2\"])\n",
    "        _map[\"Seccion3\"].append(row[\"Seccion_3\"])\n",
    "        if row[\"Seccion_1\"] == row[\"Seccion_2\"] and row[\"Seccion_2\"] == row[\"Seccion_3\"]:\n",
    "            count += 1\n",
    "        if row[\"Seccion_1\"] == row[\"Seccion_2\"] and row[\"Seccion_3\"] == None:\n",
    "            count += 1\n",
    "        if row[\"Seccion_1\"] == row[\"Seccion_3\"] and row[\"Seccion_2\"] == None:  \n",
    "            count += 1 \n",
    "        if row[\"Seccion_2\"] == None and row[\"Seccion_3\"] == None:\n",
    "            count += 1      \n",
    "print('done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250339\n",
      "220068\n"
     ]
    }
   ],
   "source": [
    "print( len(_map['Seccion2']) )\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data before dropping duplicates: 250339\n",
      "Number of data after dropping duplicates : 238931\n",
      "\n",
      "Number of duplicated data : 11408\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(_map)\n",
    "numberOfDuplicates = df.shape[0]\n",
    "print('Number of data before dropping duplicates: {}'.format(df.shape[0]))\n",
    "df = df.drop_duplicates(inplace= False)\n",
    "df.reset_index(drop=True, inplace= True)\n",
    "numberOfDuplicates -= df.shape[0]\n",
    "print('Number of data after dropping duplicates : {}'.format(df.shape[0]))\n",
    "print('\\nNumber of duplicated data : {}'.format(numberOfDuplicates))\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 238931\n"
     ]
    }
   ],
   "source": [
    "X_untransformed = df['Cuerpo'].reset_index(drop=True)\n",
    "y_untransformed = df['Seccion'].reset_index(drop=True)\n",
    "\n",
    "assert X_untransformed.shape[0] == y_untransformed.shape[0], 'X and y dimenssion must be the same.'\n",
    "print('Number of data: {}'.format(X_untransformed.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seccion\n",
      "Corporativo        82\n",
      "Cultura          3128\n",
      "Deportes        76458\n",
      "Economía         6750\n",
      "Entretención    29438\n",
      "Mundo           31664\n",
      "País            75180\n",
      "Sociedad         9079\n",
      "Tecnología       7130\n",
      "Name: Cuerpo, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "classes = df.groupby('Seccion')['Cuerpo'].nunique()\n",
    "\n",
    "print(classes)\n",
    "nClasses = classes.shape[0]\n",
    "\n",
    "#print('\\nNumber of classes: {}'.format(nClasses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraccion\n",
    "\n",
    "### Data representation\n",
    "\n",
    "TFID calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "TFIDF end time :101.38772940635681\n",
      "Number of features: 1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "print(\"start\")\n",
    "\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "def normalize_text(text):\n",
    "    return u\" \".join([x for x in [y for y in text.lower().strip().split(\" \")] \n",
    "                      if len(x) > 1 and x not in spanish_stopwords])\n",
    "\n",
    "X_untransformed = X_untransformed\n",
    "n_docs = X_untransformed.shape[0]\n",
    "n_cpu = 20\n",
    "\n",
    "batch_size = int(n_docs/n_cpu)\n",
    "\n",
    "wb = wordbatch.WordBatch(normalize_text, \n",
    "                         extractor=(WordBag, {\"hash_ngrams\": 1, \"hash_ngrams_weights\": [1.0, 1.0],\n",
    "                                              \"hash_size\": 2**28, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "                                              \"idf\": 1.0}), procs=n_cpu, n_words=1000, minibatch_size=batch_size)\n",
    "wb.dictionary_freeze = True\n",
    "word_comment = wb.fit_transform(list(X_untransformed),reset= False)\n",
    "X_transformed = word_comment[:, np.array(np.clip(word_comment.getnnz(axis=0) - 1, 0, 1), dtype = bool)]\n",
    "\n",
    "end = time.time()\n",
    "print(\"TFIDF end time :\" + str(end - start) )\n",
    "\n",
    "\n",
    "X = X_transformed\n",
    "print('Number of features: {}'.format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DataObject():\n",
    "    def __init__(self, x, y):\n",
    "        self.X = x\n",
    "        self.Y = y\n",
    "\n",
    "        try:\n",
    "            self.Y.reset_index(drop=True, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        self.X_train = self.X\n",
    "        self.Y_train = self.Y\n",
    "\n",
    "        self.X_test  = None\n",
    "        self.Y_test  = None\n",
    "\n",
    "        self.X_test_dense = None\n",
    "\n",
    "        self.k = 0\n",
    "    def next_train_batch(self, batch_size):\n",
    "        indexes = np.random.choice(self.X_train.shape[0], size=batch_size, replace=False)\n",
    "        return self.X_train[indexes], self.Y_train.reindex(indexes)\n",
    "\n",
    "    def set_train_test(self,train_fraction = None, test_fraction = None, dense=True):\n",
    "        # Tests for create subsets\n",
    "        if train_fraction == None and test_fraction == None:\n",
    "            train_fraction = 0.8\n",
    "        elif train_fraction == None:\n",
    "            assert 0 <= test_fraction and test_fraction < 1, 'Test fraction must be in [0,1)'\n",
    "            train_fraction = 1 - test_fraction\n",
    "        else:\n",
    "            assert 0 < train_fraction and train_fraction <= 1, 'Train fraction must be in (0,1]'\n",
    "\n",
    "        train_indices = np.random.choice(self.X.shape[0], round(train_fraction*self.X.shape[0]), replace=False)\n",
    "        test_indices = np.array(list(set(range(self.X.shape[0])) - set(train_indices)))\n",
    "\n",
    "        self.X_train = self.X[train_indices]\n",
    "        self.Y_train = self.Y.reindex(train_indices)\n",
    "        self.Y_train = self.Y_train.reset_index()\n",
    "\n",
    "        self.X_test = self.X[test_indices]\n",
    "        self.Y_test = self.Y.reindex(test_indices)\n",
    "        self.Y_test = self.Y_test.reset_index()\n",
    "        if dense:\n",
    "            self.X_test_dense = self.X_test.todense()\n",
    "        return True\n",
    "\n",
    "\n",
    "Y = y_untransformed\n",
    "Y = Y.reset_index(drop=True)\n",
    "\n",
    "def remove_accents(a):\n",
    "    return unidecode.unidecode(a)\n",
    "\n",
    "data = DataObject(X,Y.apply(remove_accents))\n",
    "\n",
    "labels = [remove_accents(x) for x in list(data.Y.unique())]\n",
    "#print(labels)\n",
    "\n",
    "data.set_train_test(0.8)\n",
    "#print(data.Y_train.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### DNN graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f891fa88748>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': 500, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': './tmp'}\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(42)\n",
    "\n",
    "feature_columns = [tf.contrib.layers.real_valued_column('x', dimension=1000)]\n",
    "\n",
    "classifier = DNNClassifier(\n",
    "                           n_classes=nClasses, label_keys=labels, feature_columns=feature_columns,\n",
    "                           hidden_units=[2000, 1000, 100],\n",
    "                           dropout=0.8,\n",
    "                           weight_column_name='class_weights',\n",
    "                           #model_dir = './model-parallel/seccion_dnn',\n",
    "                           model_dir = './tmp',    \n",
    "                           config = tf.contrib.learn.RunConfig(save_checkpoints_steps = 500,\n",
    "                           save_checkpoints_secs = None)                           \n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions graph tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "# Define the test inputs\n",
    "def get_train_inputs():\n",
    "    print(\"Paso por aca.........................\")\n",
    "    X_ = data.X_train\n",
    "    Y_ = data.Y_train\n",
    "    X_ = X_.todense()\n",
    "    x = X_\n",
    "    Y_ = Y_['Seccion'].reset_index(drop=True)\n",
    "#     y = y.apply(remove_accents)\n",
    "#     y = y.values\n",
    "\n",
    "    y = Y_.values\n",
    "    \n",
    "    # computing lengths of each class\n",
    "    sizes = {label: Y_[Y_ == label].shape[0] for label in labels}\n",
    "#     print(sizes)\n",
    "\n",
    "    # creating weights for each sample\n",
    "    scale_factor = 10e3\n",
    "    weights = np.asarray([1.0/sizes[label] for label in y])\n",
    "    weights = scale_factor*weights\n",
    "    weights = weights[:,np.newaxis]\n",
    "    print('weights shape: {}'.format(weights.shape))\n",
    "    \n",
    "    dataset = tf.estimator.inputs.numpy_input_fn({'x': x, 'class_weights': weights}, y[:,np.newaxis], shuffle=True, batch_size=500, num_epochs=epochs)\n",
    "    return dataset\n",
    "\n",
    "def get_test_inputs():\n",
    "    X_ = data.X_test_dense\n",
    "    Y_ = data.Y_test\n",
    "#     X_ = X_.todense()\n",
    "    x = X_\n",
    "    Y_ = Y_['Seccion'].reset_index(drop=True)\n",
    "    y = Y_.values\n",
    "    \n",
    "    # computing lengths of each class\n",
    "    sizes = {label: Y_[Y_ == label].shape[0] for label in labels}\n",
    "#     print(sizes)\n",
    "\n",
    "    # creating weights for each sample\n",
    "    scale_factor = 10e3\n",
    "    weights = np.asarray([1.0/sizes[label] for label in y])\n",
    "    weights = scale_factor*weights\n",
    "    weights = weights[:,np.newaxis]\n",
    "    print('weights shape: {}'.format(weights.shape))\n",
    "    \n",
    "    dataset = tf.estimator.inputs.numpy_input_fn({'x': x, 'class_weights': weights}, y[:,np.newaxis], shuffle=False)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpgijwm4n0\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f891f88b320>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/tmpgijwm4n0'}\n"
     ]
    }
   ],
   "source": [
    "estimator = DNNEstimator(\n",
    "    head=tf.contrib.estimator.multi_label_head(n_classes=9),\n",
    "    feature_columns=feature_columns,\n",
    "    #feature_columns=[tf.feature_column.numeric_column(\"\", shape=[1000, 1])],\n",
    "    hidden_units=[2000, 1000, 100],\n",
    "    optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "      learning_rate=0.1,\n",
    "      l1_regularization_strength=0.001\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso por aca.........................\n",
      "weights shape: (191145, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d0c2813fae87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Input builders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_train_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 instructions)\n\u001b[0;32m--> 316\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    318\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    478\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_session_run_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks)\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_random_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m       \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_or_create_global_step_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Input builders\n",
    "\n",
    "estimator.fit(input_fn=get_train_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights shape: (47786, 1)\n",
      "Paso por aca.........................\n",
      "weights shape: (191145, 1)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt-2\n",
      "INFO:tensorflow:Saving checkpoints for 3 into ./tmp/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.9336558, step = 3\n",
      "INFO:tensorflow:Saving checkpoints for 4 into ./tmp/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.97240984.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x7f891fabecf8>, 'hidden_units': [2000, 1000, 100], 'feature_columns': (_RealValuedColumn(column_name='x', dimension=1000, default_value=None, dtype=tf.float32, normalizer=None),), 'optimizer': None, 'activation_fn': <function relu at 0x7f8a08c0d2f0>, 'dropout': 0.8, 'gradient_clip_norm': None, 'embedding_lr_multipliers': None, 'input_layer_min_slice_size': None})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    input_fn= get_test_inputs(),\n",
    "    every_n_steps=500,\n",
    "    early_stopping_metric=\"accuracy\",\n",
    "    early_stopping_rounds = 5000)\n",
    "\n",
    "classifier.fit(input_fn=get_train_inputs(), monitors=[validation_monitor], steps=epochs, max_steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_evaluate():\n",
    "    dataset = {'x': tf.constant(data.X_test_dense)}\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "pred_test = classifier.predict_classes(input_fn=input_fn_evaluate)\n",
    "\n",
    "# print(pred_test)\n",
    "\n",
    "y_test_hat = np.asarray([x.decode('UTF-8') for x in list(pred_test)])\n",
    "y_test_hat = y_test_hat.astype(str)\n",
    "y_test = data.Y_test['Seccion'].values\n",
    "y_test = y_test.astype(str)\n",
    "print('Test shape: {}\\nReal shape: {}'.format(y_test_hat.shape, y_test.shape))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_true=y_test, y_pred=y_test_hat)\n",
    "print('Accuracy in test: {}'.format(acc))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba = classifier.predict_proba(input_fn=input_fn_evaluate,as_iterable=False)\n",
    "np.sum(predict_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = skplt.plot_confusion_matrix(y_test, y_test_hat,normalize=True)\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "for tick in ax.xaxis.get_minor_ticks():\n",
    "    tick.tick1line.set_markersize(0)\n",
    "    tick.tick2line.set_markersize(0)\n",
    "    tick.label1.set_horizontalalignment('center')\n",
    "\n",
    "plt2.xticks(rotation=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so | _pywrap_tensorflow_internal\n",
      "/root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/toco/python/_tensorflow_wrap_toco.so | _tensorflow_wrap_toco\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#import _pickle as pickle\n",
    "import pickle\n",
    "import re, sys, unidecode\n",
    "#import unidecode\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import scikitplot as skplt\n",
    "\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordBag, WordHash\n",
    "from wordbatch.models import FTRL\n",
    "\n",
    "from tensorflow.contrib.learn import DNNClassifier\n",
    "\n",
    "#from tecnosmartlib import DataObject\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plt.style.use('fivethirtyeight')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoticias = pd.read_pickle(\"dfNoticias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre procesing\n",
    "\n",
    "Cleaning data, select clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting useful information...done.\n"
     ]
    }
   ],
   "source": [
    "print('Selecting useful information...', end='')\n",
    "_map = {}\n",
    "_map[\"Cuerpo\"] = []\n",
    "_map[\"Seccion\"] = []\n",
    "\n",
    "\n",
    "count = 0\n",
    "for index, row in dfNoticias.iterrows():\n",
    "    if (row[\"Seccion_1\"] != None and row[\"Cuerpo\"] != None):\n",
    "        _map[\"Cuerpo\"].append(row[\"Cuerpo\"]) \n",
    "        _map[\"Seccion\"].append(row[\"Seccion_1\"])     \n",
    "print('done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data before dropping duplicates: 250339\n",
      "Number of data after dropping duplicates : 238909\n",
      "\n",
      "Number of duplicated data : 11430\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(_map)\n",
    "\n",
    "numberOfDuplicates = df.shape[0]\n",
    "print('Number of data before dropping duplicates: {}'.format(df.shape[0]))\n",
    "df = df.drop_duplicates(inplace= False)\n",
    "df.reset_index(drop=True, inplace= True)\n",
    "numberOfDuplicates -= df.shape[0]\n",
    "print('Number of data after dropping duplicates : {}'.format(df.shape[0]))\n",
    "print('\\nNumber of duplicated data : {}'.format(numberOfDuplicates))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_untransformed = df['Cuerpo'].reset_index(drop=True)\n",
    "y_untransformed = df['Seccion'].reset_index(drop=True)\n",
    "\n",
    "def remove_accents(a):\n",
    "    return unidecode.unidecode(a)\n",
    "\n",
    "y = y_untransformed.apply(remove_accents)\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraccion\n",
    "\n",
    "### Data representation\n",
    "\n",
    "TFID calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-39e9ba811fc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspanish_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spanish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     return u\" \".join([x for x in [y for y in text.lower().strip().split(\" \")] \n\u001b[1;32m      5\u001b[0m                       if len(x) > 1 and x not in spanish_stopwords])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "def normalize_text(text):\n",
    "    return u\" \".join([x for x in [y for y in text.lower().strip().split(\" \")] \n",
    "                      if len(x) > 1 and x not in spanish_stopwords])\n",
    "\n",
    "\n",
    "calc_tfid = False\n",
    "save_file = \"tfid_features_hash20.p\"\n",
    "\n",
    "if calc_tfid :\n",
    "    start = time.time()\n",
    "    print(\"start\")\n",
    "\n",
    "\n",
    "    X_untransformed = X_untransformed\n",
    "    n_docs = X_untransformed.shape[0]\n",
    "    n_cpu = 20\n",
    "\n",
    "    batch_size = int(n_docs/n_cpu)\n",
    "    # optimo 2**20 hash , min 14\n",
    "    wb = wordbatch.WordBatch(normalize_text, \n",
    "                             extractor=(WordBag, {\"hash_ngrams\": 4, \"hash_ngrams_weights\": [-5.0, -4.0, -3.0, -2.0],\n",
    "                                                  \"hash_size\": 2**15, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "                                                  \"idf\": 1.0}), procs=n_cpu, n_words=10000, minibatch_size=batch_size)\n",
    "    wb.dictionary_freeze = True\n",
    "    word_comment = wb.fit_transform(list(X_untransformed),reset= False)\n",
    "    X_transformed = word_comment[:, np.array(np.clip(word_comment.getnnz(axis=0) - 1, 0, 1), dtype = bool)]\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"TFIDF end time :\" + str(end - start) )\n",
    "\n",
    "\n",
    "    X = X_transformed\n",
    "\n",
    "\n",
    "    #X = X.todense()\n",
    "    print('Number of features: {}'.format(X.shape[1]))\n",
    "\n",
    "    pickle.dump( X, open( save_file, \"wb\" ) )\n",
    "else:    \n",
    "    X = pickle.load( open( save_file, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(y))\n",
    "umbral = 100\n",
    "sizes = [ [label,y[y== label].shape[0]] for label in labels ]\n",
    "filter_sizes = list(filter(lambda x:x[1]>umbral ,sizes ))\n",
    "names_clases = set( map(lambda x:x[0], filter_sizes ) )\n",
    "\n",
    "index = list(map(lambda x: {x}.issubset(names_clases), y ))\n",
    "\n",
    "y = y[index]\n",
    "X = X[np.nonzero(index)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "train_fraction = 0.8\n",
    "\n",
    "train_indices = np.random.choice(X.shape[0], round(train_fraction*X.shape[0]), replace=False)\n",
    "test_indices = np.array(list(set(range(X.shape[0])) - set(train_indices)))\n",
    "\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lerko' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c84a95743ef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# weights = np.asarray([float(len(y_train))/(sizes[label]*nClasses) for label in y]) #n_samples / (n_classes * np.bincount(y))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlerko\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mscale_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10e3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lerko' is not defined"
     ]
    }
   ],
   "source": [
    "labels = list(set(y))\n",
    "labels.sort()\n",
    "nClasses = len(labels)\n",
    "\n",
    "sizes = {label: y_train[y_train == label].shape[0] for label in labels}\n",
    "weights = np.asarray([float(len(y_train))/(sizes[label]*nClasses) for label in y]) #n_samples / (n_classes * np.bincount(y))\n",
    "\n",
    "# #lerko\n",
    "# scale_factor = 10e3\n",
    "# sizes = {label: y_train[y_train == label].shape[0] for label in labels}\n",
    "# weights = np.asarray([scale_factor/sizes[label] for label in y])\n",
    "\n",
    "weights_train = weights[train_indices]\n",
    "weights_test = weights[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.sort()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                   np.unique(y_train),\n",
    "#                                                   y_train)\n",
    "# np.power(class_weights,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels\n",
    "\n",
    "y_train_new = []\n",
    "for yTmp in y_train:\n",
    "    for idx, label in enumerate(labels):\n",
    "        if yTmp==label: \n",
    "            y_train_new.append(idx)\n",
    "            \n",
    "y_test_new = []\n",
    "for yTmp in y_test:\n",
    "    for idx, label in enumerate(labels):\n",
    "        if yTmp==label: \n",
    "            y_test_new.append(idx)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### DNN graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train_new, weight= np.power(np.array(weights_train),1))\n",
    "#lgb_train = lgb.Dataset(X_train, y_train_new)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test_new, reference=lgb_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.power(np.array(weights_train),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your configurations as a dict\n",
    "# params = {\n",
    "#     'task': 'train',\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'objective': 'multiclass',\n",
    "#     'num_class':nClasses,\n",
    "#     'metric' : \"multi_logloss\",\n",
    "#     'num_leaves': 31,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'feature_fraction': 0.9,\n",
    "#     'bagging_fraction': 0.8,\n",
    "#     'bagging_freq': 5,\n",
    "#     'verbose': 1,\n",
    "#     #'\n",
    "# }\n",
    "\n",
    "# leave 31 def\n",
    "params = {'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class':nClasses,\n",
    "    'metric': 'multi_logloss',\n",
    "    'learning_rate': 0.4,\n",
    "    'min_data_in_leaf':15,       \n",
    "    'max_depth':-1,\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 1.0,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': 1,    \n",
    "    'num_threads':20}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=4,\n",
    "                )\n",
    "\n",
    "end = time.time()\n",
    "print(\"training time :\" + str(end - start) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds =  gbm.predict(X_test)\n",
    "predictions = []\n",
    "for x in preds:\n",
    "    predictions.append(np.argmax(x))\n",
    "    \n",
    "y_test_hat = predictions\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_test_new, y_test_hat,normalize=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "acc = accuracy_score(y_true=y_test_new, y_pred=y_test_hat)\n",
    "\n",
    "print('Accuracy in test: {}'.format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds =  gbm.predict(X_test)\n",
    "predictions = []\n",
    "for x in preds:\n",
    "    predictions.append(np.argmax(x))\n",
    "    \n",
    "y_test_hat = predictions\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_test_new, y_test_hat,normalize=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "acc = accuracy_score(y_true=y_test_new, y_pred=y_test_hat)\n",
    "\n",
    "print('Accuracy in test: {}'.format(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEpyna3SpgQ_"
   },
   "source": [
    "####  https://www.tensorflow.org/tutorials/sequences/text_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG_n40gFzf9s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0-dev20181220\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "### Download the Shakespeare dataset\n",
    "\n",
    "Change the following line to run this code on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "root_path = \"../\"\n",
    "path_data_clean = root_path + \"data/clean/\"\n",
    "dfNoticias = pd.read_pickle(path_data_clean + \"/dfNoticiasCleanV2.p\")\n",
    "#dfNoticias.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\n",
    "count = 0\n",
    "for index, row in dfNoticias.iterrows():    \n",
    "    if index < 800: #max aporx 3500\n",
    "        raw_text += row[\"Cuerpo\"]\n",
    "        \n",
    "text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### Read the data\n",
    "\n",
    "First, look in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aavnuByVymwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1277689 characters\n"
     ]
    }
   ],
   "source": [
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Duhg9NrUymwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "la presidenta argentina, cristina fernández, tomó juramento a los tres nuevos ministros de su gabinete que designó el pasado lunes, en su primer acto público tras regresar a las funciones de gobierno tras 40 días de reposo por una neurocirugía.\n",
      "fern\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlCgQBRVymwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### Vectorize the text\n",
    "\n",
    "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZfqhkYCymwX"
   },
   "source": [
    "Now we have an integer representation for each character. Notice that we mapped the character as indexes from 0 to `len(unique)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYyNlCNXymwY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '\"' :   3,\n",
      "  '#' :   4,\n",
      "  '$' :   5,\n",
      "  '%' :   6,\n",
      "  '&' :   7,\n",
      "  \"'\" :   8,\n",
      "  '(' :   9,\n",
      "  ')' :  10,\n",
      "  '*' :  11,\n",
      "  '+' :  12,\n",
      "  ',' :  13,\n",
      "  '-' :  14,\n",
      "  '.' :  15,\n",
      "  '/' :  16,\n",
      "  '0' :  17,\n",
      "  '1' :  18,\n",
      "  '2' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1VKcQHcymwb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\nla president' ---- characters mapped to int ---- > [ 0 45 34  1 49 51 38 52 42 37 38 47 53]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "### The prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wssHQ1oGymwe"
   },
   "source": [
    "Given a character, or a sequence of characters, what is the most probable next character? This is the task we're training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the output—the following character at each time step.\n",
    "\n",
    "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### Create training examples and targets\n",
    "\n",
    "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text. \n",
    "\n",
    "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right. \n",
    "\n",
    "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
    "\n",
    "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/envs/cpu1.13-py3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "\n",
      "l\n",
      "a\n",
      " \n",
      "p\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "The `batch` method lets us easily convert these individual characters to sequences of the desired size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4hkDU3i7ozi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\nla presidenta argentina, cristina fernández, tomó juramento a los tres nuevos ministros de su gabine'\n",
      "'te que designó el pasado lunes, en su primer acto público tras regresar a las funciones de gobierno t'\n",
      "'ras 40 días de reposo por una neurocirugía.\\nfernández, que alivió el riguroso luto que vestía desde l'\n",
      "'a muerte de su esposo, el ex presidente néstor kirchner, en 2010, había aparecido en un acto en la ca'\n",
      "'sa rosada por última vez el 4 de octubre, en vísperas de la intervención a la que se sometió por un h'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbLcIPBj_mWZ"
   },
   "source": [
    "For each sequence, duplicate and shift it to form the input and target text by using the `map` method to apply a simple function to each batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiCopyGZymwi"
   },
   "source": [
    "Print the first examples input and target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNbw-iR0ymwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '\\nla presidenta argentina, cristina fernández, tomó juramento a los tres nuevos ministros de su gabin'\n",
      "Target data: 'la presidenta argentina, cristina fernández, tomó juramento a los tres nuevos ministros de su gabine'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_33OHL3b84i0"
   },
   "source": [
    "Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the `RNN` considers the previous step context in addition to the current input character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eBu9WZG84i0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 0 ('\\n')\n",
      "  expected output: 45 ('l')\n",
      "Step    1\n",
      "  input: 45 ('l')\n",
      "  expected output: 34 ('a')\n",
      "Step    2\n",
      "  input: 34 ('a')\n",
      "  expected output: 1 (' ')\n",
      "Step    3\n",
      "  input: 1 (' ')\n",
      "  expected output: 49 ('p')\n",
      "Step    4\n",
      "  input: 49 ('p')\n",
      "  expected output: 51 ('r')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### Create training batches\n",
    "\n",
    "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2pGotuNzf-S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 64 #64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
    "\n",
    "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
    "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use a LSTM layer here.)\n",
    "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNVB-jmMEOzP"
   },
   "source": [
    "Next define a function to build the model.\n",
    "\n",
    "Use `CuDNNGRU` if running on GPU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjKZrC39ELy0"
   },
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "  rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwsrpOik5zhv"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RkA5upJIJ7W7"
   },
   "source": [
    "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-liklihood of the next character:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## Try the model\n",
    "\n",
    "Now run the model to see that it behaves as expected.\n",
    "\n",
    "First check the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-_70kKAPrPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 90) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6NzLBi4VM4o"
   },
   "source": [
    "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPGmAAXmVLGC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           23040     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3935232   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 90)            92250     \n",
      "=================================================================\n",
      "Total params: 4,050,522\n",
      "Trainable params: 4,050,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwv0gEkURfx1"
   },
   "source": [
    "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary. \n",
    "\n",
    "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0-dev20181220\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QM1Vbxs_URw5"
   },
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqFMUQc_UFgM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77, 27, 31, 44, 78, 11, 36, 43,  2, 41, 51,  8, 73, 31, 55, 72, 17,\n",
       "       16, 64, 72, 63, 21,  7, 89, 48, 41, 26, 78,  1, 73, 85, 29,  9, 51,\n",
       "       85, 26, 89, 89, 37,  3, 38, 54,  0, 49, 31, 28, 15, 45, 20, 32, 58,\n",
       "       68, 44, 75, 82, 72,  2,  1, 23, 75, 33, 68, 70, 61, 61, 20, 70,  7,\n",
       "        1, 28,  3, 56, 89, 31, 22, 32, 42,  2, 15,  7, 14, 37, 69,  9,  7,\n",
       "       48, 41, 85, 50,  8,  0, 55, 66, 88,  8, 35, 62, 83, 20, 63])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfLtsP3mUhCG"
   },
   "source": [
    "Decode these to see the text predicted by this untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWcFwPwLSo05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'iones y las conversaciones mantenidas con todas las partes creemos que esta es la mejor solución par'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'ó:[kö*cj!hr\\'í[vé0/°é\\xad4&…oh9ö í—?(r—9……d\"eu\\np[;.l3]yákñÿé! 6ñ_áæ¡¡3æ& ;\"w…[5]i!.&-dã(&oh—q\\'\\nvº”\\'bªć3\\xad'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCbHQHiaa4Ic"
   },
   "source": [
    "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trpqTWyvk0nr"
   },
   "source": [
    "### Attach an optimizer, and a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UAjbjY03eiQ4"
   },
   "source": [
    "The standard `tf.keras.losses.sparse_softmax_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions. \n",
    "\n",
    "Because our model returns logits, we need to set the `from_logits` flag. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HrXTACTdzY-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 90)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.498343\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeOXriLcymww"
   },
   "source": [
    "Configure the training procedure using the `tf.keras.Model.compile` method. We'll use `tf.train.AdamOptimizer` with default arguments and the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XBUUavgF56"
   },
   "source": [
    "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "filepath=\"weights/weights.besttf.hdf5\"\n",
    "checkpointBest = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def on_epoch_end(epoch, _):\n",
    "#     # Function invoked at end of each epoch. Prints generated text.\n",
    "#     print()\n",
    "#     print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "#     start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "#     for diversity in [0.2, 0.5, 1.0]:\n",
    "#     #for diversity in [0.5]:\n",
    "#         print('\\n\\n\\n')\n",
    "#         print('----- diversidad:', diversity)\n",
    "#         print('\\n\\n')\n",
    "\n",
    "#         generated = ''\n",
    "#         sentence = text[start_index: start_index + maxlen]\n",
    "#         #generated += sentence\n",
    "#         print('----- semilla: \"' + sentence + '\"')\n",
    "#         print('\\n\\n')\n",
    "#         sys.stdout.write(generated)\n",
    "\n",
    "#         for i in range(300):\n",
    "#             x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "#             for t, char in enumerate(sentence):\n",
    "#                 x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "#             preds = model.predict(x_pred, verbose=0)[0]\n",
    "#             next_index = sample(preds, diversity)\n",
    "#             next_char = indices_char[next_index]\n",
    "\n",
    "#             generated += next_char\n",
    "#             sentence = sentence[1:] + next_char\n",
    "\n",
    "#             sys.stdout.write(next_char)\n",
    "#             sys.stdout.flush()\n",
    "#         print()\n",
    "\n",
    "# # updatable plot\n",
    "# # a minimal example (sort of)\n",
    "# import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class PlotLosses(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"train_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()    \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxdOA-rgyGvs"
   },
   "source": [
    "To keep this quick, train the model for just 3 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yGBE2zxMMHs"
   },
   "outputs": [],
   "source": [
    "EPOCHS=60\n",
    "\n",
    "model.load_weights(\"weights/weights.besttf.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 1.2468\n",
      "Epoch 00001: loss improved from 1.39728 to 1.24649, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 429s 2s/step - loss: 1.2465\n",
      "Epoch 2/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 1.1923\n",
      "Epoch 00002: loss improved from 1.24649 to 1.19180, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 423s 2s/step - loss: 1.1918\n",
      "Epoch 3/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 1.1440\n",
      "Epoch 00003: loss improved from 1.19180 to 1.14361, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 414s 2s/step - loss: 1.1436\n",
      "Epoch 4/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 1.0998\n",
      "Epoch 00004: loss improved from 1.14361 to 1.09959, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 406s 2s/step - loss: 1.0996\n",
      "Epoch 5/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 1.0683\n",
      "Epoch 00005: loss improved from 1.09959 to 1.06795, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 406s 2s/step - loss: 1.0679\n",
      "Epoch 6/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 1.0307\n",
      "Epoch 00006: loss improved from 1.06795 to 1.03038, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 406s 2s/step - loss: 1.0304\n",
      "Epoch 7/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.9944\n",
      "Epoch 00007: loss improved from 1.03038 to 0.99411, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 403s 2s/step - loss: 0.9941\n",
      "Epoch 8/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.9606\n",
      "Epoch 00008: loss improved from 0.99411 to 0.96038, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 406s 2s/step - loss: 0.9604\n",
      "Epoch 9/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.9280\n",
      "Epoch 00009: loss improved from 0.96038 to 0.92783, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 405s 2s/step - loss: 0.9278\n",
      "Epoch 10/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.8996\n",
      "Epoch 00010: loss improved from 0.92783 to 0.89928, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 405s 2s/step - loss: 0.8993\n",
      "Epoch 11/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.8736\n",
      "Epoch 00011: loss improved from 0.89928 to 0.87340, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 405s 2s/step - loss: 0.8734\n",
      "Epoch 12/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.8485\n",
      "Epoch 00012: loss improved from 0.87340 to 0.84817, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 407s 2s/step - loss: 0.8482\n",
      "Epoch 13/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.8278\n",
      "Epoch 00013: loss improved from 0.84817 to 0.82747, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 405s 2s/step - loss: 0.8275\n",
      "Epoch 14/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.8068\n",
      "Epoch 00014: loss improved from 0.82747 to 0.80660, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 406s 2s/step - loss: 0.8066\n",
      "Epoch 15/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.7899\n",
      "Epoch 00015: loss improved from 0.80660 to 0.78958, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 406s 2s/step - loss: 0.7896\n",
      "Epoch 16/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.7747\n",
      "Epoch 00016: loss improved from 0.78958 to 0.77438, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 406s 2s/step - loss: 0.7744\n",
      "Epoch 17/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.7607\n",
      "Epoch 00017: loss improved from 0.77438 to 0.76049, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 407s 2s/step - loss: 0.7605\n",
      "Epoch 18/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.7483\n",
      "Epoch 00018: loss improved from 0.76049 to 0.74815, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 405s 2s/step - loss: 0.7481\n",
      "Epoch 19/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.7362\n",
      "Epoch 00019: loss improved from 0.74815 to 0.73600, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 407s 2s/step - loss: 0.7360\n",
      "Epoch 20/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.7249\n",
      "Epoch 00020: loss improved from 0.73600 to 0.72465, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 407s 2s/step - loss: 0.7246\n",
      "Epoch 21/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.7177\n",
      "Epoch 00021: loss improved from 0.72465 to 0.71761, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 405s 2s/step - loss: 0.7176\n",
      "Epoch 22/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.7080\n",
      "Epoch 00022: loss improved from 0.71761 to 0.70786, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 405s 2s/step - loss: 0.7079\n",
      "Epoch 23/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6979\n",
      "Epoch 00023: loss improved from 0.70786 to 0.69775, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 405s 2s/step - loss: 0.6978\n",
      "Epoch 24/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6936\n",
      "Epoch 00024: loss improved from 0.69775 to 0.69341, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 405s 2s/step - loss: 0.6934\n",
      "Epoch 25/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6844\n",
      "Epoch 00025: loss improved from 0.69341 to 0.68423, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 405s 2s/step - loss: 0.6842\n",
      "Epoch 26/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6791\n",
      "Epoch 00026: loss improved from 0.68423 to 0.67901, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 406s 2s/step - loss: 0.6790\n",
      "Epoch 27/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6743\n",
      "Epoch 00027: loss improved from 0.67901 to 0.67426, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 404s 2s/step - loss: 0.6743\n",
      "Epoch 28/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6714\n",
      "Epoch 00028: loss improved from 0.67426 to 0.67135, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 405s 2s/step - loss: 0.6713\n",
      "Epoch 29/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6641\n",
      "Epoch 00029: loss improved from 0.67135 to 0.66408, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 409s 2s/step - loss: 0.6641\n",
      "Epoch 30/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/199 [============================>.] - ETA: 2s - loss: 0.6615\n",
      "Epoch 00030: loss improved from 0.66408 to 0.66142, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 407s 2s/step - loss: 0.6614\n",
      "Epoch 31/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6541\n",
      "Epoch 00031: loss improved from 0.66142 to 0.65413, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 405s 2s/step - loss: 0.6541\n",
      "Epoch 32/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6523\n",
      "Epoch 00032: loss improved from 0.65413 to 0.65222, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 405s 2s/step - loss: 0.6522\n",
      "Epoch 33/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6495\n",
      "Epoch 00033: loss improved from 0.65222 to 0.64940, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 407s 2s/step - loss: 0.6494\n",
      "Epoch 34/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6446\n",
      "Epoch 00034: loss improved from 0.64940 to 0.64465, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 406s 2s/step - loss: 0.6446\n",
      "Epoch 35/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6423\n",
      "Epoch 00035: loss improved from 0.64465 to 0.64226, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 408s 2s/step - loss: 0.6423\n",
      "Epoch 36/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6403\n",
      "Epoch 00036: loss improved from 0.64226 to 0.64016, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 407s 2s/step - loss: 0.6402\n",
      "Epoch 37/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6363\n",
      "Epoch 00037: loss improved from 0.64016 to 0.63618, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 408s 2s/step - loss: 0.6362\n",
      "Epoch 38/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6346\n",
      "Epoch 00038: loss improved from 0.63618 to 0.63462, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 408s 2s/step - loss: 0.6346\n",
      "Epoch 39/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6296\n",
      "Epoch 00039: loss improved from 0.63462 to 0.62947, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 409s 2s/step - loss: 0.6295\n",
      "Epoch 40/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6311\n",
      "Epoch 00040: loss did not improve from 0.62947\n",
      "199/199 [==============================] - 409s 2s/step - loss: 0.6311\n",
      "Epoch 41/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6278\n",
      "Epoch 00041: loss improved from 0.62947 to 0.62776, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 409s 2s/step - loss: 0.6278\n",
      "Epoch 42/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6235\n",
      "Epoch 00042: loss improved from 0.62776 to 0.62343, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 409s 2s/step - loss: 0.6234\n",
      "Epoch 43/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6235\n",
      "Epoch 00043: loss did not improve from 0.62343\n",
      "199/199 [==============================] - 409s 2s/step - loss: 0.6235\n",
      "Epoch 44/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6201\n",
      "Epoch 00044: loss improved from 0.62343 to 0.61990, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 410s 2s/step - loss: 0.6199\n",
      "Epoch 45/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6176\n",
      "Epoch 00045: loss improved from 0.61990 to 0.61767, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 412s 2s/step - loss: 0.6177\n",
      "Epoch 46/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6179\n",
      "Epoch 00046: loss did not improve from 0.61767\n",
      "199/199 [==============================] - 411s 2s/step - loss: 0.6179\n",
      "Epoch 47/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6178\n",
      "Epoch 00047: loss improved from 0.61767 to 0.61765, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 410s 2s/step - loss: 0.6177\n",
      "Epoch 48/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6184\n",
      "Epoch 00048: loss did not improve from 0.61765\n",
      "199/199 [==============================] - 410s 2s/step - loss: 0.6184\n",
      "Epoch 49/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6179\n",
      "Epoch 00049: loss did not improve from 0.61765\n",
      "199/199 [==============================] - 410s 2s/step - loss: 0.6180\n",
      "Epoch 50/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6165\n",
      "Epoch 00050: loss improved from 0.61765 to 0.61655, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 411s 2s/step - loss: 0.6165\n",
      "Epoch 51/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6105\n",
      "Epoch 00051: loss improved from 0.61655 to 0.61040, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 410s 2s/step - loss: 0.6104\n",
      "Epoch 52/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6109\n",
      "Epoch 00052: loss did not improve from 0.61040\n",
      "199/199 [==============================] - 411s 2s/step - loss: 0.6109\n",
      "Epoch 53/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6104\n",
      "Epoch 00053: loss did not improve from 0.61040\n",
      "199/199 [==============================] - 410s 2s/step - loss: 0.6104\n",
      "Epoch 54/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6083\n",
      "Epoch 00054: loss improved from 0.61040 to 0.60835, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 410s 2s/step - loss: 0.6083\n",
      "Epoch 55/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6078\n",
      "Epoch 00055: loss improved from 0.60835 to 0.60775, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 411s 2s/step - loss: 0.6077\n",
      "Epoch 56/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6057\n",
      "Epoch 00056: loss improved from 0.60775 to 0.60583, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 413s 2s/step - loss: 0.6058\n",
      "Epoch 57/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6084\n",
      "Epoch 00057: loss did not improve from 0.60583\n",
      "199/199 [==============================] - 413s 2s/step - loss: 0.6084\n",
      "Epoch 58/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6076\n",
      "Epoch 00058: loss did not improve from 0.60583\n",
      "199/199 [==============================] - 413s 2s/step - loss: 0.6077\n",
      "Epoch 59/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6097\n",
      "Epoch 00059: loss did not improve from 0.60583\n",
      "199/199 [==============================] - 412s 2s/step - loss: 0.6097\n",
      "Epoch 60/60\n",
      "198/199 [============================>.] - ETA: 2s - loss: 0.6015\n",
      "Epoch 00060: loss improved from 0.60583 to 0.60166, saving model to weights/weights.besttf.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 413s 2s/step - loss: 0.6017\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#, plot_losses\n",
    "history = model.fit(dataset.repeat(), \n",
    "                    epochs=EPOCHS,\n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    callbacks=[checkpoint_callback , checkpointBest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G0SlodEQsdV9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIPcXllKjkdr"
   },
   "source": [
    "### Restore the latest checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyeYRiuVjodY"
   },
   "source": [
    "To keep this prediction step simple, use a batch size of 1.\n",
    "\n",
    "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built. \n",
    "\n",
    "To run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zk2WJ2-XjkGz"
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71xa6jnYVrAN"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### The prediction loop\n",
    "\n",
    "The following code block generates the text:\n",
    "\n",
    "* It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
    "\n",
    "* Get the prediction distribution of the next character using the start string and the RNN state.\n",
    "\n",
    "* Then, use a multinomial distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
    "\n",
    "* The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one word. After predicting the next word, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted words.\n",
    "\n",
    "\n",
    "\n",
    "Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # You can change the start string to experiment\n",
    "  #start_string = 'c'\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 0.7\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n",
    "\n",
    "print(generate_text(model, start_string=\"colo colo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktovv0RFhrkn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AM2Uma_-yVIq"
   },
   "source": [
    "The easiest thing you can do to improve the results it to train it for longer (try `EPOCHS=30`).\n",
    "\n",
    "You can also experiment with a different start string, or try adding another RNN layer to improve the model's accuracy, or adjusting the temperature parameter to generate more or less random predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4QwTjAM6A2O"
   },
   "source": [
    "## Advanced: Customized Training\n",
    "\n",
    "The above training procedure is simple, but does not give you much control.\n",
    "\n",
    "So now that you've seen how to run the model manually let's unpack the training loop, and implement it ourselves. This gives a starting point if, for example, to implement _curriculum learning_ to help stabilize the model's open-loop output. \n",
    "\n",
    "We will use `tf.GradientTape` to track the gradiends. You can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
    "\n",
    "The procedure works as follows:\n",
    "\n",
    "* First, initialize the RNN state. We do this by calling the `tf.keras.Model.reset_states` method.\n",
    "\n",
    "* Next, iterate over the dataset (batch by batch) and calculate the *predictions* associated with each.\n",
    "\n",
    "* Open a `tf.GradientTape`, and calculate the predictions and loss in that context.\n",
    "\n",
    "* Calculate the gradients of the loss with respect to the model variables using the `tf.GradientTape.grads` method.\n",
    "\n",
    "* Finally, take a step downwards by using the optimizer's `tf.train.Optimizer.apply_gradients` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-QDD8lKshrx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "cpu1.13-py3.6",
   "language": "python",
   "name": "cpu1.13-py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

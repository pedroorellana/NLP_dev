{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, time, string, unicodedata\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.contrib.learn import DNNClassifier\n",
    "import time\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Carga stop word\n",
    "#nltk.download('stopwords')\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# texto de prueba\n",
    "## campo \"test_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_text = 'El Banco Falabella se convertirá en el mayor emisor de tarjetas de crédito del país, después de que la Superintendencia de Bancos e Instituciones Financieras (SBIF) aprobara la integración de CMR Falabella a la compañía. La figura bajo la cual CMR se integra a Banco Falabella es la de Sociedad de Apoyo al Giro (SAG). Con esto, Banco Falabella será el mayor emisor de tarjetas de crédito del país, con una cantidad superior a los 3 millones de ellas activas, según Diario Financiero.'\n",
    "test_text = 'Un total de 916 personas murieron en acciones que involucraron a la policía y otras 2.989 fallecieron en homicidios dolosos, durante los ochos meses de intervención de la seguridad pública en Río de Janeiro, la ciudad más emblemática de Brasil, según informe divulgado este lunes por una ONG.Las muertes de miembros de la Policía y el Ejército durante el período en que ha sido implementada la intervención, también fueron retomadas en el informe, según el cual 74 agentes han fallecido entre febrero y octubre de 2018.\"Las políticas de guerra contra las drogas y los enfrentamientos como método de seguridad pública son responsables por los inaceptables números de Río de janeiro: además de las muertes de civiles y militares, casi mil muertes de civiles por acción policial\", aseguró el informe de la ONG.Los datos se recogen en un informe divulgado este martes por el Observatorio de la Intervención del Centro de Estudios de Seguridad y Ciudadanía de la Universidad brasileña Cándido Mendes, a través de las redes sociales.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### funciones y clases \"template\"\n",
    "\n",
    "Para normalizar y clasficador , calcular features TFIDF+SVD,  DNN para evular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\" Funcion de normalizacion \"\"\"    \n",
    "    # split into words\n",
    "    tokens = nltk.tokenize.word_tokenize(text,language='spanish', preserve_line=False)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]    \n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are n<<<<<<<<<<<<<<<<<<<<<\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # stop word and remove accent\n",
    "    def strip_accents(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    stop_words = set(spanish_stopwords)\n",
    "    words = [strip_accents(w) for w in words if not w in stop_words]\n",
    "#    return u\" \".join(words)\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    out = \"\"\n",
    "    for word in words:\n",
    "        out += stemmer.stem(word)+\" \"    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidf_svd:\n",
    "    def __init__(self, wordbach_trained , svd_trained,non_zero_index_feat,normalize_text):\n",
    "        \"\"\" extractor features\"\"\"\n",
    "        self.wordbach = wordbach_trained\n",
    "        self.svdT = svd_trained\n",
    "        self.non_zero_index_feat = non_zero_index_feat\n",
    "        self.normalize_text = normalize_text\n",
    "    \n",
    "    def calc(self,text):\n",
    "        \n",
    "        self.tfidf = self.wordbach.transform([self.normalize_text(text)])\n",
    "        self.tfidf = self.tfidf[:, self.non_zero_index_feat]\n",
    "        tfidf_svd = self.svdT.transform(self.tfidf)\n",
    "        return(tfidf_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_eval:\n",
    "    def __init__(self, labels, path_model,dim_vec_input):\n",
    "        \"\"\" DNN eval\"\"\"\n",
    "        self.path_model = path_model\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.nClasses = len(self.labels)\n",
    "        self.feature_columns = [tf.contrib.layers.real_valued_column('x', dimension = dim_vec_input)]\n",
    "        self.classifier = DNNClassifier(                                \n",
    "                                   n_classes=len(labels), label_keys=self.labels, \n",
    "                                   feature_columns=self.feature_columns,\n",
    "                                   hidden_units=[2000], \n",
    "                                   model_dir = self.path_model                         \n",
    "                                  )\n",
    "    def input_fn_evaluate(self):\n",
    "        input = {'x': tf.constant(self.vec_input )}    \n",
    "        return input    \n",
    "\n",
    "    def calc(self,vec_input):\n",
    "        self.vec_input = vec_input\n",
    "        #pred_test = self.classifier.predict_classes(input_fn=self.input_fn_evaluate)\n",
    "        pred_prob = self.classifier.predict_proba(input_fn=self.input_fn_evaluate)\n",
    "        pred_prob = [x for x in list(pred_prob)]\n",
    "        y_test_hat = self.labels[np.argmax(pred_prob)]\n",
    "        #y_test_hat = np.asarray([x.decode('UTF-8') for x in list(pred_test)])\n",
    "        #y_test_hat = y_test_hat.astype(str)\n",
    "        return (y_test_hat , pred_prob[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../../\"\n",
    "\n",
    "file_name = \"tfid_hash28_n10000_svd1000.p\"\n",
    "features_path = root_path + 'data/features/'\n",
    "features_path_ = features_path + \"calcFeat_\" + file_name\n",
    "path_model = root_path + 'models/test/'\n",
    "path_model_tema = root_path + 'models/seccion/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### carga info modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tema_all = pickle.load( open( path_model_tema+ \"info_model.p\", \"rb\" ) )[1]\n",
    "labels = pickle.load( open( path_model + \"info_model.p\", \"rb\" ) )[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### carga clase extractor features TFIDF+SVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb,svdT,non_zero_index_feat = pickle.load( open( features_path_, \"rb\" ) )\n",
    "\n",
    "tfidf_svd_model = tfidf_svd(wb,svdT,non_zero_index_feat,normalize_text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### carga modelo DNN seccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:378: multi_class_head (from tensorflow.contrib.learn.python.learn.estimators.head) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.contrib.estimator.*_head.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1179: BaseEstimator.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:427: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f83ee9bbe48>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '../../models/test/'}\n"
     ]
    }
   ],
   "source": [
    "sec1 = DNN_eval(labels, path_model,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evalua una entrada de test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordbags\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:678: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../../models/test/model.ckpt-14000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "end time :2.3750014305114746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mundo'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "feat_vec = tfidf_svd_model.calc(test_text)\n",
    "y_test_hat , pred_prob = sec1.calc( vec_input = feat_vec)\n",
    "\n",
    "print(\"end time :\" + str(time.time() - start) )\n",
    "y_test_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clasificador de tema\n",
    "\n",
    "carga modelo a partir de seccion predicha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f83edd46a20>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '../../models/seccion//mundo'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_model_tema_ = path_model_tema + \"/\" + y_test_hat\n",
    "labels_tema = labels_tema_all[y_test_hat]\n",
    "tema1 = DNN_eval(labels_tema, path_model_tema_,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../../models/seccion//mundo/model.ckpt-14000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "end time :0.18731451034545898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'brasil'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "y_test_hat_tema , pred_prob_tema = tema1.calc( vec_input = feat_vec)\n",
    "\n",
    "print(\"end time :\" + str(time.time() - start) )\n",
    "y_test_hat_tema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 4000x4000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seccion : mundo\n",
      "tema: brasil\n",
      "\n",
      "Un total de 916 personas murieron en acciones que involucraron a la policía y otras 2.989 fallecieron en homicidios dolosos, durante los ochos meses de intervención de la seguridad pública en Río de Janeiro, la ciudad más emblemática de Brasil, según informe divulgado este lunes por una ONG.Las muertes de miembros de la Policía y el Ejército durante el período en que ha sido implementada la intervención, también fueron retomadas en el informe, según el cual 74 agentes han fallecido entre febrero y octubre de 2018.\"Las políticas de guerra contra las drogas y los enfrentamientos como método de seguridad pública son responsables por los inaceptables números de Río de janeiro: además de las muertes de civiles y militares, casi mil muertes de civiles por acción policial\", aseguró el informe de la ONG.Los datos se recogen en un informe divulgado este martes por el Observatorio de la Intervención del Centro de Estudios de Seguridad y Ciudadanía de la Universidad brasileña Cándido Mendes, a través de las redes sociales.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.bar(np.arange(8), pred_prob)\n",
    "plt.xticks(np.arange(8), labels)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.bar(np.arange(len(labels_tema)), pred_prob_tema)\n",
    "plt.xticks(np.arange(len(labels_tema)), labels_tema)\n",
    "plt.xticks(rotation=90)\n",
    "plt.figure(figsize=(40,40))\n",
    "plt.show()\n",
    "\n",
    "print( \"seccion : \" +y_test_hat+ \"\\ntema: \"+y_test_hat_tema + \"\\n\")\n",
    "print(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

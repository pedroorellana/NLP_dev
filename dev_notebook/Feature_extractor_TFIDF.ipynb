{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so | _pywrap_tensorflow_internal\n",
      "/root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/toco/python/_tensorflow_wrap_toco.so | _tensorflow_wrap_toco\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utils\n",
    "import os, sys, re, time, gc, types, string, unicodedata, unidecode, string, warnings, inspect\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#import _pickle as pickle\n",
    "import pickle\n",
    "import re, sys, unidecode\n",
    "#import unidecode\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import scikitplot as skplt\n",
    "\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordBag, WordHash\n",
    "from wordbatch.models import FTRL\n",
    "\n",
    "from tensorflow.contrib.learn import DNNClassifier\n",
    "\n",
    "#from tecnosmartlib import DataObject\n",
    "\n",
    "#plt.style.use('fivethirtyeight')\n",
    "\n",
    "#Carga stop word\n",
    "nltk.download('stopwords')\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# punkt:  módulo contiene modelos para la tokenización de textos\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../\"\n",
    "path_data_clean = root_path + \"data/clean/\"\n",
    "path_model = root_path + 'models/'\n",
    "features_path = root_path + 'data/features/'\n",
    "model_name = \"test\"\n",
    "path_model += model_name\n",
    "\n",
    "delete_old_model = True\n",
    "if delete_old_model:\n",
    "    try:\n",
    "        os.system(\"rm -rf \"+path_model)\n",
    "        os.system(\"mkdir \"+path_model)\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoticias = pd.read_pickle(path_data_clean + \"/dfNoticiasClean.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cuerpo</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Hora</th>\n",
       "      <th>ID</th>\n",
       "      <th>Resumen</th>\n",
       "      <th>Seccion_1</th>\n",
       "      <th>Seccion_2</th>\n",
       "      <th>Seccion_3</th>\n",
       "      <th>Subtema_1</th>\n",
       "      <th>Subtema_2</th>\n",
       "      <th>Subtema_3</th>\n",
       "      <th>Tema_1</th>\n",
       "      <th>Tema_2</th>\n",
       "      <th>Tema_3</th>\n",
       "      <th>Titular</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nMarcel Granollers (44°) sorprendió a David F...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>09:57</td>\n",
       "      <td>20140929095927</td>\n",
       "      <td>\\nEl español cayó ante su compatriota Marcel G...</td>\n",
       "      <td>deportes</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>torneos atp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>tenis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nDavid Ferrer sufrió otra temprana eliminación\\n</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nEl Gobierno de Barack Obama ha enviado cuatr...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>07:03</td>\n",
       "      <td>20140929065051</td>\n",
       "      <td>\\nLas conversaciones comenzaron el año 2010 y ...</td>\n",
       "      <td>pais</td>\n",
       "      <td>mundo</td>\n",
       "      <td>mundo</td>\n",
       "      <td>eeuu</td>\n",
       "      <td>relaciones exteriores</td>\n",
       "      <td>None</td>\n",
       "      <td>relaciones exteriores</td>\n",
       "      <td>eeuu</td>\n",
       "      <td>cuba</td>\n",
       "      <td>\\nObama ha enviado cuatro solicitudes a Chile ...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nEste miércoles la Comisión Asesora Presidenc...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>11:37</td>\n",
       "      <td>20140929105234</td>\n",
       "      <td>\\nRepresentantes del sector privado acusaron q...</td>\n",
       "      <td>pais</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>isapre</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>salud</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nQuiebre en comisión presidencial de isapres ...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nWolfgang Burmann, del Team Avanti, se adjudi...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>10:05</td>\n",
       "      <td>20140929100825</td>\n",
       "      <td>\\nEl pedalero del equipo Avanti terminó en el ...</td>\n",
       "      <td>deportes</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>chilenos</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ciclismo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nWolfgang Burmann ganó el segundo clasificato...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\nEl Gobierno, a través de Sernapesca, present...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>15:47</td>\n",
       "      <td>20140929152501</td>\n",
       "      <td>\\nBachelet instruyó a los ministros de Economí...</td>\n",
       "      <td>pais</td>\n",
       "      <td>pais</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>medioambiente</td>\n",
       "      <td>region de valparaiso</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nGobierno presentó querella por derrame de pe...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Cuerpo     Fecha   Hora  \\\n",
       "1  \\nMarcel Granollers (44°) sorprendió a David F...  20140929  09:57   \n",
       "2  \\nEl Gobierno de Barack Obama ha enviado cuatr...  20140929  07:03   \n",
       "3  \\nEste miércoles la Comisión Asesora Presidenc...  20140929  11:37   \n",
       "4  \\nWolfgang Burmann, del Team Avanti, se adjudi...  20140929  10:05   \n",
       "6  \\nEl Gobierno, a través de Sernapesca, present...  20140929  15:47   \n",
       "\n",
       "               ID                                            Resumen  \\\n",
       "1  20140929095927  \\nEl español cayó ante su compatriota Marcel G...   \n",
       "2  20140929065051  \\nLas conversaciones comenzaron el año 2010 y ...   \n",
       "3  20140929105234  \\nRepresentantes del sector privado acusaron q...   \n",
       "4  20140929100825  \\nEl pedalero del equipo Avanti terminó en el ...   \n",
       "6  20140929152501  \\nBachelet instruyó a los ministros de Economí...   \n",
       "\n",
       "  Seccion_1 Seccion_2 Seccion_3    Subtema_1              Subtema_2 Subtema_3  \\\n",
       "1  deportes      None      None  torneos atp                   None      None   \n",
       "2      pais     mundo     mundo         eeuu  relaciones exteriores      None   \n",
       "3      pais      None      None       isapre                   None      None   \n",
       "4  deportes      None      None     chilenos                   None      None   \n",
       "6      pais      pais      None         None                   None      None   \n",
       "\n",
       "                  Tema_1                Tema_2 Tema_3  \\\n",
       "1                  tenis                  None   None   \n",
       "2  relaciones exteriores                  eeuu   cuba   \n",
       "3                  salud                  None   None   \n",
       "4               ciclismo                  None   None   \n",
       "6          medioambiente  region de valparaiso   None   \n",
       "\n",
       "                                             Titular         Type  \n",
       "1  \\nDavid Ferrer sufrió otra temprana eliminación\\n  fid_noticia  \n",
       "2  \\nObama ha enviado cuatro solicitudes a Chile ...  fid_noticia  \n",
       "3  \\nQuiebre en comisión presidencial de isapres ...  fid_noticia  \n",
       "4  \\nWolfgang Burmann ganó el segundo clasificato...  fid_noticia  \n",
       "6  \\nGobierno presentó querella por derrame de pe...  fid_noticia  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNoticias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre procesing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_map = {}\n",
    "_map[\"Cuerpo\"] = []\n",
    "_map[\"Seccion\"] = []\n",
    "_map[\"Subtema\"] = []\n",
    "\n",
    "count = 0\n",
    "for index, row in dfNoticias.iterrows():    \n",
    "    _map[\"Cuerpo\"].append(row[\"Cuerpo\"]) \n",
    "    _map[\"Seccion\"].append(row[\"Seccion_1\"])\n",
    "    _map[\"Tema\"].append(row[\"Tema_1\"])\n",
    "    _map[\"Subtema\"].append(row[\"Subtema_1\"])\n",
    "\n",
    "df = pd.DataFrame(_map)\n",
    "# Elimino clase corporativo, muy pocos ejemplos\n",
    "df = df[df.Seccion != \"corporativo\"]\n",
    "\n",
    "    \n",
    "X_untransformed = df['Cuerpo'].reset_index(drop=True)\n",
    "y1_untransformed = df['Seccion'].reset_index(drop=True)\n",
    "y2_untransformed = df['Tema'].reset_index(drop=True)\n",
    "y3_untransformed = df['Subtema'].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraccion\n",
    "\n",
    "### Data representation\n",
    "* Normalisacion\n",
    "* TFID calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\" Funcion de normalizacion \"\"\"    \n",
    "    # split into words\n",
    "    tokens = nltk.tokenize.word_tokenize(text,language='spanish', preserve_line=False)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]    \n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are n<<<<<<<<<<<<<<<<<<<<<\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # stop word and remove accent\n",
    "    def strip_accents(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    stop_words = set(spanish_stopwords)\n",
    "    words = [strip_accents(w) for w in words if not w in stop_words]\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    out = \"\"\n",
    "    for word in words:\n",
    "        out += stemmer.stem(word)+\" \"\n",
    "    \n",
    "    return out\n",
    "    \n",
    "#   return u\" \".join(words)\n",
    "\n",
    "# spanish_stopwords = stopwords.words('spanish')\n",
    "# def normalize_text(text):\n",
    "#     return u\" \".join([x for x in [y for y in text.lower().strip().split(\" \")] \n",
    "#                       if len(x) > 1 and x not in spanish_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "TFIDF end time :229.15021920204163\n",
      "Number of features: 1000\n"
     ]
    }
   ],
   "source": [
    " # true calcula features, false carga si ya estan calculadas 272 old time\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "print(\"start\")\n",
    "\n",
    "\n",
    "X_untransformed = X_untransformed\n",
    "n_docs = X_untransformed.shape[0]\n",
    "n_cpu = 24\n",
    "\n",
    "batch_size = int(n_docs/n_cpu)\n",
    "\n",
    "#'log', \"idf\":50.0\n",
    "wb = wordbatch.WordBatch(normalize_text, \n",
    "                         extractor=(WordBag, {\"hash_ngrams\": 1, \"hash_ngrams_weights\": [1.0, 1.0],\n",
    "                                              \"hash_size\": 2**28, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "                                              \"idf\": 1.0}), procs=n_cpu, n_words=1000, minibatch_size=batch_size)\n",
    "\n",
    "wb.dictionary_freeze = True\n",
    "word_comment = wb.fit_transform(list(X_untransformed),reset= False)\n",
    "# revisar esta normalizacion\n",
    "X_transformed = word_comment[:, np.array(np.clip(word_comment.getnnz(axis=0) - 1, 0, 1), dtype = bool)]\n",
    "\n",
    "end = time.time()\n",
    "print(\"TFIDF end time :\" + str(end - start) )\n",
    "\n",
    "X = X_transformed\n",
    "\n",
    "#X = X.todense()\n",
    "print('Number of features: {}'.format(X.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output data set train/test and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = y1_untransformed.values\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "train_fraction = 0.8\n",
    "\n",
    "train_indices = np.random.choice(X.shape[0], round(train_fraction*X.shape[0]), replace=False)\n",
    "test_indices = np.array(list(set(range(X.shape[0])) - set(train_indices)))\n",
    "\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_path_ = features_path + \"dataset_tfid_hash28_n1000.p\"\n",
    "\n",
    "pickle.dump( (X_train,y_train,X_test,y_test) , open( features_path_, \"wb\" ) )\n",
    "\n",
    "\n",
    "# with open(\"features_path\", \"wb\") as f:\n",
    "#     pickle.dump(, f)\n",
    "\n",
    "# with open(\"features_path\", \"rb\") as f:\n",
    "#     X_train,y_train,X_test,y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordbags\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x268435456 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb.transform(['asd auto casa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train,X_test,y_test = pickle.load( open( features_path_, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

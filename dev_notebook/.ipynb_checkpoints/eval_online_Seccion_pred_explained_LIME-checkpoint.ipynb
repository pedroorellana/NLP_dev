{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, time, string, unicodedata\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.contrib.learn import DNNClassifier\n",
    "import time\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from lime import lime_text\n",
    "\n",
    "#Carga stop word\n",
    "#nltk.download('stopwords')\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### funciones y clases \"template\"\n",
    "\n",
    "Para normalizar y clasficador , calcular features TFIDF+SVD,  DNN para evular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\" Funcion de normalizacion \"\"\"    \n",
    "    # split into words\n",
    "    tokens = nltk.tokenize.word_tokenize(text,language='spanish', preserve_line=False)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]    \n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are n<<<<<<<<<<<<<<<<<<<<<\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # stop word and remove accent\n",
    "    def strip_accents(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    stop_words = set(spanish_stopwords)\n",
    "    words = [strip_accents(w) for w in words if not w in stop_words]\n",
    "#    return u\" \".join(words)\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    out = \"\"\n",
    "    for word in words:\n",
    "        out += stemmer.stem(word)+\" \"    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidf_svd:\n",
    "    def __init__(self, wordbach_trained , svd_trained,non_zero_index_feat,normalize_text):\n",
    "        \"\"\" extractor features\"\"\"\n",
    "        self.wordbach = wordbach_trained\n",
    "        self.svdT = svd_trained\n",
    "        self.non_zero_index_feat = non_zero_index_feat\n",
    "        self.normalize_text = normalize_text\n",
    "    \n",
    "    def calc(self,text):\n",
    "        \n",
    "        self.tfidf = self.wordbach.transform([self.normalize_text(text)])\n",
    "        self.tfidf = self.tfidf[:, self.non_zero_index_feat]\n",
    "        tfidf_svd = self.svdT.transform(self.tfidf)\n",
    "        return(tfidf_svd)\n",
    "    \n",
    "    def calcBatch(self,texts):\n",
    "        \n",
    "        normTxts = []\n",
    "        for text in texts:\n",
    "            tmp = self.normalize_text(text)\n",
    "            normTxts.append(tmp)\n",
    "        \n",
    "        self.tfidf = self.wordbach.transform(normTxts)\n",
    "        self.tfidf = self.tfidf[:, self.non_zero_index_feat]\n",
    "        tfidf_svd = self.svdT.transform(self.tfidf)\n",
    "        return(tfidf_svd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_eval:\n",
    "    def __init__(self, labels, path_model,dim_vec_input):\n",
    "        \"\"\" DNN eval\"\"\"\n",
    "        self.path_model = path_model\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.nClasses = len(self.labels)\n",
    "        self.feature_columns = [tf.contrib.layers.real_valued_column('x', dimension = dim_vec_input)]\n",
    "        self.classifier = DNNClassifier(                                \n",
    "                                   n_classes=len(labels), label_keys=self.labels, \n",
    "                                   feature_columns=self.feature_columns,\n",
    "                                   hidden_units=[2000], \n",
    "                                   model_dir = self.path_model                         \n",
    "                                  )\n",
    "    def input_fn_evaluate(self):\n",
    "        input = {'x': tf.constant(self.vec_input )}    \n",
    "        return input    \n",
    "\n",
    "    def calc(self,vec_input):\n",
    "        self.vec_input = vec_input\n",
    "        #pred_test = self.classifier.predict_classes(input_fn=self.input_fn_evaluate)\n",
    "        pred_prob = self.classifier.predict_proba(input_fn=self.input_fn_evaluate)\n",
    "        pred_prob = [x for x in list(pred_prob)]\n",
    "        y_test_hat = self.labels[np.argmax(pred_prob)]\n",
    "        #y_test_hat = np.asarray([x.decode('UTF-8') for x in list(pred_test)])\n",
    "        #y_test_hat = y_test_hat.astype(str)\n",
    "        return (y_test_hat , pred_prob[0])\n",
    "    \n",
    "    def calcBatch(self,vec_input):\n",
    "        self.vec_input = vec_input\n",
    "        pred_prob = self.classifier.predict_proba(input_fn=self.input_fn_evaluate)\n",
    "        pred_prob = [x for x in list(pred_prob)]  \n",
    "        return (np.array(pred_prob))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../\"\n",
    "\n",
    "file_name = \"tfid_hash28_n10000_svd1000.p\"\n",
    "features_path = root_path + 'data/features/'\n",
    "features_path_ = features_path + \"calcFeat_\" + file_name\n",
    "path_model = root_path + 'models/test/info_model.p'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### carga clase extractor features TFIDF+SVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb,svdT,non_zero_index_feat = pickle.load( open( features_path_, \"rb\" ) )\n",
    "\n",
    "tfidf_svd_model = tfidf_svd(wb,svdT,non_zero_index_feat,normalize_text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### carga modelo DNN para inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:378: multi_class_head (from tensorflow.contrib.learn.python.learn.estimators.head) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.contrib.estimator.*_head.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1179: BaseEstimator.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:427: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4d64d3cc18>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '../models/test'}\n"
     ]
    }
   ],
   "source": [
    "path_model,labels = pickle.load( open( path_model, \"rb\" ) )\n",
    "\n",
    "sec1 = DNN_eval(labels, path_model,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evalua una entrada de test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordbags\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:678: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../models/test/model.ckpt-14000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "end time :2.413109302520752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'economia'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evalua \n",
    "test_text = 'El Banco Falabella se convertirá en el mayor emisor de tarjetas de crédito del país, después de que la Superintendencia de Bancos e Instituciones Financieras (SBIF) aprobara la integración de CMR Falabella a la compañía. La figura bajo la cual CMR se integra a Banco Falabella es la de Sociedad de Apoyo al Giro (SAG). Con esto, Banco Falabella será el mayor emisor de tarjetas de crédito del país, con una cantidad superior a los 3 millones de ellas activas, según Diario Financiero.'\n",
    "\n",
    "start = time.time()\n",
    "test_text = tfidf_svd_model.calc(test_text)\n",
    "y_test_hat , pred_prob = sec1.calc( vec_input = test_text)\n",
    "\n",
    "print(\"end time :\" + str(time.time() - start) )\n",
    "y_test_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(np.arange(8), pred_prob)\n",
    "plt.xticks(np.arange(8), labels)\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicciton explained LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTema2(texts):\n",
    "    outs = []\n",
    "    \n",
    "    feat_data = tfidf_svd_model.transformParalel(texts)\n",
    "    pred_prob = sec1.fitBatch( vec_input = feat_data)\n",
    "\n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

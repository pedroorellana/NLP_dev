{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so | _pywrap_tensorflow_internal\n",
      "/root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/toco/python/_tensorflow_wrap_toco.so | _tensorflow_wrap_toco\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utils\n",
    "import os, sys, re, time, gc, types, string, unicodedata, unidecode, string, warnings, inspect\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#import _pickle as pickle\n",
    "import pickle\n",
    "import re, sys, unidecode\n",
    "#import unidecode\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import scikitplot as skplt\n",
    "\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordBag, WordHash\n",
    "from wordbatch.models import FTRL\n",
    "\n",
    "from tensorflow.contrib.learn import DNNClassifier\n",
    "\n",
    "#from tecnosmartlib import DataObject\n",
    "\n",
    "#plt.style.use('fivethirtyeight')\n",
    "\n",
    "#Carga stop word\n",
    "nltk.download('stopwords')\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# punkt:  módulo contiene modelos para la tokenización de textos\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../\"\n",
    "path_data_clean = root_path + \"data/clean/\"\n",
    "path_model = root_path + 'models/'\n",
    "features_path = root_path + 'data/features/'\n",
    "model_name = \"test\"\n",
    "path_model += model_name\n",
    "\n",
    "delete_old_model = True\n",
    "if delete_old_model:\n",
    "    try:\n",
    "        os.system(\"rm -rf \"+path_model)\n",
    "        os.system(\"mkdir \"+path_model)\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoticias = pd.read_pickle(path_data_clean + \"/dfNoticiasClean.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cuerpo</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Hora</th>\n",
       "      <th>ID</th>\n",
       "      <th>Resumen</th>\n",
       "      <th>Seccion_1</th>\n",
       "      <th>Seccion_2</th>\n",
       "      <th>Seccion_3</th>\n",
       "      <th>Subtema_1</th>\n",
       "      <th>Subtema_2</th>\n",
       "      <th>Subtema_3</th>\n",
       "      <th>Tema_1</th>\n",
       "      <th>Tema_2</th>\n",
       "      <th>Tema_3</th>\n",
       "      <th>Titular</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nMarcel Granollers (44°) sorprendió a David F...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>09:57</td>\n",
       "      <td>20140929095927</td>\n",
       "      <td>\\nEl español cayó ante su compatriota Marcel G...</td>\n",
       "      <td>deportes</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>torneos atp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>tenis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nDavid Ferrer sufrió otra temprana eliminación\\n</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nEl Gobierno de Barack Obama ha enviado cuatr...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>07:03</td>\n",
       "      <td>20140929065051</td>\n",
       "      <td>\\nLas conversaciones comenzaron el año 2010 y ...</td>\n",
       "      <td>pais</td>\n",
       "      <td>mundo</td>\n",
       "      <td>mundo</td>\n",
       "      <td>eeuu</td>\n",
       "      <td>relaciones exteriores</td>\n",
       "      <td>None</td>\n",
       "      <td>relaciones exteriores</td>\n",
       "      <td>eeuu</td>\n",
       "      <td>cuba</td>\n",
       "      <td>\\nObama ha enviado cuatro solicitudes a Chile ...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nEste miércoles la Comisión Asesora Presidenc...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>11:37</td>\n",
       "      <td>20140929105234</td>\n",
       "      <td>\\nRepresentantes del sector privado acusaron q...</td>\n",
       "      <td>pais</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>isapre</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>salud</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nQuiebre en comisión presidencial de isapres ...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nWolfgang Burmann, del Team Avanti, se adjudi...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>10:05</td>\n",
       "      <td>20140929100825</td>\n",
       "      <td>\\nEl pedalero del equipo Avanti terminó en el ...</td>\n",
       "      <td>deportes</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>chilenos</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ciclismo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nWolfgang Burmann ganó el segundo clasificato...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\nEl Gobierno, a través de Sernapesca, present...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>15:47</td>\n",
       "      <td>20140929152501</td>\n",
       "      <td>\\nBachelet instruyó a los ministros de Economí...</td>\n",
       "      <td>pais</td>\n",
       "      <td>pais</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>medioambiente</td>\n",
       "      <td>region de valparaiso</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nGobierno presentó querella por derrame de pe...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Cuerpo     Fecha   Hora  \\\n",
       "1  \\nMarcel Granollers (44°) sorprendió a David F...  20140929  09:57   \n",
       "2  \\nEl Gobierno de Barack Obama ha enviado cuatr...  20140929  07:03   \n",
       "3  \\nEste miércoles la Comisión Asesora Presidenc...  20140929  11:37   \n",
       "4  \\nWolfgang Burmann, del Team Avanti, se adjudi...  20140929  10:05   \n",
       "6  \\nEl Gobierno, a través de Sernapesca, present...  20140929  15:47   \n",
       "\n",
       "               ID                                            Resumen  \\\n",
       "1  20140929095927  \\nEl español cayó ante su compatriota Marcel G...   \n",
       "2  20140929065051  \\nLas conversaciones comenzaron el año 2010 y ...   \n",
       "3  20140929105234  \\nRepresentantes del sector privado acusaron q...   \n",
       "4  20140929100825  \\nEl pedalero del equipo Avanti terminó en el ...   \n",
       "6  20140929152501  \\nBachelet instruyó a los ministros de Economí...   \n",
       "\n",
       "  Seccion_1 Seccion_2 Seccion_3    Subtema_1              Subtema_2 Subtema_3  \\\n",
       "1  deportes      None      None  torneos atp                   None      None   \n",
       "2      pais     mundo     mundo         eeuu  relaciones exteriores      None   \n",
       "3      pais      None      None       isapre                   None      None   \n",
       "4  deportes      None      None     chilenos                   None      None   \n",
       "6      pais      pais      None         None                   None      None   \n",
       "\n",
       "                  Tema_1                Tema_2 Tema_3  \\\n",
       "1                  tenis                  None   None   \n",
       "2  relaciones exteriores                  eeuu   cuba   \n",
       "3                  salud                  None   None   \n",
       "4               ciclismo                  None   None   \n",
       "6          medioambiente  region de valparaiso   None   \n",
       "\n",
       "                                             Titular         Type  \n",
       "1  \\nDavid Ferrer sufrió otra temprana eliminación\\n  fid_noticia  \n",
       "2  \\nObama ha enviado cuatro solicitudes a Chile ...  fid_noticia  \n",
       "3  \\nQuiebre en comisión presidencial de isapres ...  fid_noticia  \n",
       "4  \\nWolfgang Burmann ganó el segundo clasificato...  fid_noticia  \n",
       "6  \\nGobierno presentó querella por derrame de pe...  fid_noticia  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNoticias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre procesing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_map = {}\n",
    "_map[\"Cuerpo\"] = []\n",
    "_map[\"Seccion\"] = []\n",
    "_map[\"Subtema\"] = []\n",
    "\n",
    "count = 0\n",
    "for index, row in dfNoticias.iterrows():    \n",
    "    _map[\"Cuerpo\"].append(row[\"Cuerpo\"]) \n",
    "    _map[\"Seccion\"].append(row[\"Seccion_1\"])\n",
    "    _map[\"Subtema\"].append(row[\"Subtema_1\"])\n",
    "\n",
    "df = pd.DataFrame(_map)\n",
    "# Elimino clase corporativo, muy pocos ejemplos\n",
    "df = df[df.Seccion != \"Corporativo\"]\n",
    "\n",
    "    \n",
    "X_untransformed = df['Cuerpo'].reset_index(drop=True)\n",
    "y_untransformed = df['Seccion'].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraccion\n",
    "\n",
    "### Data representation\n",
    "* Normalisacion\n",
    "* TFID calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "\n",
    "# def normalize_text(text):\n",
    "#     \"\"\" Funcion de normalizacion \"\"\"    \n",
    "#     # split into words\n",
    "#     tokens = nltk.tokenize.word_tokenize(text,language='spanish', preserve_line=False)\n",
    "#     # convert to lower case\n",
    "#     tokens = [w.lower() for w in tokens]    \n",
    "    \n",
    "#     # remove punctuation from each word\n",
    "#     table = str.maketrans('', '', string.punctuation)\n",
    "#     stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "#     # remove remaining tokens that are n<<<<<<<<<<<<<<<<<<<<<\n",
    "#     words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "#     # stop word and remove accent\n",
    "#     def strip_accents(s):\n",
    "#         return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "#     stop_words = set(spanish_stopwords)\n",
    "#     words = [strip_accents(w) for w in words if not w in stop_words]\n",
    "    \n",
    "#     stemmer = SnowballStemmer(\"spanish\")\n",
    "#     out = \"\"\n",
    "#     for word in words:\n",
    "#         out += stemmer.stem(word)+\" \"\n",
    "    \n",
    "#     return out\n",
    "    \n",
    "#    return u\" \".join(words)\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "def normalize_text(text):\n",
    "    return u\" \".join([x for x in [y for y in text.lower().strip().split(\" \")] \n",
    "                      if len(x) > 1 and x not in spanish_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "TFIDF end time :100.97004628181458\n",
      "Number of features: 1000\n"
     ]
    }
   ],
   "source": [
    "# true calcula features, false carga si ya estan calculadas\n",
    "calc_tfid = True\n",
    "\n",
    "features_path = features_path + \"tfid_hash28_n1000.p\"\n",
    "\n",
    "if calc_tfid :\n",
    "    start = time.time()\n",
    "    print(\"start\")\n",
    "\n",
    "\n",
    "    X_untransformed = X_untransformed\n",
    "    n_docs = X_untransformed.shape[0]\n",
    "    n_cpu = 20\n",
    "\n",
    "    batch_size = int(n_docs/n_cpu)\n",
    "\n",
    "    #'log', \"idf\":50.0\n",
    "    wb = wordbatch.WordBatch(normalize_text, \n",
    "                             extractor=(WordBag, {\"hash_ngrams\": 1, \"hash_ngrams_weights\": [1.0, 1.0],\n",
    "                                                  \"hash_size\": 2**22, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "                                                  \"idf\": 1.0}), procs=n_cpu, n_words=1000, minibatch_size=batch_size)\n",
    "    wb.dictionary_freeze = True\n",
    "    word_comment = wb.fit_transform(list(X_untransformed),reset= False)\n",
    "    # revisar esta normalizacion\n",
    "    X_transformed = word_comment[:, np.array(np.clip(word_comment.getnnz(axis=0) - 1, 0, 1), dtype = bool)]\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"TFIDF end time :\" + str(end - start) )\n",
    "\n",
    "\n",
    "    X = X_transformed\n",
    "\n",
    "\n",
    "    #X = X.todense()\n",
    "    print('Number of features: {}'.format(X.shape[1]))\n",
    "\n",
    "    pickle.dump( X, open( features_path, \"wb\" ) )\n",
    "else:    \n",
    "    X = pickle.load( open( features_path, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_untransformed.values\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "train_fraction = 0.8\n",
    "\n",
    "train_indices = np.random.choice(X.shape[0], round(train_fraction*X.shape[0]), replace=False)\n",
    "test_indices = np.array(list(set(range(X.shape[0])) - set(train_indices)))\n",
    "\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(y))\n",
    "nClasses = len(labels)\n",
    "\n",
    "sizes = {label: y_train[y_train == label].shape[0] for label in labels}\n",
    "# pesos calculados segun formula sklearn\n",
    "weights = np.asarray([len(y_train)/(sizes[label]*nClasses) for label in y])\n",
    "\n",
    "weights = weights[:,np.newaxis]\n",
    "weights_train = weights[train_indices]\n",
    "weights_test = weights[test_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### DNN graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 14000\n",
    "\n",
    "# Define the test inputs\n",
    "def get_train_inputs():    \n",
    "    dataset = tf.estimator.inputs.numpy_input_fn({'x': X_train.todense(),'class_weights': weights_train},\n",
    "                                                  y_train[:,np.newaxis],\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=500,\n",
    "                                                  num_epochs=epochs)\n",
    "    return dataset\n",
    "\n",
    "def get_test_inputs():\n",
    "    dataset = tf.estimator.inputs.numpy_input_fn({'x': X_test.todense(),'class_weights': weights_test},\n",
    "                                                  y_test[:,np.newaxis],\n",
    "                                                  shuffle=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions graph tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f772e194f60>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': 500, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '../models/test'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "feature_columns = [tf.contrib.layers.real_valued_column('x', dimension=1000)]\n",
    "\n",
    "optimizer = tf.train.ProximalAdagradOptimizer(\n",
    "                              learning_rate=0.1,\n",
    "                              l1_regularization_strength= 0#0.0001\n",
    "                              )\n",
    "\n",
    "classifier = DNNClassifier(                                \n",
    "                           n_classes=len(labels), label_keys=labels, feature_columns=feature_columns,\n",
    "                           hidden_units=[2000, 1000, 100],\n",
    "                           dropout=0.5,\n",
    "                           #optimizer = optimizer,\n",
    "                           weight_column_name='class_weights',\n",
    "                           model_dir = path_model,\n",
    "                           config = tf.contrib.learn.RunConfig(save_checkpoints_steps = 500,\n",
    "                           save_checkpoints_secs = None)                           \n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "start\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ../models/test/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.1966596, step = 1\n",
      "INFO:tensorflow:global_step/sec: 22.4948\n",
      "INFO:tensorflow:loss = 2.0112884, step = 101 (4.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.4369\n",
      "INFO:tensorflow:loss = 1.826689, step = 201 (4.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.5945\n",
      "INFO:tensorflow:loss = 1.4578383, step = 301 (4.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.9909\n",
      "INFO:tensorflow:loss = 1.3649328, step = 401 (4.348 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-13-12:53:26\n",
      "INFO:tensorflow:Restoring parameters from ../models/test/model.ckpt-1\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-13-12:53:31\n",
      "INFO:tensorflow:Saving dict for global step 1: accuracy = 0.13319984, global_step = 1, loss = 2.0791676\n",
      "INFO:tensorflow:Validation (step 500): loss = 2.0791676, accuracy = 0.13319984, global_step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 501 into ../models/test/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.4949\n",
      "INFO:tensorflow:loss = 1.2345774, step = 501 (9.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.9753\n",
      "INFO:tensorflow:loss = 0.93795824, step = 601 (4.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.4669\n",
      "INFO:tensorflow:loss = 0.81203014, step = 701 (4.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.0574\n",
      "INFO:tensorflow:loss = 1.0254695, step = 801 (4.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.3513\n",
      "INFO:tensorflow:loss = 0.8513144, step = 901 (4.281 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-13-12:53:53\n",
      "INFO:tensorflow:Restoring parameters from ../models/test/model.ckpt-501\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-13-12:53:57\n",
      "INFO:tensorflow:Saving dict for global step 501: accuracy = 0.6378422, global_step = 501, loss = 1.0197191\n",
      "INFO:tensorflow:Validation (step 1000): loss = 1.0197191, accuracy = 0.6378422, global_step = 501\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into ../models/test/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.1457\n",
      "INFO:tensorflow:loss = 1.0027443, step = 1001 (8.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.3482\n",
      "INFO:tensorflow:loss = 0.80696195, step = 1101 (4.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.872\n",
      "INFO:tensorflow:loss = 0.829713, step = 1201 (4.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.7395\n",
      "INFO:tensorflow:loss = 0.7071746, step = 1301 (4.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.0317\n",
      "INFO:tensorflow:loss = 0.858702, step = 1401 (4.341 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-13-12:54:20\n",
      "INFO:tensorflow:Restoring parameters from ../models/test/model.ckpt-1001\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-13-12:54:24\n",
      "INFO:tensorflow:Saving dict for global step 1001: accuracy = 0.73413426, global_step = 1001, loss = 0.7733324\n",
      "INFO:tensorflow:Validation (step 1500): loss = 0.7733324, accuracy = 0.73413426, global_step = 1001\n",
      "INFO:tensorflow:Saving checkpoints for 1501 into ../models/test/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.8161\n",
      "INFO:tensorflow:loss = 0.8051014, step = 1501 (9.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.0954\n",
      "INFO:tensorflow:loss = 0.7356974, step = 1601 (4.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.0622\n",
      "INFO:tensorflow:loss = 0.7073109, step = 1701 (4.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.5219\n",
      "INFO:tensorflow:loss = 0.8926588, step = 1801 (4.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.7182\n",
      "INFO:tensorflow:loss = 0.6617939, step = 1901 (4.402 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-13-12:54:46\n",
      "INFO:tensorflow:Restoring parameters from ../models/test/model.ckpt-1501\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-13-12:54:50\n",
      "INFO:tensorflow:Saving dict for global step 1501: accuracy = 0.7547465, global_step = 1501, loss = 0.7199072\n",
      "INFO:tensorflow:Validation (step 2000): loss = 0.7199072, accuracy = 0.7547465, global_step = 1501\n",
      "INFO:tensorflow:Saving checkpoints for 2001 into ../models/test/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.6978\n",
      "INFO:tensorflow:loss = 0.65519875, step = 2001 (9.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.8961\n",
      "INFO:tensorflow:loss = 0.69776267, step = 2101 (4.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.193\n",
      "INFO:tensorflow:loss = 0.628036, step = 2201 (4.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.3856\n",
      "INFO:tensorflow:loss = 0.71202123, step = 2301 (4.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.2426\n",
      "INFO:tensorflow:loss = 0.6569657, step = 2401 (4.302 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-13-12:55:13\n",
      "INFO:tensorflow:Restoring parameters from ../models/test/model.ckpt-2001\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-13-12:55:17\n",
      "INFO:tensorflow:Saving dict for global step 2001: accuracy = 0.76160705, global_step = 2001, loss = 0.69809073\n",
      "INFO:tensorflow:Validation (step 2500): loss = 0.69809073, accuracy = 0.76160705, global_step = 2001\n",
      "INFO:tensorflow:Saving checkpoints for 2501 into ../models/test/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.1765\n",
      "INFO:tensorflow:loss = 0.7865208, step = 2501 (9.828 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.7784\n",
      "INFO:tensorflow:loss = 0.50214094, step = 2601 (4.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.0547\n",
      "INFO:tensorflow:loss = 0.7766391, step = 2701 (4.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.4251\n",
      "INFO:tensorflow:loss = 0.75052696, step = 2801 (4.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.8532\n",
      "INFO:tensorflow:loss = 0.5922625, step = 2901 (4.376 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-13-12:55:40\n",
      "INFO:tensorflow:Restoring parameters from ../models/test/model.ckpt-2501\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(42)\n",
    "\n",
    "    \n",
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    #input_fn= get_train_inputs(),\n",
    "    input_fn= get_test_inputs(),\n",
    "    \n",
    "    every_n_steps=500,\n",
    "    #early_stopping_metric=\"accuracy\",#loss\n",
    "    early_stopping_metric=\"loss\",\n",
    "    early_stopping_metric_minimize=True,\n",
    "    early_stopping_rounds=3000)\n",
    "\n",
    "start = time.time()\n",
    "print(\"start\")\n",
    "\n",
    "#classifier.fit(input_fn=get_train_inputs(), monitors=[validation_monitor], steps=epochs, max_steps=None)\n",
    "classifier.fit(input_fn=get_train_inputs(), monitors=[validation_monitor], steps=epochs, max_steps=None)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Training time :\" + str(end - start) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_evaluate():\n",
    "    dataset = {'x': tf.constant(X_test.todense())}    \n",
    "    return dataset\n",
    "\n",
    "pred_test = classifier.predict_classes(input_fn=input_fn_evaluate)\n",
    "y_test_hat = np.asarray([x.decode('UTF-8') for x in list(pred_test)])\n",
    "y_test_hat = y_test_hat.astype(str)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_true=y_test, y_pred=y_test_hat)\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_test_hat,normalize='True')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "#print('Accuracy in test: {}'.format(acc))\n",
    "display(Markdown('## Accuracy in test: {} '.format(acc*100)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

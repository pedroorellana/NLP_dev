{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "1000# Utils\n",
    "import os, sys, re, time, gc, types, string, warnings, inspect,  unicodedata, unidecode\n",
    "\n",
    "#import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import re, sys\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "#plt.style.use('fivethirtyeight')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Carga stop word\n",
    "nltk.download('stopwords')\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# punkt:  módulo contiene modelos para la tokenización de textos\n",
    "nltk.download('punkt')\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../\"\n",
    "path_data_clean = root_path + \"data/clean\"\n",
    "path_model = root_path + 'models/'\n",
    "features_path = root_path + 'data/features/'\n",
    "model_name = \"tfidf10000_svd1000\"\n",
    "path_model += model_name\n",
    "\n",
    "delete_old_model = True\n",
    "if delete_old_model:\n",
    "    try:\n",
    "        os.system(\"rm -rf \"+path_model)\n",
    "        os.system(\"mkdir \"+path_model)\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "falta agregar data v2 y beatiful soup para limpiar html en data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoticias = pd.read_pickle(path_data_clean + \"/dfNoticiasCleanV2.p\")\n",
    "dfNoticias.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoticias = pd.read_pickle(path_data_clean + \"/dfNoticiasCleanV2.p\")\n",
    "\n",
    "\n",
    "dfNoticias = dfNoticias.sample(frac=0.1,random_state=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoticias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre procesing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_map = {}\n",
    "_map[\"Cuerpo\"] = []\n",
    "_map[\"Seccion\"] = []\n",
    "_map[\"Tema\"] = []\n",
    "_map[\"Subtema\"] = []\n",
    "\n",
    "count = 0\n",
    "for index, row in dfNoticias.iterrows():    \n",
    "    _map[\"Cuerpo\"].append(row[\"Cuerpo\"]) \n",
    "    _map[\"Seccion\"].append(row[\"Seccion_1\"])\n",
    "    _map[\"Tema\"].append(row[\"Tema_1\"])\n",
    "    _map[\"Subtema\"].append(row[\"Subtema_1\"])\n",
    "\n",
    "df = pd.DataFrame(_map)\n",
    "# Elimino clase corporativo, muy pocos ejemplos\n",
    "df = df[df.Seccion != \"corporativo\"]\n",
    "\n",
    "    \n",
    "X_untransformed = df['Cuerpo'].reset_index(drop=True)\n",
    "y1 = df['Seccion'].reset_index(drop=True)\n",
    "y2 = df['Tema'].reset_index(drop=True)\n",
    "y3 = df['Subtema'].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secciones=y1.unique()\n",
    "dic={}\n",
    "for i,seccion in enumerate(secciones):\n",
    "    dic[seccion]=i\n",
    "labels=y1.apply(lambda x:dic[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Procesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\" Funcion de normalizacion \"\"\"    \n",
    "    # split into words\n",
    "    text = text.lower()\n",
    "    \n",
    "    tokens = nltk.tokenize.word_tokenize(text,language='spanish', preserve_line=False)\n",
    " \n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are n<<<<<<<<<<<<<<<<<<<<<\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # stop word and remove accent\n",
    "    def strip_accents(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    stop_words = set(spanish_stopwords)\n",
    "    words = [strip_accents(w) for w in words if not w in stop_words]\n",
    "    #return words\n",
    "    return u\" \".join(words)\n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# what are your inputs, and what operation do you want to \n",
    "# perform on each input. For example...\n",
    "inputs = range(10) \n",
    "\n",
    "def processInput(i):\n",
    "    return i * i\n",
    " \n",
    "num_cores = multiprocessing.cpu_count()\n",
    "     \n",
    "results = Parallel(n_jobs=num_cores)(delayed(processInput)(i) for i in inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)\n",
    "\n",
    "results = Parallel(n_jobs=num_cores, verbose = 1)(delayed(normalize_text)(i) for i in X_untransformed)\n",
    "texts = pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return( len(text.split(\" \") ) )\n",
    "\n",
    "\n",
    "counts_ = []\n",
    "for text in results:\n",
    "    counts_.append(count_words(text) )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = np.array(counts_)\n",
    "plt.hist(counts,bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutWords(text, lim):    \n",
    "    tmp = text.split(\" \")\n",
    "    if len(tmp) < lim:\n",
    "        lim = len(tmp)\n",
    "    return( u\" \".join( tmp[0:lim] ) )\n",
    "\n",
    "\n",
    "texts_cut = []\n",
    "for text in results:\n",
    "    texts_cut.append(cutWords(text,300) )\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# counts = np.array(counts_)\n",
    "# plt.hist(counts,bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_ = texts_cut\n",
    "\n",
    "counts_ = []\n",
    "for text in results_:\n",
    "    counts_.append(count_words(text) )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = np.array(counts_)\n",
    "plt.hist(counts,bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = pd.Series(results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.test.utils import common_texts, get_tmpfile\n",
    "# from gensim.models import Word2Vec\n",
    "# import gensim\n",
    "\n",
    "\n",
    "# path = get_tmpfile(\"/opt/NLP_dev/models/word2vec/sbw_vectors.bin\")\n",
    "# word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.7\n",
    "\n",
    "np.random.seed(42)\n",
    "train_indices = np.random.choice(labels.shape[0], round(train_fraction*labels.shape[0]), replace=False)\n",
    "test_indices = np.array(list(set(range(labels.shape[0])) - set(train_indices)))\n",
    "\n",
    "texts_ = texts\n",
    "\n",
    "X_train0 = texts.iloc[train_indices]\n",
    "y_train0 = labels.iloc[train_indices]\n",
    "\n",
    "X_test0 = texts.iloc[test_indices]\n",
    "y_test0 = labels.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text of the training data with keras text preprocessing functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS=133000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "\n",
    "tokenizer.fit_on_texts(texts_)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train0)\n",
    "sequences_valid=tokenizer.texts_to_sequences(X_test0)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = pad_sequences(sequences_train)\n",
    "X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n",
    "y_train = to_categorical(y_train0)\n",
    "y_val = to_categorical(y_test0)\n",
    "print('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\n",
    "print('Shape of label train and validation tensor:', y_train.shape,y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(np.power(X_train[0:900],1/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word embedding ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "#from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "#word_vectors = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "word2vec_path = \"/opt/NLP_dev/models/word2vec/sbw_vectors.bin\"\n",
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False) # TRue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without pretrained data we can just initalize embedding matrixs as: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "# from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "# from keras.layers.core import Reshape, Flatten\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.models import Model\n",
    "# from keras import regularizers\n",
    "# sequence_length = X_train.shape[1]\n",
    "# filter_sizes = [3,4,5]\n",
    "# num_filters = 100\n",
    "# drop = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# inputs = Input(shape=(sequence_length,))\n",
    "# embedding = embedding_layer(inputs)\n",
    "# reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "# conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "# conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "# conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "# maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "# maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "# maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "# merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "# flatten = Flatten()(merged_tensor)\n",
    "# reshape = Reshape((3*num_filters,))(flatten)\n",
    "# dropout = Dropout(drop)(flatten)\n",
    "# output = Dense(units=8, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "# # this creates a model that includes\n",
    "# model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "sequence_length = X_train.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.001))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.001))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.001))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=8, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "# from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "# from keras.layers.core import Reshape, Flatten\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.models import Model\n",
    "# from keras import regularizers\n",
    "\n",
    "# inputs = Input(shape=(sequence_length,))\n",
    "\n",
    "# embedded_sequences = embedding_layer(inputs)\n",
    "\n",
    "# x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "# x = MaxPooling1D(5)(x)\n",
    "# x = Conv1D(128, 5, activation='relu')(x)\n",
    "# x = MaxPooling1D(5)(x)\n",
    "# x = Conv1D(128, 5, activation='relu')(x)\n",
    "# x = MaxPooling1D(35)(x)  # global max pooling\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# preds = Dense(8, activation='softmax')(x)\n",
    "\n",
    "# model = Model(sequence_input, preds)\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsSet = list(set(labels))\n",
    "labelsSet.sort()\n",
    "nClasses = len(labelsSet)\n",
    "\n",
    "def calcWeights(y):\n",
    "    sizes = {label: y_train0[y_train0 == label].shape[0] for label in labelsSet}\n",
    "    weights = np.asarray([float(len(y_train))/(sizes[label]*nClasses) for label in y]) #n_samples / (n_classes * np.bincount(y))\n",
    "    weights = np.power(weights,1) # 1.4\n",
    "    return weights \n",
    "\n",
    "def calcWeightsL(y):\n",
    "    #lerko\n",
    "    scale_factor = 10e3\n",
    "    sizes = {label: y_train0[y_train0 == label].shape[0] for label in labelsSet}\n",
    "    weights = np.asarray([scale_factor/sizes[label] for label in y])\n",
    "    return weights \n",
    "\n",
    "weights_train = calcWeights(y_train0)\n",
    "#weights_test = calcWeights(y_test0)\n",
    "\n",
    "weights_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "sample_weights = class_weight.compute_sample_weight('balanced', y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt    \n",
    "import matplotlib.patches as mpatches  \n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ConfMat(Callback):\n",
    "    def __init__(self, x, y_true, num_classes, X_val, y_val):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.x = x\n",
    "        self.y_true = y_true\n",
    "        self.num_classes = num_classes\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        y_pred=model.predict(self.x)\n",
    "        \n",
    "        y_val_pred_hard = []    \n",
    "\n",
    "        for idx,val in enumerate(y_pred):\n",
    "            tmp = [0]*8\n",
    "            tmp[np.argmax(val)] = 1\n",
    "            y_val_pred_hard.append(tmp)    \n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for idx,val in enumerate(y_val_pred_hard):\n",
    "            y_pred.append(np.argmax(val))\n",
    "\n",
    "        for idx,val in enumerate(self.y_true):\n",
    "            y_true.append(np.argmax(val))\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        skplt.metrics.plot_confusion_matrix(y_true,y_pred,normalize='True')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        y_pred=model.predict(self.X_val)\n",
    "\n",
    "        y_val_pred_hard = []    \n",
    "\n",
    "        for idx,val in enumerate(y_pred):\n",
    "            tmp = [0]*8\n",
    "            tmp[np.argmax(val)] = 1\n",
    "            y_val_pred_hard.append(tmp)    \n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for idx,val in enumerate(y_val_pred_hard):\n",
    "            y_pred.append(np.argmax(val))\n",
    "\n",
    "        for idx,val in enumerate(self.y_val):\n",
    "            y_true.append(np.argmax(val))\n",
    "\n",
    "        \n",
    "        print(\"##### conf matrix val data\")\n",
    "        \n",
    "        skplt.metrics.plot_confusion_matrix(y_true,y_pred,normalize='True')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfMat_callback = ConfMat(X_train, y_train, 8, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\")\n",
    "\n",
    "adam = Adam(lr=1e-3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weight=weights_train,\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss'),tensorboard,ConfMat_callback]\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=512,\n",
    "          epochs=150,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val),\n",
    "          class_weight = weights_train,\n",
    "          callbacks=callbacks) # starts training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred=model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_hard = []\n",
    "\n",
    "\n",
    "for idx,val in enumerate(y_pred):\n",
    "    tmp = [0]*8\n",
    "    tmp[np.argmax(val)] = 1\n",
    "    y_val_pred_hard.append(tmp)    \n",
    "    \n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for idx,val in enumerate(y_val_pred_hard):\n",
    "    y_pred.append(np.argmax(val))\n",
    "\n",
    "for idx,val in enumerate(y_val):\n",
    "    y_true.append(np.argmax(val))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#acc = accuracy_score(y_true=y_val, y_pred=y_val_pred)\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix( y_true,y_pred,normalize='True')\n",
    "plt.xticks(rotation=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

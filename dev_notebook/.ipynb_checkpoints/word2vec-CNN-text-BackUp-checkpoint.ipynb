{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utils\n",
    "import os, sys, re, time, gc, types, string, warnings, inspect\n",
    "\n",
    "#import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import re, sys\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "#plt.style.use('fivethirtyeight')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Carga stop word\n",
    "nltk.download('stopwords')\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# punkt:  módulo contiene modelos para la tokenización de textos\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../\"\n",
    "path_data_clean = root_path + \"data/clean/\"\n",
    "path_model = root_path + 'models/'\n",
    "features_path = root_path + 'data/features/'\n",
    "model_name = \"tfidf10000_svd1000\"\n",
    "path_model += model_name\n",
    "\n",
    "delete_old_model = True\n",
    "if delete_old_model:\n",
    "    try:\n",
    "        os.system(\"rm -rf \"+path_model)\n",
    "        os.system(\"mkdir \"+path_model)\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "falta agregar data v2 y beatiful soup para limpiar html en data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoticias = pd.read_pickle(path_data_clean + \"/dfNoticiasCleanV2.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cuerpo</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Hora</th>\n",
       "      <th>ID</th>\n",
       "      <th>Resumen</th>\n",
       "      <th>Seccion_1</th>\n",
       "      <th>Seccion_2</th>\n",
       "      <th>Seccion_3</th>\n",
       "      <th>Subtema_1</th>\n",
       "      <th>Subtema_2</th>\n",
       "      <th>Subtema_3</th>\n",
       "      <th>Tema_1</th>\n",
       "      <th>Tema_2</th>\n",
       "      <th>Tema_3</th>\n",
       "      <th>Titular</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nLa presidenta argentina, Cristina Fernández,...</td>\n",
       "      <td>2013-11-20</td>\n",
       "      <td>21:37</td>\n",
       "      <td>20131120214139</td>\n",
       "      <td>\\nEste fue el primer acto público de la mandat...</td>\n",
       "      <td>mundo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>argentina</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nPresidenta argentina tomó juramento a tres n...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nPor problemas de agenda de la jueza del Trib...</td>\n",
       "      <td>2013-11-20</td>\n",
       "      <td>17:09</td>\n",
       "      <td>20131120164254</td>\n",
       "      <td>\\nAudiencia se realizará el próximo lunes 2 de...</td>\n",
       "      <td>pais</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>region de la araucania</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nNuevamente se postergó preparación del juici...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nEl presidente uruguayo, José Mujica, sufre u...</td>\n",
       "      <td>2013-11-20</td>\n",
       "      <td>12:45</td>\n",
       "      <td>20131120124736</td>\n",
       "      <td>\\nPresidente uruguayo está afectado por un \"es...</td>\n",
       "      <td>mundo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>uruguay</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nResfrío que afecta a José Mujica obligó a su...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nEl Reino Unido considera que el Ejecutivo es...</td>\n",
       "      <td>2013-11-20</td>\n",
       "      <td>11:43</td>\n",
       "      <td>20131120114123</td>\n",
       "      <td>\\nGobierno hispano ha ejercido medidas de pres...</td>\n",
       "      <td>mundo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>europa</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nReino Unido arremete contra España ante incu...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\nFuncionarios municipales de Santiago y otras...</td>\n",
       "      <td>2013-11-20</td>\n",
       "      <td>12:44</td>\n",
       "      <td>20131120124031</td>\n",
       "      <td>\\nUfemuch desconoce acuerdo con el Gobierno pa...</td>\n",
       "      <td>pais</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>gremios</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>trabajo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nFuncionarios municipales de Santiago siguen ...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Cuerpo       Fecha   Hora  \\\n",
       "0  \\nLa presidenta argentina, Cristina Fernández,...  2013-11-20  21:37   \n",
       "1  \\nPor problemas de agenda de la jueza del Trib...  2013-11-20  17:09   \n",
       "2  \\nEl presidente uruguayo, José Mujica, sufre u...  2013-11-20  12:45   \n",
       "4  \\nEl Reino Unido considera que el Ejecutivo es...  2013-11-20  11:43   \n",
       "5  \\nFuncionarios municipales de Santiago y otras...  2013-11-20  12:44   \n",
       "\n",
       "               ID                                            Resumen  \\\n",
       "0  20131120214139  \\nEste fue el primer acto público de la mandat...   \n",
       "1  20131120164254  \\nAudiencia se realizará el próximo lunes 2 de...   \n",
       "2  20131120124736  \\nPresidente uruguayo está afectado por un \"es...   \n",
       "4  20131120114123  \\nGobierno hispano ha ejercido medidas de pres...   \n",
       "5  20131120124031  \\nUfemuch desconoce acuerdo con el Gobierno pa...   \n",
       "\n",
       "  Seccion_1 Seccion_2 Seccion_3 Subtema_1 Subtema_2 Subtema_3  \\\n",
       "0     mundo      None      None      None      None      None   \n",
       "1      pais      None      None      None      None      None   \n",
       "2     mundo      None      None      None      None      None   \n",
       "4     mundo      None      None      None      None      None   \n",
       "5      pais      None      None   gremios      None      None   \n",
       "\n",
       "                   Tema_1 Tema_2 Tema_3  \\\n",
       "0               argentina   None   None   \n",
       "1  region de la araucania   None   None   \n",
       "2                 uruguay   None   None   \n",
       "4                  europa   None   None   \n",
       "5                 trabajo   None   None   \n",
       "\n",
       "                                             Titular         Type  \n",
       "0  \\nPresidenta argentina tomó juramento a tres n...  fid_noticia  \n",
       "1  \\nNuevamente se postergó preparación del juici...  fid_noticia  \n",
       "2  \\nResfrío que afecta a José Mujica obligó a su...  fid_noticia  \n",
       "4  \\nReino Unido arremete contra España ante incu...  fid_noticia  \n",
       "5  \\nFuncionarios municipales de Santiago siguen ...  fid_noticia  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNoticias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre procesing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_map = {}\n",
    "_map[\"Cuerpo\"] = []\n",
    "_map[\"Seccion\"] = []\n",
    "_map[\"Tema\"] = []\n",
    "_map[\"Subtema\"] = []\n",
    "\n",
    "count = 0\n",
    "for index, row in dfNoticias.iterrows():    \n",
    "    _map[\"Cuerpo\"].append(row[\"Cuerpo\"]) \n",
    "    _map[\"Seccion\"].append(row[\"Seccion_1\"])\n",
    "    _map[\"Tema\"].append(row[\"Tema_1\"])\n",
    "    _map[\"Subtema\"].append(row[\"Subtema_1\"])\n",
    "\n",
    "df = pd.DataFrame(_map)\n",
    "# Elimino clase corporativo, muy pocos ejemplos\n",
    "df = df[df.Seccion != \"corporativo\"]\n",
    "\n",
    "    \n",
    "X_untransformed = df['Cuerpo'].reset_index(drop=True)\n",
    "y1 = df['Seccion'].reset_index(drop=True)\n",
    "y2 = df['Tema'].reset_index(drop=True)\n",
    "y3 = df['Subtema'].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraccion\n",
    "\n",
    "### Data representation\n",
    "* Normalisacion\n",
    "* TFID calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\" Funcion de normalizacion \"\"\"    \n",
    "    # split into words\n",
    "    tokens = nltk.tokenize.word_tokenize(text,language='spanish', preserve_line=False)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]    \n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are n<<<<<<<<<<<<<<<<<<<<<\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # stop word and remove accent\n",
    "    def strip_accents(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    stop_words = set(spanish_stopwords)\n",
    "    words = [strip_accents(w) for w in words if not w in stop_words]\n",
    "    return words\n",
    "#    return u\" \".join(words)\n",
    "    \n",
    "#     stemmer = SnowballStemmer(\"spanish\")\n",
    "#     out = \"\"\n",
    "#     for word in words:\n",
    "#         out += stemmer.stem(word)+\" \"    \n",
    "#     return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "X_new=[]\n",
    "lengths = []\n",
    "for idx, sample in enumerate(X_sample):\n",
    "    tmp = normalize_text(sample)\n",
    "    X_new.append(tmp)\n",
    "    all_words.extend(tmp)\n",
    "    lengths.append(len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = sorted(list(set(all_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "#load_word2vec_format()\n",
    "\n",
    "path = get_tmpfile(\"/opt/NLP_dev/models/word2vec/sbw_vectors.bin\")\n",
    "#Word2Vec.load(path)\n",
    "\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tensorflow_cpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = max(lengths)\n",
    "VOCAB_SIZE = len(VOCAB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_embedding = {}\n",
    "for idx, word in enumerate(VOCAB):\n",
    "    try :\n",
    "        hash_embedding[word] = word2vec_model.get_vector(word)\n",
    "    except :\n",
    "        hash_embedding[word] = np.random.rand(EMBEDDING_DIM)\n",
    "\n",
    "len(hash_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "tensorflow_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

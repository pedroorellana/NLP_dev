{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import os, sys, re, time, gc, types, string, unicodedata, unidecode, string, warnings, inspect\n",
    "\n",
    "#import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import re, sys, unidecode\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordBag, WordHash\n",
    "from wordbatch.models import FTRL\n",
    "\n",
    "#plt.style.use('fivethirtyeight')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Carga stop word\n",
    "nltk.download('stopwords')\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# punkt:  módulo contiene modelos para la tokenización de textos\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../\"\n",
    "path_data_clean = root_path + \"data/clean/\"\n",
    "path_model = root_path + 'models/'\n",
    "features_path = root_path + 'data/features/'\n",
    "model_name = \"tfidf10000_svd1000\"\n",
    "path_model += model_name\n",
    "\n",
    "delete_old_model = True\n",
    "if delete_old_model:\n",
    "    try:\n",
    "        os.system(\"rm -rf \"+path_model)\n",
    "        os.system(\"mkdir \"+path_model)\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "falta agregar data v2 y beatiful soup para limpiar html en data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoticias = pd.read_pickle(path_data_clean + \"/dfNoticiasCleanV2.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cuerpo</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Hora</th>\n",
       "      <th>ID</th>\n",
       "      <th>Resumen</th>\n",
       "      <th>Seccion_1</th>\n",
       "      <th>Seccion_2</th>\n",
       "      <th>Seccion_3</th>\n",
       "      <th>Subtema_1</th>\n",
       "      <th>Subtema_2</th>\n",
       "      <th>Subtema_3</th>\n",
       "      <th>Tema_1</th>\n",
       "      <th>Tema_2</th>\n",
       "      <th>Tema_3</th>\n",
       "      <th>Titular</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nLa presidenta argentina, Cristina Fernández,...</td>\n",
       "      <td>2013-11-20</td>\n",
       "      <td>21:37</td>\n",
       "      <td>20131120214139</td>\n",
       "      <td>\\nEste fue el primer acto público de la mandat...</td>\n",
       "      <td>mundo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>argentina</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nPresidenta argentina tomó juramento a tres n...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nPor problemas de agenda de la jueza del Trib...</td>\n",
       "      <td>2013-11-20</td>\n",
       "      <td>17:09</td>\n",
       "      <td>20131120164254</td>\n",
       "      <td>\\nAudiencia se realizará el próximo lunes 2 de...</td>\n",
       "      <td>pais</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>region de la araucania</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nNuevamente se postergó preparación del juici...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nEl presidente uruguayo, José Mujica, sufre u...</td>\n",
       "      <td>2013-11-20</td>\n",
       "      <td>12:45</td>\n",
       "      <td>20131120124736</td>\n",
       "      <td>\\nPresidente uruguayo está afectado por un \"es...</td>\n",
       "      <td>mundo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>uruguay</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nResfrío que afecta a José Mujica obligó a su...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nEl Reino Unido considera que el Ejecutivo es...</td>\n",
       "      <td>2013-11-20</td>\n",
       "      <td>11:43</td>\n",
       "      <td>20131120114123</td>\n",
       "      <td>\\nGobierno hispano ha ejercido medidas de pres...</td>\n",
       "      <td>mundo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>europa</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nReino Unido arremete contra España ante incu...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\nFuncionarios municipales de Santiago y otras...</td>\n",
       "      <td>2013-11-20</td>\n",
       "      <td>12:44</td>\n",
       "      <td>20131120124031</td>\n",
       "      <td>\\nUfemuch desconoce acuerdo con el Gobierno pa...</td>\n",
       "      <td>pais</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>gremios</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>trabajo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nFuncionarios municipales de Santiago siguen ...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Cuerpo       Fecha   Hora  \\\n",
       "0  \\nLa presidenta argentina, Cristina Fernández,...  2013-11-20  21:37   \n",
       "1  \\nPor problemas de agenda de la jueza del Trib...  2013-11-20  17:09   \n",
       "2  \\nEl presidente uruguayo, José Mujica, sufre u...  2013-11-20  12:45   \n",
       "4  \\nEl Reino Unido considera que el Ejecutivo es...  2013-11-20  11:43   \n",
       "5  \\nFuncionarios municipales de Santiago y otras...  2013-11-20  12:44   \n",
       "\n",
       "               ID                                            Resumen  \\\n",
       "0  20131120214139  \\nEste fue el primer acto público de la mandat...   \n",
       "1  20131120164254  \\nAudiencia se realizará el próximo lunes 2 de...   \n",
       "2  20131120124736  \\nPresidente uruguayo está afectado por un \"es...   \n",
       "4  20131120114123  \\nGobierno hispano ha ejercido medidas de pres...   \n",
       "5  20131120124031  \\nUfemuch desconoce acuerdo con el Gobierno pa...   \n",
       "\n",
       "  Seccion_1 Seccion_2 Seccion_3 Subtema_1 Subtema_2 Subtema_3  \\\n",
       "0     mundo      None      None      None      None      None   \n",
       "1      pais      None      None      None      None      None   \n",
       "2     mundo      None      None      None      None      None   \n",
       "4     mundo      None      None      None      None      None   \n",
       "5      pais      None      None   gremios      None      None   \n",
       "\n",
       "                   Tema_1 Tema_2 Tema_3  \\\n",
       "0               argentina   None   None   \n",
       "1  region de la araucania   None   None   \n",
       "2                 uruguay   None   None   \n",
       "4                  europa   None   None   \n",
       "5                 trabajo   None   None   \n",
       "\n",
       "                                             Titular         Type  \n",
       "0  \\nPresidenta argentina tomó juramento a tres n...  fid_noticia  \n",
       "1  \\nNuevamente se postergó preparación del juici...  fid_noticia  \n",
       "2  \\nResfrío que afecta a José Mujica obligó a su...  fid_noticia  \n",
       "4  \\nReino Unido arremete contra España ante incu...  fid_noticia  \n",
       "5  \\nFuncionarios municipales de Santiago siguen ...  fid_noticia  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNoticias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre procesing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_map = {}\n",
    "_map[\"Cuerpo\"] = []\n",
    "_map[\"Seccion\"] = []\n",
    "_map[\"Tema\"] = []\n",
    "_map[\"Subtema\"] = []\n",
    "\n",
    "count = 0\n",
    "for index, row in dfNoticias.iterrows():    \n",
    "    _map[\"Cuerpo\"].append(row[\"Cuerpo\"]) \n",
    "    _map[\"Seccion\"].append(row[\"Seccion_1\"])\n",
    "    _map[\"Tema\"].append(row[\"Tema_1\"])\n",
    "    _map[\"Subtema\"].append(row[\"Subtema_1\"])\n",
    "\n",
    "df = pd.DataFrame(_map)\n",
    "# Elimino clase corporativo, muy pocos ejemplos\n",
    "df = df[df.Seccion != \"corporativo\"]\n",
    "\n",
    "    \n",
    "X_untransformed = df['Cuerpo'].reset_index(drop=True)\n",
    "y1 = df['Seccion'].reset_index(drop=True)\n",
    "y2 = df['Tema'].reset_index(drop=True)\n",
    "y3 = df['Subtema'].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraccion\n",
    "\n",
    "### Data representation\n",
    "* Normalisacion\n",
    "* TFID calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\" Funcion de normalizacion \"\"\"    \n",
    "    # split into words\n",
    "    tokens = nltk.tokenize.word_tokenize(text,language='spanish', preserve_line=False)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]    \n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are n<<<<<<<<<<<<<<<<<<<<<\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # stop word and remove accent\n",
    "    def strip_accents(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    stop_words = set(spanish_stopwords)\n",
    "    words = [strip_accents(w) for w in words if not w in stop_words]\n",
    "    return u\" \".join(words)\n",
    "    \n",
    "#     stemmer = SnowballStemmer(\"spanish\")\n",
    "#     out = \"\"\n",
    "#     for word in words:\n",
    "#         out += stemmer.stem(word)+\" \"    \n",
    "#     return out\n",
    "\n",
    "\n",
    "\n",
    "# spanish_stopwords = stopwords.words('spanish')\n",
    "# def normalize_text(text):\n",
    "#     return u\" \".join([x for x in [y for y in text.lower().strip().split(\" \")] \n",
    "#                       if len(x) > 1 and x not in spanish_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Normalize text\n",
      "Parallelization fail. Method: multiprocessing Task: <function batch_normalize_texts at 0x7fbcd1df3510>\n",
      "Retrying, attempt: 1 timeout limit: 1200 seconds\n",
      "Parallelization fail. Method: multiprocessing Task: <function batch_normalize_texts at 0x7fbcd1df3510>\n",
      "Retrying, attempt: 2 timeout limit: 2400 seconds\n",
      "Parallelization fail. Method: multiprocessing Task: <function batch_normalize_texts at 0x7fbcd1df3510>\n",
      "Retrying, attempt: 3 timeout limit: 4800 seconds\n",
      "Parallelization fail. Method: multiprocessing Task: <function batch_normalize_texts at 0x7fbcd1df3510>\n",
      "Retrying, attempt: 4 timeout limit: 9600 seconds\n",
      "Parallelization fail. Method: multiprocessing Task: <function batch_normalize_texts at 0x7fbcd1df3510>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f58c34f73e16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary_freeze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mX_transformed_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_untransformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mnon_zero_index_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_transformed_zero\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/Wordbatch-1.3.4-py3.5-linux-x86_64.egg/wordbatch/wordbatch.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, texts, extractor, cache_features, input_split, reset)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/Wordbatch-1.3.4-py3.5-linux-x86_64.egg/wordbatch/wordbatch.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, texts, extractor, cache_features, input_split, reset, update)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_output\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/Wordbatch-1.3.4-py3.5-linux-x86_64.egg/wordbatch/wordbatch.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, texts, input_split, reset, update)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len(self.raw_dft):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_dft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"len(self.dft):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/Wordbatch-1.3.4-py3.5-linux-x86_64.egg/wordbatch/wordbatch.py\u001b[0m in \u001b[0;36mupdate_dictionary\u001b[0;34m(self, texts, dft, dictionary, min_df, input_split)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m#Update document frequencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         dfts2= self.parallelize_batches(batch_get_dfs, texts, [], input_split= input_split,\n\u001b[0;32m--> 148\u001b[0;31m                                         merge_output=False)\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_sc\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mdfts2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfts2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_count\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdft2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWB_DOC_CNT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdft2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfts2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/Wordbatch-1.3.4-py3.5-linux-x86_64.egg/wordbatch/wordbatch.py\u001b[0m in \u001b[0;36mparallelize_batches\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallelize_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msplit_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/Wordbatch-1.3.4-py3.5-linux-x86_64.egg/wordbatch/batcher.py\u001b[0m in \u001b[0;36mparallelize_batches\u001b[0;34m(self, task, data, args, method, timeout, rdd_col, input_split, merge_output, minibatch_size, procs)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mparal_params\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mparal_params\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"serial\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparal_params\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "print(\"start\")\n",
    "\n",
    "\n",
    "X_untransformed = X_untransformed\n",
    "n_docs = X_untransformed.shape[0]\n",
    "n_cpu = 40\n",
    "#n_cpu = 6\n",
    "\n",
    "batch_size = int(n_docs/n_cpu)\n",
    "\n",
    "#'log', \"idf\":50.0\n",
    "\n",
    "wb = wordbatch.WordBatch(normalize_text, \n",
    "                         extractor=(WordBag, {\"hash_ngrams\": 1, \"hash_ngrams_weights\": [1.0, 1.0],\n",
    "                                              \"hash_size\": 2**28, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "                                              \"idf\": 1.0}), procs=n_cpu, n_words=10000, minibatch_size=batch_size)\n",
    "                                              #\"idf\": 1.0}), procs=n_cpu, n_words=1000, minibatch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# wb = wordbatch.WordBatch(normalize_text, \n",
    "#                          extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.0, 1.0],\n",
    "#                                               \"hash_size\": 2**28, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "#                                               \"idf\": 1.0}), procs=n_cpu, n_words=10000, minibatch_size=batch_size)\n",
    "\n",
    "# add method=\"serial\" to extractor for debug normalize function\n",
    "\n",
    "wb.dictionary_freeze = True\n",
    "X_transformed_zero = wb.fit_transform(list(X_untransformed),reset= False)\n",
    "\n",
    "non_zero_index_feat = np.array(np.clip(X_transformed_zero.getnnz(axis=0) - 1, 0, 1), dtype = bool)\n",
    "X_transformed = X_transformed_zero[:, non_zero_index_feat]\n",
    "\n",
    "print(\"TFIDF end time :\" + str(time.time() - start) )\n",
    "\n",
    "X = X_transformed\n",
    "\n",
    "#X = X.todense()\n",
    "print('Number of features: {}'.format(X.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD\n",
    "\n",
    "\n",
    "Dim redution, libreria con implementacion multicore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_enable = True\n",
    "new_dim = 1000\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if svd_enable :\n",
    "    start = time.time()\n",
    "    svdT = TruncatedSVD(n_components = new_dim)\n",
    "    X = svdT.fit_transform(X)\n",
    "    print(\"end time :\" + str(time.time() - start) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data\n",
    "\n",
    "### save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features = False \n",
    "split_data = False\n",
    "file_name = \"tfid_hash28_n10000_svd1000_sin_stemmig.p\"\n",
    "train_fraction = 0.8\n",
    "\n",
    "if split_data and save_features :\n",
    "    y1 = y1.values    \n",
    "\n",
    "    np.random.seed(42)\n",
    "    train_indices = np.random.choice(X.shape[0], round(train_fraction*X.shape[0]), replace=False)\n",
    "    test_indices = np.array(list(set(range(X.shape[0])) - set(train_indices)))\n",
    "\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y1[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y1[test_indices]\n",
    "    features_path_ = features_path + \"dataTrainTest_\" + file_name\n",
    "    pickle.dump( (X_train,y_train,X_test,y_test) , open( features_path_, \"wb\" ) )\n",
    "elif save_features:\n",
    "    features_path_ = features_path + \"dataV2_\" + file_name\n",
    "    pickle.dump( (X,y1,y2,y3) , open( features_path_, \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save models\n",
    "* wordabatch calculator\n",
    "* svd transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models = False\n",
    "if save_models:\n",
    "    file_name = \"tfid_hash28_n10000_svd1000_sin_stemmig.p\"\n",
    "    features_path_ = features_path + \"calcFeat_dataV2_\" + file_name\n",
    "    pickle.dump( (wb,svdT,non_zero_index_feat), open( features_path_, \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

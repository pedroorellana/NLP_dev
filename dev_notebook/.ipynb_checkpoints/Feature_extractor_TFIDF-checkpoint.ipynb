{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os, sys, re, time, gc, types, string, unicodedata, unidecode, string, warnings, inspect\n",
    "\n",
    "#import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import re, sys, unidecode\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordBag, WordHash\n",
    "from wordbatch.models import FTRL\n",
    "\n",
    "#plt.style.use('fivethirtyeight')\n",
    "\n",
    "#Carga stop word\n",
    "#nltk.download('stopwords')\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# punkt:  módulo contiene modelos para la tokenización de textos\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../\"\n",
    "path_data_clean = root_path + \"data/clean/\"\n",
    "path_model = root_path + 'models/'\n",
    "features_path = root_path + 'data/features/'\n",
    "model_name = \"tfidf10000_svd1000\"\n",
    "path_model += model_name\n",
    "\n",
    "delete_old_model = True\n",
    "if delete_old_model:\n",
    "    try:\n",
    "        os.system(\"rm -rf \"+path_model)\n",
    "        os.system(\"mkdir \"+path_model)\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoticias = pd.read_pickle(path_data_clean + \"/dfNoticiasClean.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cuerpo</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Hora</th>\n",
       "      <th>ID</th>\n",
       "      <th>Resumen</th>\n",
       "      <th>Seccion_1</th>\n",
       "      <th>Seccion_2</th>\n",
       "      <th>Seccion_3</th>\n",
       "      <th>Subtema_1</th>\n",
       "      <th>Subtema_2</th>\n",
       "      <th>Subtema_3</th>\n",
       "      <th>Tema_1</th>\n",
       "      <th>Tema_2</th>\n",
       "      <th>Tema_3</th>\n",
       "      <th>Titular</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nMarcel Granollers (44°) sorprendió a David F...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>09:57</td>\n",
       "      <td>20140929095927</td>\n",
       "      <td>\\nEl español cayó ante su compatriota Marcel G...</td>\n",
       "      <td>deportes</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>torneos atp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>tenis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nDavid Ferrer sufrió otra temprana eliminación\\n</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nEl Gobierno de Barack Obama ha enviado cuatr...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>07:03</td>\n",
       "      <td>20140929065051</td>\n",
       "      <td>\\nLas conversaciones comenzaron el año 2010 y ...</td>\n",
       "      <td>pais</td>\n",
       "      <td>mundo</td>\n",
       "      <td>mundo</td>\n",
       "      <td>eeuu</td>\n",
       "      <td>relaciones exteriores</td>\n",
       "      <td>None</td>\n",
       "      <td>relaciones exteriores</td>\n",
       "      <td>eeuu</td>\n",
       "      <td>cuba</td>\n",
       "      <td>\\nObama ha enviado cuatro solicitudes a Chile ...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nEste miércoles la Comisión Asesora Presidenc...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>11:37</td>\n",
       "      <td>20140929105234</td>\n",
       "      <td>\\nRepresentantes del sector privado acusaron q...</td>\n",
       "      <td>pais</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>isapre</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>salud</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nQuiebre en comisión presidencial de isapres ...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nWolfgang Burmann, del Team Avanti, se adjudi...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>10:05</td>\n",
       "      <td>20140929100825</td>\n",
       "      <td>\\nEl pedalero del equipo Avanti terminó en el ...</td>\n",
       "      <td>deportes</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>chilenos</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ciclismo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nWolfgang Burmann ganó el segundo clasificato...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\nEl Gobierno, a través de Sernapesca, present...</td>\n",
       "      <td>20140929</td>\n",
       "      <td>15:47</td>\n",
       "      <td>20140929152501</td>\n",
       "      <td>\\nBachelet instruyó a los ministros de Economí...</td>\n",
       "      <td>pais</td>\n",
       "      <td>pais</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>medioambiente</td>\n",
       "      <td>region de valparaiso</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nGobierno presentó querella por derrame de pe...</td>\n",
       "      <td>fid_noticia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Cuerpo     Fecha   Hora  \\\n",
       "1  \\nMarcel Granollers (44°) sorprendió a David F...  20140929  09:57   \n",
       "2  \\nEl Gobierno de Barack Obama ha enviado cuatr...  20140929  07:03   \n",
       "3  \\nEste miércoles la Comisión Asesora Presidenc...  20140929  11:37   \n",
       "4  \\nWolfgang Burmann, del Team Avanti, se adjudi...  20140929  10:05   \n",
       "6  \\nEl Gobierno, a través de Sernapesca, present...  20140929  15:47   \n",
       "\n",
       "               ID                                            Resumen  \\\n",
       "1  20140929095927  \\nEl español cayó ante su compatriota Marcel G...   \n",
       "2  20140929065051  \\nLas conversaciones comenzaron el año 2010 y ...   \n",
       "3  20140929105234  \\nRepresentantes del sector privado acusaron q...   \n",
       "4  20140929100825  \\nEl pedalero del equipo Avanti terminó en el ...   \n",
       "6  20140929152501  \\nBachelet instruyó a los ministros de Economí...   \n",
       "\n",
       "  Seccion_1 Seccion_2 Seccion_3    Subtema_1              Subtema_2 Subtema_3  \\\n",
       "1  deportes      None      None  torneos atp                   None      None   \n",
       "2      pais     mundo     mundo         eeuu  relaciones exteriores      None   \n",
       "3      pais      None      None       isapre                   None      None   \n",
       "4  deportes      None      None     chilenos                   None      None   \n",
       "6      pais      pais      None         None                   None      None   \n",
       "\n",
       "                  Tema_1                Tema_2 Tema_3  \\\n",
       "1                  tenis                  None   None   \n",
       "2  relaciones exteriores                  eeuu   cuba   \n",
       "3                  salud                  None   None   \n",
       "4               ciclismo                  None   None   \n",
       "6          medioambiente  region de valparaiso   None   \n",
       "\n",
       "                                             Titular         Type  \n",
       "1  \\nDavid Ferrer sufrió otra temprana eliminación\\n  fid_noticia  \n",
       "2  \\nObama ha enviado cuatro solicitudes a Chile ...  fid_noticia  \n",
       "3  \\nQuiebre en comisión presidencial de isapres ...  fid_noticia  \n",
       "4  \\nWolfgang Burmann ganó el segundo clasificato...  fid_noticia  \n",
       "6  \\nGobierno presentó querella por derrame de pe...  fid_noticia  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNoticias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre procesing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "_map = {}\n",
    "_map[\"Cuerpo\"] = []\n",
    "_map[\"Seccion\"] = []\n",
    "_map[\"Tema\"] = []\n",
    "_map[\"Subtema\"] = []\n",
    "\n",
    "count = 0\n",
    "for index, row in dfNoticias.iterrows():    \n",
    "    _map[\"Cuerpo\"].append(row[\"Cuerpo\"]) \n",
    "    _map[\"Seccion\"].append(row[\"Seccion_1\"])\n",
    "    _map[\"Tema\"].append(row[\"Tema_1\"])\n",
    "    _map[\"Subtema\"].append(row[\"Subtema_1\"])\n",
    "\n",
    "df = pd.DataFrame(_map)\n",
    "# Elimino clase corporativo, muy pocos ejemplos\n",
    "df = df[df.Seccion != \"corporativo\"]\n",
    "\n",
    "    \n",
    "X_untransformed = df['Cuerpo'].reset_index(drop=True)\n",
    "y1 = df['Seccion'].reset_index(drop=True)\n",
    "y2 = df['Tema'].reset_index(drop=True)\n",
    "y3 = df['Subtema'].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraccion\n",
    "\n",
    "### Data representation\n",
    "* Normalisacion\n",
    "* TFID calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\" Funcion de normalizacion \"\"\"    \n",
    "    # split into words\n",
    "    tokens = nltk.tokenize.word_tokenize(text,language='spanish', preserve_line=False)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]    \n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are n<<<<<<<<<<<<<<<<<<<<<\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # stop word and remove accent\n",
    "    def strip_accents(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    stop_words = set(spanish_stopwords)\n",
    "    words = [strip_accents(w) for w in words if not w in stop_words]\n",
    "#    return u\" \".join(words)\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    out = \"\"\n",
    "    for word in words:\n",
    "        out += stemmer.stem(word)+\" \"    \n",
    "    return out\n",
    "    \n",
    "\n",
    "# spanish_stopwords = stopwords.words('spanish')\n",
    "# def normalize_text(text):\n",
    "#     return u\" \".join([x for x in [y for y in text.lower().strip().split(\" \")] \n",
    "#                       if len(x) > 1 and x not in spanish_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "TFIDF end time :132.3093593120575\n",
      "Number of features: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "print(\"start\")\n",
    "\n",
    "\n",
    "X_untransformed = X_untransformed\n",
    "n_docs = X_untransformed.shape[0]\n",
    "n_cpu = 40\n",
    "\n",
    "batch_size = int(n_docs/n_cpu)\n",
    "\n",
    "#'log', \"idf\":50.0\n",
    "\n",
    "wb = wordbatch.WordBatch(normalize_text, \n",
    "                         extractor=(WordBag, {\"hash_ngrams\": 1, \"hash_ngrams_weights\": [1.0, 1.0],\n",
    "                                              \"hash_size\": 2**28, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "                                              \"idf\": 1.0}), procs=n_cpu, n_words=10000, minibatch_size=batch_size)\n",
    "\n",
    "# wb = wordbatch.WordBatch(normalize_text, \n",
    "#                          extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.0, 1.0],\n",
    "#                                               \"hash_size\": 2**28, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "#                                               \"idf\": 1.0}), procs=n_cpu, n_words=10000, minibatch_size=batch_size)\n",
    "\n",
    "# add method=\"serial\" to extractor for debug normalize function\n",
    "\n",
    "wb.dictionary_freeze = True\n",
    "word_comment = wb.fit_transform(list(X_untransformed),reset= False)\n",
    "\n",
    "# revisar esta normalizacion\n",
    "X_transformed = word_comment[:, np.array(np.clip(word_comment.getnnz(axis=0) - 1, 0, 1), dtype = bool)]\n",
    "\n",
    "print(\"TFIDF end time :\" + str(time.time() - start) )\n",
    "\n",
    "X = X_transformed\n",
    "\n",
    "#X = X.todense()\n",
    "print('Number of features: {}'.format(X.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD\n",
    "\n",
    "Dim redution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time :282.3027448654175\n"
     ]
    }
   ],
   "source": [
    "svd_enable = False\n",
    "new_dim = 1000\n",
    "\n",
    "if svd_enable :\n",
    "    start = time.time()\n",
    "    svdT = TruncatedSVD(n_components = new_dim)\n",
    "    X = svdT.fit_transform(X)\n",
    "    print(\"end time :\" + str(time.time() - start) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data\n",
    "\n",
    "### save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features = False \n",
    "split_data = False\n",
    "file_name = \"tfid_hash28_n10000.p\"\n",
    "train_fraction = 0.8\n",
    "\n",
    "if split_data and save_features :\n",
    "    y1 = y1.values    \n",
    "\n",
    "    np.random.seed(42)\n",
    "    train_indices = np.random.choice(X.shape[0], round(train_fraction*X.shape[0]), replace=False)\n",
    "    test_indices = np.array(list(set(range(X.shape[0])) - set(train_indices)))\n",
    "\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y1[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y1[test_indices]\n",
    "    features_path_ = features_path + \"dataTrainTest_\" + file_name\n",
    "    pickle.dump( (X_train,y_train,X_test,y_test) , open( features_path_, \"wb\" ) )\n",
    "elif save_features:\n",
    "    features_path_ = features_path + \"data_\" + file_name\n",
    "    pickle.dump( (X,y1,y2,y3) , open( features_path_, \"wb\" ) )    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( (wb,y_train,X_test,y_test) , open( features_path_, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wb.transform(['asd auto casa'])\n",
    "#X_train,y_train,X_test,y_test = pickle.load( open( features_path_, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asd aut cas '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text('asd auto casa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordbags\n",
      "end time :15.28244161605835\n",
      "end time :3.846388816833496\n",
      "end time :0.05496048927307129\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "start = time.time()\n",
    "\n",
    "tfidf = wb.transform([normalize_text(X_untransformed[1])])\n",
    "print(\"end time :\" + str(time.time() - start) )\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tfidf = tfidf[:, np.array(np.clip(word_comment.getnnz(axis=0) - 1, 0, 1), dtype = bool)]\n",
    "#tfidf.shape\n",
    "print(\"end time :\" + str(time.time() - start) )\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "svd = svdT.transform(tfidf)\n",
    "print(\"end time :\" + str(time.time() - start) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidf_svd:\n",
    "    def __init__(self, wordbach_trained ,normalize_text , svd_trained,word_comment):\n",
    "        self.wordbach = wordbach_trained\n",
    "        self.svdT = svd_trained\n",
    "        self.normalize = normalize_text\n",
    "        self.word_comment = word_comment\n",
    "    def calc(self,text):\n",
    "        self.tfidf = self.wordbach.transform([self.normalize(text)])\n",
    "        self.tfidf = self.tfidf[:, np.array(np.clip(self.word_comment.getnnz(axis=0) - 1, 0, 1), dtype = bool)]\n",
    "        tfidf_svd = self.svdT.transform(tfidf)\n",
    "        return(tfidf_svd)\n",
    "        \n",
    "\n",
    "tfidf_svd_model = tfidf_svd(wb,normalize_text,svdT,word_comment)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordbags\n",
      "end time :20.13368320465088\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "tfidf_svd_model.calc(X_untransformed[1])\n",
    "print(\"end time :\" + str(time.time() - start) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

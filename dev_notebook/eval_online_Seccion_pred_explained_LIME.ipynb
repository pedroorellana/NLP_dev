{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, time, string, unicodedata\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.contrib.learn import DNNClassifier\n",
    "import time\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "#Carga stop word\n",
    "#nltk.download('stopwords')\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### funciones y clases \"template\"\n",
    "\n",
    "Para normalizar y clasficador , calcular features TFIDF+SVD,  DNN para evular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\" Funcion de normalizacion \"\"\"    \n",
    "    # split into words\n",
    "    tokens = nltk.tokenize.word_tokenize(text,language='spanish', preserve_line=False)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]    \n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are n<<<<<<<<<<<<<<<<<<<<<\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # stop word and remove accent\n",
    "    def strip_accents(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    stop_words = set(spanish_stopwords)\n",
    "    words = [strip_accents(w) for w in words if not w in stop_words]\n",
    "#    return u\" \".join(words)\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    out = \"\"\n",
    "    for word in words:\n",
    "        out += stemmer.stem(word)+\" \"    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidf_svd:\n",
    "    def __init__(self, wordbach_trained , svd_trained,non_zero_index_feat,normalize_text):\n",
    "        \"\"\" extractor features\"\"\"\n",
    "        self.wordbach = wordbach_trained\n",
    "        self.svdT = svd_trained\n",
    "        self.non_zero_index_feat = non_zero_index_feat\n",
    "        self.normalize_text = normalize_text\n",
    "    \n",
    "    def calc(self,text):\n",
    "        \n",
    "        self.tfidf = self.wordbach.transform([self.normalize_text(text)])\n",
    "        self.tfidf = self.tfidf[:, self.non_zero_index_feat]\n",
    "        tfidf_svd = self.svdT.transform(self.tfidf)\n",
    "        return(tfidf_svd)\n",
    "    \n",
    "    def calcBatch(self,texts):\n",
    "        \n",
    "        normTxts = []\n",
    "        for text in texts:\n",
    "            tmp = self.normalize_text(text)\n",
    "            normTxts.append(tmp)\n",
    "        \n",
    "        self.tfidf = self.wordbach.transform(normTxts)\n",
    "        self.tfidf = self.tfidf[:, self.non_zero_index_feat]\n",
    "        tfidf_svd = self.svdT.transform(self.tfidf)\n",
    "        return(tfidf_svd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_eval:\n",
    "    def __init__(self, labels, path_model,dim_vec_input):\n",
    "        \"\"\" DNN eval\"\"\"\n",
    "        self.path_model = path_model\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.nClasses = len(self.labels)\n",
    "        self.feature_columns = [tf.contrib.layers.real_valued_column('x', dimension = dim_vec_input)]\n",
    "        self.classifier = DNNClassifier(                                \n",
    "                                   n_classes=len(labels), label_keys=self.labels, \n",
    "                                   feature_columns=self.feature_columns,\n",
    "                                   hidden_units=[2000], \n",
    "                                   model_dir = self.path_model                         \n",
    "                                  )\n",
    "    def input_fn_evaluate(self):\n",
    "        input = {'x': tf.constant(self.vec_input )}    \n",
    "        return input    \n",
    "\n",
    "    def calc(self,vec_input):\n",
    "        self.vec_input = vec_input\n",
    "        #pred_test = self.classifier.predict_classes(input_fn=self.input_fn_evaluate)\n",
    "        pred_prob = self.classifier.predict_proba(input_fn=self.input_fn_evaluate)\n",
    "        pred_prob = [x for x in list(pred_prob)]\n",
    "        y_test_hat = self.labels[np.argmax(pred_prob)]\n",
    "        #y_test_hat = np.asarray([x.decode('UTF-8') for x in list(pred_test)])\n",
    "        #y_test_hat = y_test_hat.astype(str)\n",
    "        return (y_test_hat , pred_prob[0])\n",
    "    \n",
    "    def calcBatch(self,vec_input):\n",
    "        self.vec_input = vec_input\n",
    "        pred_prob = self.classifier.predict_proba(input_fn=self.input_fn_evaluate)\n",
    "        pred_prob = [x for x in list(pred_prob)]  \n",
    "        return (np.array(pred_prob))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../\"\n",
    "\n",
    "file_name = \"tfid_hash28_n10000_svd1000.p\"\n",
    "features_path = root_path + 'data/features/'\n",
    "features_path_ = features_path + \"calcFeat_\" + file_name\n",
    "path_model = root_path + 'models/test/info_model.p'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### carga clase extractor features TFIDF+SVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/features/calcFeat_tfid_hash28_n10000_svd1000.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-14174587ffd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvdT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnon_zero_index_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfeatures_path_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtfidf_svd_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_svd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvdT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnon_zero_index_feat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalize_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/features/calcFeat_tfid_hash28_n10000_svd1000.p'"
     ]
    }
   ],
   "source": [
    "wb,svdT,non_zero_index_feat = pickle.load( open( features_path_, \"rb\" ) )\n",
    "\n",
    "tfidf_svd_model = tfidf_svd(wb,svdT,non_zero_index_feat,normalize_text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### carga modelo DNN para inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model,labels = pickle.load( open( path_model, \"rb\" ) )\n",
    "\n",
    "sec1 = DNN_eval(labels, path_model,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evalua una entrada de test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalua \n",
    "test_text = 'El Banco Falabella se convertirá en el mayor emisor de tarjetas de crédito del país, después de que la Superintendencia de Bancos e Instituciones Financieras (SBIF) aprobara la integración de CMR Falabella a la compañía. La figura bajo la cual CMR se integra a Banco Falabella es la de Sociedad de Apoyo al Giro (SAG). Con esto, Banco Falabella será el mayor emisor de tarjetas de crédito del país, con una cantidad superior a los 3 millones de ellas activas, según Diario Financiero.'\n",
    "\n",
    "start = time.time()\n",
    "feat = tfidf_svd_model.calc(test_text)\n",
    "y_test_hat , pred_prob = sec1.calc( vec_input = feat)\n",
    "\n",
    "print(\"end time :\" + str(time.time() - start) )\n",
    "y_test_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(np.arange(8), pred_prob)\n",
    "plt.xticks(np.arange(8), labels)\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicciton explained LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTema2(texts):\n",
    "    outs = []\n",
    "    \n",
    "    feat_data = tfidf_svd_model.calcBatch(texts)\n",
    "    pred_prob = sec1.calcBatch( vec_input = feat_data)\n",
    "\n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "exp = explainer.explain_instance(test_text, predictTema2, num_features=10, top_labels=2 )#labels=[0, 2] top_labels=2\n",
    "exp.show_in_notebook(text=test_text)\n",
    "\n",
    "print(\"end time :\" + str(time.time() - start) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(test_text, predictTema2, num_features=10, top_labels=2 )#labels=[0, 2] top_labels=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_svd_model.calcBatch([test_text,test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = tfidf_svd_model.calc(test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec1.calcBatch( vec_input = feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import re, sys, unidecode\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import scikitplot as skplt\n",
    "\n",
    "from tensorflow.contrib.learn import DNNClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../\"\n",
    "path_model = root_path + 'models/seccion'\n",
    "features_path = root_path + 'data/features/data_tfid_hash28_n1000_SVD2.p'\n",
    "\n",
    "delete_old_model = True\n",
    "if delete_old_model:\n",
    "    try:\n",
    "        os.system(\"rm -rf \"+path_model)\n",
    "        os.system(\"mkdir \"+path_model)\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y1, y2, y3 = pickle.load( open( features_path, \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['deportes', 'pais', 'pais', ..., 'cultura', 'pais', 'mundo'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre procesing\n",
    "\n",
    "Cleaning data, select clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = np.array(y2)\n",
    "y_2 = list(map(lambda x: unidecode.unidecode(x) if x!=None else None, y2))\n",
    "y_2 = np.array(y_2)\n",
    "\n",
    "\n",
    "y1 = y1.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraccion\n",
    "\n",
    "### Data representation\n",
    "\n",
    "TFID calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deportes',\n",
       " 'mundo',\n",
       " 'cultura',\n",
       " 'economia',\n",
       " 'tecnologia',\n",
       " 'entretencion',\n",
       " 'sociedad',\n",
       " 'pais']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(set(y1))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterClases(X,y,umbral):\n",
    "    labels = list(set(y))\n",
    "    sizes = [ [label,y[y== label].shape[0]] for label in labels ]\n",
    "    filter_sizes = list(filter(lambda x:x[1]>umbral ,sizes ))\n",
    "    names_clases = set( map(lambda x:x[0], filter_sizes ) )\n",
    "    index = list(map(lambda x: {x}.issubset(names_clases), y ))\n",
    "    y = y[index]\n",
    "    X = X[np.nonzero(index)]\n",
    "    return X,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "deportes ,total : 33 ,deleted : 11\n",
      "['copa libertadores', 'baloncesto', 'hockey patin', 'al aire libre', 'velerismo', 'rugby', 'torneo de clausura', 'copa davis', 'torneo de apertura', 'motociclismo', 'tenis de mesa', 'londres ', 'copa america', 'brasil ', 'olimpismo', 'mundiales de futbol', 'automovilismo', 'eurocopa', 'futbol', 'balonmano', 'tenis', 'gobierno', 'natacion', 'gimnasia', 'boxeo', 'atletismo', 'paralimpicos', 'rally', 'polideportivo', 'ciclismo', 'voleibol', 'golf', 'triatlon']\n",
      "\n",
      "mundo ,total : 38 ,deleted : 8\n",
      "['organismos internacionales', 'ecuador', 'bolivia', 'africa', 'reino unido', 'pacifico sur', 'iran', 'colombia', 'paraguay', 'rusia', 'cuba', 'peru', 'afganistan', 'india', 'brasil', 'haiti', 'asia', 'peninsula de corea', 'alemania', 'accidentes aereos', 'desastres naturales', 'argentina', 'medio oriente', 'japon', 'francia', 'eeuu', 'espana', 'china', 'italia', 'america latina', 'al qaeda', 'irak', 'venezuela', 'sudafrica', 'mexico', 'vaticano', 'europa', 'uruguay']\n",
      "\n",
      "cultura ,total : 4 ,deleted : 8\n",
      "['cultura popular', 'teatro', 'arte', 'literatura']\n",
      "\n",
      "economia ,total : 16 ,deleted : 2\n",
      "['crisis financiera', 'divisas', 'retail', 'crecimiento', 'organismos internacionales', 'empresas', 'sueldo minimo', 'banco central', 'materias primas', 'sistema previsional', 'bolsas', 'servicios financieros', 'sectores productivos', 'impuestos', 'presupuesto', 'competitividad']\n",
      "\n",
      "tecnologia ,total : 4 ,deleted : 3\n",
      "['internet', 'redes sociales', 'inventos', 'industria']\n",
      "\n",
      "entretencion ,total : 14 ,deleted : 4\n",
      "['festivales', 'listas', 'musica', 'tendencias', 'comic', 'sucesos', 'cine', 'festival de vina', 'espectaculos', 'panoramas', 'humor', 'television', 'radio', 'personajes']\n",
      "\n",
      "sociedad ,total : 17 ,deleted : 12\n",
      "['medioambiente', 'astronomia', 'homosexualidad', 'historia', 'sexualidad', 'desarrollo humano', 'pedofilia', 'fauna', 'sucesos', 'ciencia', 'medios', 'celebraciones', 'premios nobel', 'mujer', 'religion', 'ciencias sociales', 'salud']\n",
      "\n",
      "pais ,total : 52 ,deleted : 15\n",
      "['medioambiente', 'region del biobio', 'augusto pinochet', 'organismos del estado', 'energia', 'tiempo', 'trabajo', 'presidente pinera', 'vivienda', 'festivos', 'salud', 'seguridad ciudadana', 'iglesia catolica', 'michelle bachelet', 'obras publicas', 'judicial', 'manifestaciones', 'isla de pascua', 'servicios', 'consumidores', 'mujer', 'pueblos originarios', 'presidenta bachelet', 'turismo', 'region de los rios', 'sismos', 'region de los lagos', 'servicios publicos', 'gobierno', 'desastres naturales', 'region de antofagasta', 'transportes', 'educacion', 'ddhh', 'sebastian pinera', 'region de coquimbo', 'region de aysen', 'ciudades', 'region de la araucania', 'relaciones exteriores', 'poblacion', 'politica', 'ffaa y de orden', 'region de magallanes', 'politicas sociales', 'region de atacama', 'policial', 'region de arica', 'region del maule', 'region de valparaiso', 'empresas del estado', 'infancia']\n"
     ]
    }
   ],
   "source": [
    "y_temas = {}\n",
    "X_temas = {}\n",
    "labels_temas = {}\n",
    "labels_temas_before = {}\n",
    "umbral_ejemplos = 100\n",
    "\n",
    "for key in labels:\n",
    "    index = y1 == key\n",
    "    y_22 = y_2[index]\n",
    "    X2 = X[np.nonzero(index)]\n",
    "\n",
    "    index = y_22 != None\n",
    "    y_22 = y_22[index]\n",
    "    X2 = X2[np.nonzero(index)]\n",
    "    X_temas[key],y_temas[key] = filterClases(X2,y_22,umbral_ejemplos)\n",
    "    \n",
    "    \n",
    "    labels_temas_before = list(set(y_22))\n",
    "    labels_temas[key] = list(set(y_temas[key]))\n",
    "    \n",
    "    print(\"\\n\"+key + \" ,total : \" + str(len(labels_temas[key])) \\\n",
    "          +\" ,deleted : \"+ str( len(labels_temas_before) - len(labels_temas[key] )) )\n",
    "    print(labels_temas[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "train_fraction = 0.8\n",
    "\n",
    "X_train = {}\n",
    "y_train = {}\n",
    "X_test = {}\n",
    "y_test = {}\n",
    "train_indices = {}\n",
    "test_indices = {}\n",
    "\n",
    "for key in labels:\n",
    "    train_indices[key] = np.random.choice(X_temas[key].shape[0], round(train_fraction*X_temas[key].shape[0]), replace=False)\n",
    "    test_indices[key] = np.array(list(set(range(X_temas[key].shape[0])) - set(train_indices)))\n",
    "\n",
    "    X_train[key] = X_temas[key][train_indices[key]]\n",
    "    y_train[key] = y_temas[key][train_indices[key]]\n",
    "    X_test[key] = X_temas[key][test_indices[key]]\n",
    "    y_test[key] = y_temas[key][test_indices[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_train = {}\n",
    "weights_test = {}\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for key in labels:\n",
    "    labelsTmp = list(set(y_train[key]))\n",
    "    nClasses = len(labelsTmp)\n",
    "\n",
    "    sizes = {label: y_train[key][y_train[key] == label].shape[0] for label in labelsTmp}\n",
    "    weights = np.asarray([len(y_train[key])/(sizes[label]*nClasses) for label in y_temas[key]])\n",
    "\n",
    "    weights = weights[:,np.newaxis]\n",
    "    weights_train[key] = weights[train_indices[key]]\n",
    "    weights_test[key] = weights[test_indices[key]]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### DNN graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 14000\n",
    "\n",
    "if isinstance(X_train, scipy.sparse.csr.csr_matrix):\n",
    "    X_train = X_train.todense()\n",
    "    X_test = X_test.todense()\n",
    "\n",
    "# Define the test inputs\n",
    "def get_train_inputs(key):    \n",
    "    dataset = tf.estimator.inputs.numpy_input_fn({'x': X_train[key],'class_weights': weights_train[key]},\n",
    "                                                  y_train[key][:,np.newaxis],\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=50,\n",
    "                                                  num_epochs=epochs)\n",
    "    return dataset\n",
    "\n",
    "def get_test_inputs(key):   \n",
    "    dataset = tf.estimator.inputs.numpy_input_fn({'x': X_test[key],'class_weights': weights_test[key]},\n",
    "                                                  y_test[key][:,np.newaxis],\n",
    "                                                  shuffle=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions graph tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tf.set_random_seed(42)\n",
    "feature_columns = [tf.contrib.layers.real_valued_column('x', dimension=1000)]\n",
    "\n",
    "classifier = {}\n",
    "path_all_models  = {}\n",
    "for key in labels:\n",
    "    path_all_models[key] = path_model+\"/\"+str(key)\n",
    "    classifier[key] = DNNClassifier(                                \n",
    "                               n_classes=len(labels_temas[key]), label_keys=labels_temas[key], feature_columns=feature_columns,\n",
    "                               hidden_units=[2000],\n",
    "                               dropout=0.5,\n",
    "                               weight_column_name='class_weights',\n",
    "                               model_dir = path_model+\"/\"+str(key),\n",
    "                               config = tf.contrib.learn.RunConfig(save_checkpoints_steps = 500,\n",
    "                               save_checkpoints_secs = None)                           \n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation_monitor = {}\n",
    "\n",
    "print(\"start\")\n",
    "start = time.time()\n",
    "\n",
    "for key in labels:\n",
    "    \n",
    "    validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "        #input_fn= get_train_inputs(),\n",
    "        input_fn= get_test_inputs(key),    \n",
    "        every_n_steps=500,\n",
    "        #early_stopping_metric=\"accuracy\",#loss\n",
    "        early_stopping_metric=\"loss\",\n",
    "        early_stopping_metric_minimize=True,\n",
    "        early_stopping_rounds=2000)\n",
    "\n",
    "    \n",
    "\n",
    "    classifier[key].fit(input_fn=get_train_inputs(key), monitors=[validation_monitor], steps=epochs, max_steps=None)\n",
    "    print(\"###################### \"+key+\" ######################\")\n",
    "    \n",
    "end = time.time()\n",
    "#print(key)\n",
    "print(\"Training time :\" + str(end - start) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = {}\n",
    "y_test_hat = {}\n",
    "y_test_hat = {}\n",
    "acc = {}\n",
    "\n",
    "for key in labels:\n",
    "\n",
    "    def input_fn_evaluate():\n",
    "        dataset = {'x': tf.constant(X_test[key].todense())}    \n",
    "        return dataset\n",
    "\n",
    "    pred_test[key] = classifier[key].predict_classes(input_fn=input_fn_evaluate)\n",
    "    y_test_hat[key] = np.asarray([x.decode('UTF-8') for x in list(pred_test[key])])\n",
    "    y_test_hat[key] = y_test_hat[key].astype(str)\n",
    "    acc[key] = accuracy_score(y_true=y_test[key], y_pred=y_test_hat[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in labels:\n",
    "    display(Markdown('## ' + key))\n",
    "    display(Markdown('## Accuracy in test: {} '.format(acc[key]*100)))    \n",
    "    skplt.metrics.plot_confusion_matrix(y_test[key], y_test_hat[key],normalize=True,figsize=(20,20))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    #print('Accuracy in test: {}'.format(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Save info model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( (path_all_models,labels_temas), open( path_model + \"/info_model.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, time, string, unicodedata\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.contrib.learn import DNNClassifier\n",
    "import time\n",
    "\n",
    "# Representation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Carga stop word\n",
    "#nltk.download('stopwords')\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\" Funcion de normalizacion \"\"\"    \n",
    "    # split into words\n",
    "    tokens = nltk.tokenize.word_tokenize(text,language='spanish', preserve_line=False)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]    \n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are n<<<<<<<<<<<<<<<<<<<<<\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # stop word and remove accent\n",
    "    def strip_accents(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    stop_words = set(spanish_stopwords)\n",
    "    words = [strip_accents(w) for w in words if not w in stop_words]\n",
    "#    return u\" \".join(words)\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    out = \"\"\n",
    "    for word in words:\n",
    "        out += stemmer.stem(word)+\" \"    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidf_svd:\n",
    "    def __init__(self, wordbach_trained , svd_trained,non_zero_index_feat,normalize_text):\n",
    "        \"\"\" extractor features\"\"\"\n",
    "        self.wordbach = wordbach_trained\n",
    "        self.svdT = svd_trained\n",
    "        self.non_zero_index_feat = non_zero_index_feat\n",
    "        self.normalize_text = normalize_text\n",
    "    \n",
    "    def calc(self,text):\n",
    "        \n",
    "        self.tfidf = self.wordbach.transform([self.normalize_text(text)])\n",
    "        self.tfidf = self.tfidf[:, self.non_zero_index_feat]\n",
    "        tfidf_svd = self.svdT.transform(self.tfidf)\n",
    "        return(tfidf_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_eval:\n",
    "    def __init__(self, labels, path_model,dim_vec_input):\n",
    "        \"\"\" DNN eval\"\"\"\n",
    "        self.path_model = path_model\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.nClasses = len(self.labels)\n",
    "        self.feature_columns = [tf.contrib.layers.real_valued_column('x', dimension = dim_vec_input)]\n",
    "        self.classifier = DNNClassifier(                                \n",
    "                                   n_classes=len(labels), label_keys=self.labels, \n",
    "                                   feature_columns=self.feature_columns,\n",
    "                                   hidden_units=[2000], \n",
    "                                   model_dir = self.path_model                         \n",
    "                                  )\n",
    "    def input_fn_evaluate(self):\n",
    "        input = {'x': tf.constant(self.vec_input )}    \n",
    "        return input    \n",
    "\n",
    "    def calc(self,vec_input):\n",
    "        self.vec_input = vec_input\n",
    "        pred_test = self.classifier.predict_classes(input_fn=self.input_fn_evaluate)\n",
    "        y_test_hat = np.asarray([x.decode('UTF-8') for x in list(pred_test)])\n",
    "        y_test_hat = y_test_hat.astype(str)\n",
    "        return (y_test_hat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../\"\n",
    "\n",
    "file_name = \"tfid_hash28_n10000_svd1000.p\"\n",
    "features_path = root_path + 'data/features/'\n",
    "features_path_ = features_path + \"calcFeat_\" + file_name\n",
    "path_model = root_path + 'models/test/info_model.p'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### carga extractor features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb,svdT,non_zero_index_feat = pickle.load( open( features_path_, \"rb\" ) )\n",
    "\n",
    "tfidf_svd_model = tfidf_svd(wb,svdT,non_zero_index_feat,normalize_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model2,labels = pickle.load( open( path_model, \"rb\" ) )\n",
    "\n",
    "sec1 = DNN_eval(labels, path_model2,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordbags\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../models/test/model.ckpt-14000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "end time :2.362139940261841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['economia'], dtype='<U8')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evalua \n",
    "test_text = 'El Banco Falabella se convertirá en el mayor emisor de tarjetas de crédito del país, después de que la Superintendencia de Bancos e Instituciones Financieras (SBIF) aprobara la integración de CMR Falabella a la compañía. La figura bajo la cual CMR se integra a Banco Falabella es la de Sociedad de Apoyo al Giro (SAG). Con esto, Banco Falabella será el mayor emisor de tarjetas de crédito del país, con una cantidad superior a los 3 millones de ellas activas, según Diario Financiero.'\n",
    "start = time.time()\n",
    "test_text = tfidf_svd_model.calc(test_text)\n",
    "a = sec1.calc( vec_input = test_text)\n",
    "\n",
    "print(\"end time :\" + str(time.time() - start) )\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
